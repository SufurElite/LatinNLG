{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial LatinBERT Language Generation Analysis\n",
    "<hr>\n",
    "\n",
    "#### Imports\n",
    "We'll first import the different necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import fetch\n",
    "fetch.text_retrieval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:17:29.776650Z",
     "iopub.status.busy": "2023-03-01T21:17:29.776151Z",
     "iopub.status.idle": "2023-03-01T21:17:38.618183Z",
     "shell.execute_reply": "2023-03-01T21:17:38.616881Z",
     "shell.execute_reply.started": "2023-03-01T21:17:29.776611Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, re\n",
    "from Data import dataExp\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import LatinBERT\n",
    "from LatinBERT.gen_berts import LatinBERT\n",
    "from LatinBERT.LatinTok import LatinTokenizer\n",
    "from LatinBERT.predict_words import predict\n",
    "from transformers import BertModel, BertForMaskedLM, BertPreTrainedModel\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "    from cltk.tokenizers.lat.lat import LatinWordTokenizer as WordTokenizer\n",
    "    from cltk.tokenizers.lat.lat import LatinPunktSentenceTokenizer as SentenceTokenizer\n",
    "from cltk.embeddings.embeddings import Word2VecEmbeddings as W2VE\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:17:41.635294Z",
     "iopub.status.busy": "2023-03-01T21:17:41.634045Z",
     "iopub.status.idle": "2023-03-01T21:17:45.248021Z",
     "shell.execute_reply": "2023-03-01T21:17:45.246416Z",
     "shell.execute_reply.started": "2023-03-01T21:17:41.635228Z"
    }
   },
   "outputs": [],
   "source": [
    "CI = dataExp.CorpusInterface(corpus_name=\"text_corpus.pickle\", shouldTokenize = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then quickly visualise each author and the amount of text available to each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:17:49.509159Z",
     "iopub.status.busy": "2023-03-01T21:17:49.508794Z",
     "iopub.status.idle": "2023-03-01T21:17:49.519790Z",
     "shell.execute_reply": "2023-03-01T21:17:49.518225Z",
     "shell.execute_reply.started": "2023-03-01T21:17:49.509133Z"
    }
   },
   "outputs": [],
   "source": [
    "top_authors = CI.get_authors_by_text_size()\n",
    "for (author, count) in top_authors:\n",
    "    print(author, count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load in the default LatinBERT model to perform text generation from different authors' sampled texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:17:53.522998Z",
     "iopub.status.busy": "2023-03-01T21:17:53.522602Z",
     "iopub.status.idle": "2023-03-01T21:18:00.071915Z",
     "shell.execute_reply": "2023-03-01T21:18:00.070225Z",
     "shell.execute_reply.started": "2023-03-01T21:17:53.522968Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizerPath = os.getcwd()+\"/LatinBERT/latin.subword.encoder\"\n",
    "bertPath = os.getcwd()+\"/LatinBERT/latin_bert\"\n",
    "encoder = text_encoder.SubwordTextEncoder(tokenizerPath)\n",
    "wp_tokenizer = LatinTokenizer(encoder)\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bertPath)\n",
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a generator function, which keeps predicting the next word after the current context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:18:06.005982Z",
     "iopub.status.busy": "2023-03-01T21:18:06.005309Z",
     "iopub.status.idle": "2023-03-01T21:18:06.164873Z",
     "shell.execute_reply": "2023-03-01T21:18:06.163956Z",
     "shell.execute_reply.started": "2023-03-01T21:18:06.005936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In omnia sunt , et sunt , et sunt , et sunt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_text(text: str, num_words:int, wp_tokenizer, model):\n",
    "    total_text = text\n",
    "    for i in range(num_words):\n",
    "        total_text = predict(wp_tokenizer, total_text, model)\n",
    "    return total_text\n",
    "gen_text(\"In omnia\", 10, wp_tokenizer, model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the output shows that the text is starting to looop \"et sunt, et sunt, et sunt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Selection\n",
    "First, we need to select particular authors and sample their texts to give LatinBERT an initial place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_authors = [\"ovid\", \"cicero\", \"jerome\", \"catullus\", \"vergil\"]\n",
    "text_by_author = {}\n",
    "for author in selected_authors:\n",
    "    text_by_author[author] = CI.get_text_for_author(author)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having selected authors, we can now generate text by randomly choosing an author, sampling randomly from their work, continuing the work with 30 more words from the sample, and then comparing it with how the work should have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "generated_text = {'author': [], 'prompt_text_length':[], 'correct_continuation': [], 'generated_continuation': [] }\n",
    "number_of_samples = 50\n",
    "text_continuation_length = 30\n",
    "max_initial_length = 200\n",
    "for i in range(number_of_samples):\n",
    "    print(i)\n",
    "    author = selected_authors[random.randint(0,len(selected_authors)-1)]\n",
    "    author_text = text_by_author[author][random.randint(0,len(text_by_author[author])-1)].split(\" \")\n",
    "    if len(author_text)<max_initial_length*2+text_continuation_length: continue\n",
    "    end_idx = random.randint(max_initial_length, len(author_text)-text_continuation_length-max_initial_length)\n",
    "    start_idx = end_idx-max_initial_length\n",
    "    \n",
    "    prompt_text = \" \".join(author_text[start_idx:end_idx])\n",
    "    \n",
    "    txt = gen_text(prompt_text, text_continuation_length, wp_tokenizer, model)\n",
    "    generated_text[\"author\"].append(author)\n",
    "    generated_text[\"prompt_text_length\"].append(end_idx-start_idx)\n",
    "    generated_text[\"correct_continuation\"].append(\" \".join(author_text[end_idx-15:end_idx+30]))\n",
    "    txt = txt.split(\" \")\n",
    "    generated_text[\"generated_continuation\"].append(\" \".join(txt[-45:]))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be easier to view the output as a dataframe, and it will allow us to save the information as a CSV, which might be convenient for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output continues to show signs of 'looping' and overall is surprisingly poorer than anticipated/hoped for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.getcwd()+\"/Results/BertGEN.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
