{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da0781f-58c8-480e-9a56-d68c580d5155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T04:27:50.848343Z",
     "iopub.status.busy": "2023-05-13T04:27:50.847597Z",
     "iopub.status.idle": "2023-05-13T04:27:55.767947Z",
     "shell.execute_reply": "2023-05-13T04:27:55.767194Z",
     "shell.execute_reply.started": "2023-05-13T04:27:50.848311Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, re\n",
    "from Data import dataExp\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, TextDataset, DataCollatorForLanguageModeling\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "270df543-d01f-4a3e-a726-cf3866a6e654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T04:27:57.710585Z",
     "iopub.status.busy": "2023-05-13T04:27:57.709314Z",
     "iopub.status.idle": "2023-05-13T04:27:57.727517Z",
     "shell.execute_reply": "2023-05-13T04:27:57.726839Z",
     "shell.execute_reply.started": "2023-05-13T04:27:57.710557Z"
    }
   },
   "outputs": [],
   "source": [
    "author = \"Caesar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1848da-eb9d-48bd-9714-655a3ee40e64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:07:08.318629Z",
     "iopub.status.busy": "2023-05-12T13:07:08.318307Z",
     "iopub.status.idle": "2023-05-12T13:07:09.984283Z",
     "shell.execute_reply": "2023-05-12T13:07:09.983588Z",
     "shell.execute_reply.started": "2023-05-12T13:07:08.318604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the existing corpus\n",
      "abbofloracensis had 1 pieces of work with a total of 34398 characters of text\n",
      "abelard had 1 pieces of work with a total of 85089 characters of text\n",
      "acticussincerius had 3 pieces of work with a total of 23788 characters of text\n",
      "addison had 1 pieces of work with a total of 1764 characters of text\n",
      "adso had 1 pieces of work with a total of 13551 characters of text\n",
      "aelredus had 1 pieces of work with a total of 118173 characters of text\n",
      "agnes had 1 pieces of work with a total of 74784 characters of text\n",
      "alanus had 1 pieces of work with a total of 195061 characters of text\n",
      "albericodamarcellise had 1 pieces of work with a total of 172 characters of text\n",
      "albertanus had 1 pieces of work with a total of 18033 characters of text\n",
      "albertofaix had 1 pieces of work with a total of 85671 characters of text\n",
      "alcuin had 1 pieces of work with a total of 2642 characters of text\n",
      "aleandrogerolamo had 2 pieces of work with a total of 13499 characters of text\n",
      "alfonsi had 1 pieces of work with a total of 105630 characters of text\n",
      "ambrose had 1 pieces of work with a total of 7005 characters of text\n",
      "ammianus had 18 pieces of work with a total of 975460 characters of text\n",
      "ampelius had 1 pieces of work with a total of 52872 characters of text\n",
      "andecavis had 1 pieces of work with a total of 920 characters of text\n",
      "andreasbergoma had 1 pieces of work with a total of 28522 characters of text\n",
      "andronicus had 1 pieces of work with a total of 2238 characters of text\n",
      "angeloambrogini had 5 pieces of work with a total of 19241 characters of text\n",
      "angelopoliziano had 2 pieces of work with a total of 4118 characters of text\n",
      "angilbert had 1 pieces of work with a total of 2094 characters of text\n",
      "annalesregnifrancorum had 1 pieces of work with a total of 155834 characters of text\n",
      "annalesvedastini had 1 pieces of work with a total of 54567 characters of text\n",
      "anon had 1 pieces of work with a total of 43284 characters of text\n",
      "anonymous had 1 pieces of work with a total of 6463 characters of text\n",
      "anselmepistula had 1 pieces of work with a total of 5009 characters of text\n",
      "anselmproslogion had 1 pieces of work with a total of 39144 characters of text\n",
      "apicius had 1 pieces of work with a total of 8603 characters of text\n",
      "appverg had 1 pieces of work with a total of 91 characters of text\n",
      "appvergcomp had 1 pieces of work with a total of 110025 characters of text\n",
      "appvergculex had 1 pieces of work with a total of 18887 characters of text\n",
      "apuleius had 13 pieces of work with a total of 604335 characters of text\n",
      "aquinas had 1 pieces of work with a total of 7076 characters of text\n",
      "arbroath had 1 pieces of work with a total of 7497 characters of text\n",
      "archpoet had 1 pieces of work with a total of 29641 characters of text\n",
      "aristotle had 1 pieces of work with a total of 11537 characters of text\n",
      "arnobius had 1 pieces of work with a total of 41270 characters of text\n",
      "arnulf had 1 pieces of work with a total of 1387 characters of text\n",
      "asconius had 1 pieces of work with a total of 112203 characters of text\n",
      "asserius had 1 pieces of work with a total of 89247 characters of text\n",
      "augustine had 7 pieces of work with a total of 402151 characters of text\n",
      "aureliusvictor had 2 pieces of work with a total of 149735 characters of text\n",
      "aus had 1 pieces of work with a total of 2241 characters of text\n",
      "ausonius had 24 pieces of work with a total of 292442 characters of text\n",
      "ave had 1 pieces of work with a total of 6949 characters of text\n",
      "avianus had 1 pieces of work with a total of 30263 characters of text\n",
      "avienus had 1 pieces of work with a total of 27798 characters of text\n",
      "axio had 1 pieces of work with a total of 16543 characters of text\n",
      "bacon had 1 pieces of work with a total of 48610 characters of text\n",
      "balbus had 1 pieces of work with a total of 16073 characters of text\n",
      "balde had 1 pieces of work with a total of 765 characters of text\n",
      "baldo had 1 pieces of work with a total of 59162 characters of text\n",
      "bebel had 1 pieces of work with a total of 131504 characters of text\n",
      "bede had 5 pieces of work with a total of 575775 characters of text\n",
      "benedict had 1 pieces of work with a total of 92323 characters of text\n",
      "berengar had 1 pieces of work with a total of 37827 characters of text\n",
      "bernardcluny had 1 pieces of work with a total of 48646 characters of text\n",
      "bible had 1 pieces of work with a total of 11363 characters of text\n",
      "biggs had 1 pieces of work with a total of 44386 characters of text\n",
      "bill had 1 pieces of work with a total of 3236 characters of text\n",
      "blesensis had 1 pieces of work with a total of 51232 characters of text\n",
      "boethius had 10 pieces of work with a total of 263001 characters of text\n",
      "boethiusdacia had 1 pieces of work with a total of 50991 characters of text\n",
      "bonaventura had 1 pieces of work with a total of 72084 characters of text\n",
      "boskovic had 1 pieces of work with a total of 22752 characters of text\n",
      "brevechronicon had 1 pieces of work with a total of 8191 characters of text\n",
      "buchanan had 1 pieces of work with a total of 36240 characters of text\n",
      "bultelius had 1 pieces of work with a total of 29037 characters of text\n",
      "caeciliusbalbus had 1 pieces of work with a total of 45340 characters of text\n",
      "caesar had 11 pieces of work with a total of 616439 characters of text\n",
      "caesaraugustus had 1 pieces of work with a total of 18129 characters of text\n",
      "calpurniusflaccus had 1 pieces of work with a total of 48342 characters of text\n",
      "calpurniussiculus had 1 pieces of work with a total of 34152 characters of text\n",
      "campion had 1 pieces of work with a total of 18761 characters of text\n",
      "capellanus had 1 pieces of work with a total of 233924 characters of text\n",
      "carm had 1 pieces of work with a total of 1530 characters of text\n",
      "carmenarvale had 1 pieces of work with a total of 560 characters of text\n",
      "carmeninvictoriam had 1 pieces of work with a total of 13588 characters of text\n",
      "carmensaliare had 1 pieces of work with a total of 306 characters of text\n",
      "cassiodorus had 1 pieces of work with a total of 63779 characters of text\n",
      "catalogueliberien had 1 pieces of work with a total of 7213 characters of text\n",
      "cato had 1 pieces of work with a total of 14361 characters of text\n",
      "catullus had 1 pieces of work with a total of 85900 characters of text\n",
      "celsus had 8 pieces of work with a total of 696619 characters of text\n",
      "celtis had 1 pieces of work with a total of 1855 characters of text\n",
      "censorinus had 1 pieces of work with a total of 71644 characters of text\n",
      "cicero had 122 pieces of work with a total of 7438016 characters of text\n",
      "cinna had 1 pieces of work with a total of 861 characters of text\n",
      "claud had 1 pieces of work with a total of 4961 characters of text\n",
      "claudian had 19 pieces of work with a total of 441142 characters of text\n",
      "clitophon had 1 pieces of work with a total of 9925 characters of text\n",
      "colman had 1 pieces of work with a total of 1737 characters of text\n",
      "columba had 1 pieces of work with a total of 37136 characters of text\n",
      "columbus had 1 pieces of work with a total of 15591 characters of text\n",
      "columella had 9 pieces of work with a total of 562550 characters of text\n",
      "comes had 1 pieces of work with a total of 441 characters of text\n",
      "commodianus had 1 pieces of work with a total of 939 characters of text\n",
      "corippus had 8 pieces of work with a total of 217825 characters of text\n",
      "corneliopaoloamalteo had 1 pieces of work with a total of 503 characters of text\n",
      "corvinus had 1 pieces of work with a total of 4815 characters of text\n",
      "cotta had 1 pieces of work with a total of 12574 characters of text\n",
      "creeds had 1 pieces of work with a total of 6010 characters of text\n",
      "curtius had 1 pieces of work with a total of 68196 characters of text\n",
      "curtiusrufus had 8 pieces of work with a total of 536805 characters of text\n",
      "dante had 1 pieces of work with a total of 41987 characters of text\n",
      "dantealighieri had 1 pieces of work with a total of 120 characters of text\n",
      "dares had 1 pieces of work with a total of 56639 characters of text\n",
      "debury had 1 pieces of work with a total of 115993 characters of text\n",
      "declaratio had 1 pieces of work with a total of 10290 characters of text\n",
      "decretum had 1 pieces of work with a total of 15220 characters of text\n",
      "descartes had 1 pieces of work with a total of 15427 characters of text\n",
      "dicquid had 1 pieces of work with a total of 328 characters of text\n",
      "diesirae had 1 pieces of work with a total of 1430 characters of text\n",
      "diravi had 1 pieces of work with a total of 795 characters of text\n",
      "don had 1 pieces of work with a total of 29181 characters of text\n",
      "donation had 1 pieces of work with a total of 20027 characters of text\n",
      "dracontius had 16 pieces of work with a total of 266831 characters of text\n",
      "dumdiane had 1 pieces of work with a total of 1532 characters of text\n",
      "dumdomus had 1 pieces of work with a total of 349 characters of text\n",
      "ebulo had 1 pieces of work with a total of 73683 characters of text\n",
      "egeria had 1 pieces of work with a total of 60681 characters of text\n",
      "ein had 1 pieces of work with a total of 54962 characters of text\n",
      "enn had 1 pieces of work with a total of 8355 characters of text\n",
      "ennius had 1 pieces of work with a total of 25266 characters of text\n",
      "ennodius had 1 pieces of work with a total of 75591 characters of text\n",
      "ep had 1 pieces of work with a total of 18656 characters of text\n",
      "epistaustras had 1 pieces of work with a total of 34585 characters of text\n",
      "epitaphs had 1 pieces of work with a total of 10341 characters of text\n",
      "epitomecononiana had 1 pieces of work with a total of 39045 characters of text\n",
      "epitomefeliciana had 1 pieces of work with a total of 44673 characters of text\n",
      "erasmus had 1 pieces of work with a total of 32404 characters of text\n",
      "erchempert had 1 pieces of work with a total of 89824 characters of text\n",
      "estas had 1 pieces of work with a total of 597 characters of text\n",
      "eucherius had 1 pieces of work with a total of 28159 characters of text\n",
      "eugenius had 1 pieces of work with a total of 584 characters of text\n",
      "eugippius had 1 pieces of work with a total of 87808 characters of text\n",
      "eutropius had 1 pieces of work with a total of 133407 characters of text\n",
      "exivi had 1 pieces of work with a total of 30157 characters of text\n",
      "fabe had 1 pieces of work with a total of 31196 characters of text\n",
      "falcandus had 1 pieces of work with a total of 262882 characters of text\n",
      "falcone had 1 pieces of work with a total of 273400 characters of text\n",
      "faustoandrelino had 1 pieces of work with a total of 4769 characters of text\n",
      "ferraria had 1 pieces of work with a total of 18083 characters of text\n",
      "ficino had 1 pieces of work with a total of 20924 characters of text\n",
      "fletcher had 1 pieces of work with a total of 41616 characters of text\n",
      "florus had 1 pieces of work with a total of 188724 characters of text\n",
      "foedusaeternum had 1 pieces of work with a total of 3611 characters of text\n",
      "forsett had 1 pieces of work with a total of 33474 characters of text\n",
      "fortunat had 1 pieces of work with a total of 703 characters of text\n",
      "fragmentumlaurentianum had 1 pieces of work with a total of 6067 characters of text\n",
      "fredegarius had 1 pieces of work with a total of 168136 characters of text\n",
      "frodebertus had 1 pieces of work with a total of 6475 characters of text\n",
      "frontinus had 1 pieces of work with a total of 2672 characters of text\n",
      "fronto had 1 pieces of work with a total of 339779 characters of text\n",
      "fulbert had 1 pieces of work with a total of 760 characters of text\n",
      "fulgentius had 1 pieces of work with a total of 37330 characters of text\n",
      "gaius had 1 pieces of work with a total of 87407 characters of text\n",
      "galileo had 1 pieces of work with a total of 71985 characters of text\n",
      "garcilaso had 1 pieces of work with a total of 6652 characters of text\n",
      "garland had 1 pieces of work with a total of 21499 characters of text\n",
      "gaud had 1 pieces of work with a total of 811 characters of text\n",
      "gauss had 1 pieces of work with a total of 66500 characters of text\n",
      "gellius had 21 pieces of work with a total of 812717 characters of text\n",
      "germanicus had 1 pieces of work with a total of 33114 characters of text\n",
      "gestafrancorum had 1 pieces of work with a total of 8247 characters of text\n",
      "gestarom had 1 pieces of work with a total of 103434 characters of text\n",
      "gioacchino had 1 pieces of work with a total of 136901 characters of text\n",
      "girolamoaccelini had 2 pieces of work with a total of 218784 characters of text\n",
      "girolamoamaseo had 1 pieces of work with a total of 16479 characters of text\n",
      "glass had 1 pieces of work with a total of 190152 characters of text\n",
      "godfrey had 1 pieces of work with a total of 9819 characters of text\n",
      "grattius had 1 pieces of work with a total of 24829 characters of text\n",
      "gravissimas had 1 pieces of work with a total of 13221 characters of text\n",
      "greg had 1 pieces of work with a total of 5508 characters of text\n",
      "gregdecretals had 1 pieces of work with a total of 221258 characters of text\n",
      "gregory had 1 pieces of work with a total of 119429 characters of text\n",
      "gregorytours had 1 pieces of work with a total of 56963 characters of text\n",
      "gwinne had 1 pieces of work with a total of 42361 characters of text\n",
      "halley had 1 pieces of work with a total of 2243 characters of text\n",
      "hebet had 1 pieces of work with a total of 660 characters of text\n",
      "henry had 1 pieces of work with a total of 8743 characters of text\n",
      "henrysettimello had 1 pieces of work with a total of 42703 characters of text\n",
      "hipp had 1 pieces of work with a total of 13938 characters of text\n",
      "histapoll had 1 pieces of work with a total of 79949 characters of text\n",
      "histbrit had 1 pieces of work with a total of 64287 characters of text\n",
      "holberg had 1 pieces of work with a total of 208989 characters of text\n",
      "horace had 10 pieces of work with a total of 297826 characters of text\n",
      "hrabanus had 1 pieces of work with a total of 1155 characters of text\n",
      "hugo had 1 pieces of work with a total of 37306 characters of text\n",
      "hydatiuschronicon had 1 pieces of work with a total of 40888 characters of text\n",
      "hydatiusfasti had 1 pieces of work with a total of 45910 characters of text\n",
      "hyginus had 1 pieces of work with a total of 22784 characters of text\n",
      "hymni had 1 pieces of work with a total of 10922 characters of text\n",
      "iabervocius had 1 pieces of work with a total of 4204 characters of text\n",
      "iamdulcis had 1 pieces of work with a total of 1239 characters of text\n",
      "ilias had 1 pieces of work with a total of 47469 characters of text\n",
      "index had 1 pieces of work with a total of 1604 characters of text\n",
      "indices had 1 pieces of work with a total of 1860 characters of text\n",
      "innocent had 1 pieces of work with a total of 101995 characters of text\n",
      "inquisitio had 1 pieces of work with a total of 28586 characters of text\n",
      "inscriptions had 1 pieces of work with a total of 5249 characters of text\n",
      "iordanes had 1 pieces of work with a total of 131914 characters of text\n",
      "ipsavivere had 1 pieces of work with a total of 532 characters of text\n",
      "isidore had 1 pieces of work with a total of 142216 characters of text\n",
      "italicus had 1 pieces of work with a total of 51727 characters of text\n",
      "jacopoallegretti had 1 pieces of work with a total of 4595 characters of text\n",
      "janus had 1 pieces of work with a total of 9758 characters of text\n",
      "jerome had 80 pieces of work with a total of 6201228 characters of text\n",
      "jfkhonor had 1 pieces of work with a total of 1925 characters of text\n",
      "johannes had 1 pieces of work with a total of 81855 characters of text\n",
      "junillus had 1 pieces of work with a total of 71795 characters of text\n",
      "justin had 1 pieces of work with a total of 7502 characters of text\n",
      "justinian had 1 pieces of work with a total of 128922 characters of text\n",
      "juvenal had 1 pieces of work with a total of 170859 characters of text\n",
      "juvencus had 4 pieces of work with a total of 144617 characters of text\n",
      "kalila had 1 pieces of work with a total of 197267 characters of text\n",
      "kempis had 1 pieces of work with a total of 33416 characters of text\n",
      "kepler had 1 pieces of work with a total of 44519 characters of text\n",
      "lactantius had 1 pieces of work with a total of 79494 characters of text\n",
      "landor had 1 pieces of work with a total of 21815 characters of text\n",
      "legenda had 1 pieces of work with a total of 25365 characters of text\n",
      "leo had 1 pieces of work with a total of 38118 characters of text\n",
      "leothegreat had 1 pieces of work with a total of 7242 characters of text\n",
      "letabundus had 1 pieces of work with a total of 969 characters of text\n",
      "levis had 1 pieces of work with a total of 614 characters of text\n",
      "lhomond had 1 pieces of work with a total of 201873 characters of text\n",
      "liberpontificalis had 1 pieces of work with a total of 107856 characters of text\n",
      "livy had 4 pieces of work with a total of 3582763 characters of text\n",
      "lotichius had 1 pieces of work with a total of 921 characters of text\n",
      "lucan had 10 pieces of work with a total of 358610 characters of text\n",
      "lucernarium had 1 pieces of work with a total of 533 characters of text\n",
      "lucretius had 6 pieces of work with a total of 329870 characters of text\n",
      "luther had 1 pieces of work with a total of 5046 characters of text\n",
      "macarius had 1 pieces of work with a total of 32831 characters of text\n",
      "macrobius had 8 pieces of work with a total of 602168 characters of text\n",
      "magnacarta had 1 pieces of work with a total of 25334 characters of text\n",
      "maidstone had 1 pieces of work with a total of 12204 characters of text\n",
      "malaterra had 1 pieces of work with a total of 70560 characters of text\n",
      "manilius had 5 pieces of work with a total of 195808 characters of text\n",
      "mapps had 1 pieces of work with a total of 10434 characters of text\n",
      "marbodus had 1 pieces of work with a total of 11471 characters of text\n",
      "marcantonioaldegati had 3 pieces of work with a total of 76026 characters of text\n",
      "marcellinus had 1 pieces of work with a total of 75395 characters of text\n",
      "marcusmincuiusfelix had 1 pieces of work with a total of 82026 characters of text\n",
      "martial had 14 pieces of work with a total of 275032 characters of text\n",
      "martinbraga had 1 pieces of work with a total of 8343 characters of text\n",
      "marullo had 1 pieces of work with a total of 2388 characters of text\n",
      "marx had 1 pieces of work with a total of 6181 characters of text\n",
      "maximianus had 1 pieces of work with a total of 29420 characters of text\n",
      "may had 1 pieces of work with a total of 21142 characters of text\n",
      "melanchthon had 1 pieces of work with a total of 12561 characters of text\n",
      "milton had 1 pieces of work with a total of 10794 characters of text\n",
      "minucius had 1 pieces of work with a total of 82425 characters of text\n",
      "mirabilia had 1 pieces of work with a total of 31569 characters of text\n",
      "mirandola had 1 pieces of work with a total of 347 characters of text\n",
      "montanus had 1 pieces of work with a total of 5494 characters of text\n",
      "more had 1 pieces of work with a total of 192797 characters of text\n",
      "musavenit had 1 pieces of work with a total of 627 characters of text\n",
      "naevius had 1 pieces of work with a total of 5843 characters of text\n",
      "navagero had 1 pieces of work with a total of 45281 characters of text\n",
      "nemesianus had 1 pieces of work with a total of 3953 characters of text\n",
      "nepos had 1 pieces of work with a total of 202032 characters of text\n",
      "newton had 1 pieces of work with a total of 4183 characters of text\n",
      "nithardus had 1 pieces of work with a total of 20803 characters of text\n",
      "nobilis had 1 pieces of work with a total of 604 characters of text\n",
      "notitia had 1 pieces of work with a total of 37953 characters of text\n",
      "novatian had 1 pieces of work with a total of 135244 characters of text\n",
      "obsequens had 1 pieces of work with a total of 39105 characters of text\n",
      "omnegenus had 1 pieces of work with a total of 1155 characters of text\n",
      "oratio had 1 pieces of work with a total of 6515 characters of text\n",
      "oresmius had 1 pieces of work with a total of 68052 characters of text\n",
      "origo had 1 pieces of work with a total of 6817 characters of text\n",
      "orosius had 1 pieces of work with a total of 78171 characters of text\n",
      "ottofreising had 1 pieces of work with a total of 114755 characters of text\n",
      "ovid had 41 pieces of work with a total of 1473588 characters of text\n",
      "owen had 1 pieces of work with a total of 3059 characters of text\n",
      "paris had 1 pieces of work with a total of 4805 characters of text\n",
      "pascoli had 1 pieces of work with a total of 6222 characters of text\n",
      "passerat had 1 pieces of work with a total of 3454 characters of text\n",
      "patricius had 1 pieces of work with a total of 21936 characters of text\n",
      "pauldeacon had 1 pieces of work with a total of 13840 characters of text\n",
      "paulinus had 1 pieces of work with a total of 173954 characters of text\n",
      "paulusdiaconus had 1 pieces of work with a total of 69912 characters of text\n",
      "perp had 1 pieces of work with a total of 24115 characters of text\n",
      "persius had 1 pieces of work with a total of 29733 characters of text\n",
      "pervig had 1 pieces of work with a total of 4300 characters of text\n",
      "petrarch had 1 pieces of work with a total of 6107 characters of text\n",
      "petrarchmedicus had 1 pieces of work with a total of 151780 characters of text\n",
      "petronius had 1 pieces of work with a total of 214546 characters of text\n",
      "petroniusfrag had 1 pieces of work with a total of 7962 characters of text\n",
      "phaedr had 1 pieces of work with a total of 7033 characters of text\n",
      "phaedrapp had 1 pieces of work with a total of 18127 characters of text\n",
      "piccolomini had 1 pieces of work with a total of 4257 characters of text\n",
      "planctus had 1 pieces of work with a total of 3076 characters of text\n",
      "plautus had 20 pieces of work with a total of 1062674 characters of text\n",
      "pliny had 1 pieces of work with a total of 44148 characters of text\n",
      "plinytheelder had 7 pieces of work with a total of 2810822 characters of text\n",
      "plinytheyounger had 10 pieces of work with a total of 467614 characters of text\n",
      "poggio had 1 pieces of work with a total of 100501 characters of text\n",
      "polignac had 3 pieces of work with a total of 898418 characters of text\n",
      "pomponius had 1 pieces of work with a total of 40156 characters of text\n",
      "pontano had 1 pieces of work with a total of 5989 characters of text\n",
      "poree had 1 pieces of work with a total of 57100 characters of text\n",
      "porphyrius had 1 pieces of work with a total of 34744 characters of text\n",
      "potatores had 1 pieces of work with a total of 1022 characters of text\n",
      "prataiam had 1 pieces of work with a total of 738 characters of text\n",
      "prec had 1 pieces of work with a total of 1374 characters of text\n",
      "precatio had 1 pieces of work with a total of 899 characters of text\n",
      "priapea had 1 pieces of work with a total of 23249 characters of text\n",
      "priscian had 4 pieces of work with a total of 125385 characters of text\n",
      "professio had 1 pieces of work with a total of 8725 characters of text\n",
      "prop had 1 pieces of work with a total of 39553 characters of text\n",
      "propertius had 1 pieces of work with a total of 164271 characters of text\n",
      "prosperus had 1 pieces of work with a total of 28919 characters of text\n",
      "protospatarius had 1 pieces of work with a total of 30329 characters of text\n",
      "prudentius had 7 pieces of work with a total of 238798 characters of text\n",
      "pseudocicero had 1 pieces of work with a total of 12256 characters of text\n",
      "pseudoquintilian had 1 pieces of work with a total of 475491 characters of text\n",
      "psplato had 1 pieces of work with a total of 8802 characters of text\n",
      "pulchracomis had 1 pieces of work with a total of 217 characters of text\n",
      "qcicero had 1 pieces of work with a total of 28874 characters of text\n",
      "quintilian had 12 pieces of work with a total of 1183641 characters of text\n",
      "quum had 1 pieces of work with a total of 2281 characters of text\n",
      "raoul had 1 pieces of work with a total of 268357 characters of text\n",
      "regula had 1 pieces of work with a total of 40185 characters of text\n",
      "reposianus had 1 pieces of work with a total of 8568 characters of text\n",
      "resgestae had 1 pieces of work with a total of 18532 characters of text\n",
      "rhetores had 1 pieces of work with a total of 678 characters of text\n",
      "richerus had 1 pieces of work with a total of 88135 characters of text\n",
      "rimbaud had 1 pieces of work with a total of 2988 characters of text\n",
      "ruaeus had 1 pieces of work with a total of 17523 characters of text\n",
      "rumor had 1 pieces of work with a total of 943 characters of text\n",
      "rutilius had 1 pieces of work with a total of 29644 characters of text\n",
      "rutiliuslupus had 1 pieces of work with a total of 31477 characters of text\n",
      "sabinus had 1 pieces of work with a total of 4623 characters of text\n",
      "sall had 1 pieces of work with a total of 150818 characters of text\n",
      "sallust had 3 pieces of work with a total of 254194 characters of text\n",
      "sannazaro had 1 pieces of work with a total of 65699 characters of text\n",
      "scaliger had 1 pieces of work with a total of 294 characters of text\n",
      "scbaccanalibus had 1 pieces of work with a total of 4922 characters of text\n",
      "scottus had 1 pieces of work with a total of 1589 characters of text\n",
      "scriptoreshistoriaeaugustae had 4 pieces of work with a total of 527723 characters of text\n",
      "sedulius had 1 pieces of work with a total of 14042 characters of text\n",
      "sen had 1 pieces of work with a total of 50841 characters of text\n",
      "seneca had 42 pieces of work with a total of 2155204 characters of text\n",
      "senecatheelder had 8 pieces of work with a total of 666264 characters of text\n",
      "septsap had 1 pieces of work with a total of 49331 characters of text\n",
      "serviushonoratus had 4 pieces of work with a total of 355299 characters of text\n",
      "sha had 1 pieces of work with a total of 40376 characters of text\n",
      "sicmeafata had 1 pieces of work with a total of 798 characters of text\n",
      "sidonius had 1 pieces of work with a total of 19281 characters of text\n",
      "sigebert had 1 pieces of work with a total of 10679 characters of text\n",
      "silius had 1 pieces of work with a total of 41084 characters of text\n",
      "siliusitalicus had 17 pieces of work with a total of 538363 characters of text\n",
      "simedignetur had 1 pieces of work with a total of 308 characters of text\n",
      "smarius had 1 pieces of work with a total of 6792 characters of text\n",
      "solet had 1 pieces of work with a total of 11763 characters of text\n",
      "solinus had 1 pieces of work with a total of 61199 characters of text\n",
      "spinoza had 1 pieces of work with a total of 91182 characters of text\n",
      "statius had 18 pieces of work with a total of 656190 characters of text\n",
      "suetonius had 12 pieces of work with a total of 518934 characters of text\n",
      "sulpicia had 1 pieces of work with a total of 1697 characters of text\n",
      "sulpiciusseveruschron had 1 pieces of work with a total of 92885 characters of text\n",
      "sulpiciusseverusmartin had 1 pieces of work with a total of 48368 characters of text\n",
      "suscipeflos had 1 pieces of work with a total of 417 characters of text\n",
      "syrus had 1 pieces of work with a total of 56263 characters of text\n",
      "tacitus had 20 pieces of work with a total of 1204956 characters of text\n",
      "tempusest had 1 pieces of work with a total of 1210 characters of text\n",
      "ter had 1 pieces of work with a total of 49625 characters of text\n",
      "terence had 6 pieces of work with a total of 310741 characters of text\n",
      "terraiam had 1 pieces of work with a total of 1107 characters of text\n",
      "tertullian had 2 pieces of work with a total of 186130 characters of text\n",
      "testamentum had 1 pieces of work with a total of 2192 characters of text\n",
      "tevigilans had 1 pieces of work with a total of 351 characters of text\n",
      "theganus had 1 pieces of work with a total of 45690 characters of text\n",
      "theodolus had 1 pieces of work with a total of 16262 characters of text\n",
      "theodosius had 1 pieces of work with a total of 76649 characters of text\n",
      "theophanes had 1 pieces of work with a total of 1930 characters of text\n",
      "thesauro had 1 pieces of work with a total of 12371 characters of text\n",
      "thomasedessa had 1 pieces of work with a total of 73511 characters of text\n",
      "tibullus had 1 pieces of work with a total of 80686 characters of text\n",
      "tunger had 1 pieces of work with a total of 89239 characters of text\n",
      "valeriusflaccus had 8 pieces of work with a total of 278759 characters of text\n",
      "valeriusmaximus had 9 pieces of work with a total of 586001 characters of text\n",
      "valesianus had 1 pieces of work with a total of 20987 characters of text\n",
      "valmax had 1 pieces of work with a total of 57104 characters of text\n",
      "varro had 1 pieces of work with a total of 40473 characters of text\n",
      "vegetius had 1 pieces of work with a total of 33312 characters of text\n",
      "vegius had 1 pieces of work with a total of 29868 characters of text\n",
      "vell had 1 pieces of work with a total of 167143 characters of text\n",
      "venantius had 1 pieces of work with a total of 5002 characters of text\n",
      "vergil had 17 pieces of work with a total of 579350 characters of text\n",
      "vicentius had 1 pieces of work with a total of 121964 characters of text\n",
      "vico had 1 pieces of work with a total of 21838 characters of text\n",
      "victor had 1 pieces of work with a total of 69826 characters of text\n",
      "vida had 1 pieces of work with a total of 30105 characters of text\n",
      "vitacaroli had 1 pieces of work with a total of 104331 characters of text\n",
      "vitruvius had 10 pieces of work with a total of 421054 characters of text\n",
      "volovirum had 1 pieces of work with a total of 1071 characters of text\n",
      "voragine had 1 pieces of work with a total of 3090 characters of text\n",
      "waardenburg had 1 pieces of work with a total of 3553 characters of text\n",
      "waltarius had 1 pieces of work with a total of 18263 characters of text\n",
      "walter had 1 pieces of work with a total of 2999 characters of text\n",
      "walton had 1 pieces of work with a total of 26036 characters of text\n",
      "williamapulia had 1 pieces of work with a total of 123908 characters of text\n",
      "williamtyre had 1 pieces of work with a total of 7105 characters of text\n",
      "withof had 1 pieces of work with a total of 1564 characters of text\n",
      "wmconchesdogma had 1 pieces of work with a total of 91687 characters of text\n",
      "wmconchesphil had 1 pieces of work with a total of 39080 characters of text\n",
      "xanten had 1 pieces of work with a total of 49919 characters of text\n",
      "xylander had 1 pieces of work with a total of 106561 characters of text\n",
      "zonaras had 1 pieces of work with a total of 76548 characters of text\n"
     ]
    }
   ],
   "source": [
    "CI = dataExp.CorpusInterface(corpus_name=\"text_corpus.pickle\", shouldTokenize = False)\n",
    "text = CI.get_text_for_author(author=author,shouldShuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d885025-ab3a-4bff-9ff5-79a9c39207bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T00:23:18.766167Z",
     "iopub.status.busy": "2023-04-14T00:23:18.765593Z",
     "iopub.status.idle": "2023-04-14T00:23:18.771454Z",
     "shell.execute_reply": "2023-04-14T00:23:18.770774Z",
     "shell.execute_reply.started": "2023-04-14T00:23:18.766144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1473628\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{author}_data.txt\", \"w+\") as f:\n",
    "    f.write(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5bc7ff1-340e-4879-9359-94ece597ecea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:07:20.290539Z",
     "iopub.status.busy": "2023-05-12T13:07:20.289836Z",
     "iopub.status.idle": "2023-05-12T13:07:25.900722Z",
     "shell.execute_reply": "2023-05-12T13:07:25.900158Z",
     "shell.execute_reply.started": "2023-05-12T13:07:20.290510Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./LatinBERT/latin_bert/ were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32900, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32900, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_latin_bert = \"./LatinBERT/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_latin_bert)\n",
    "model = AutoModelForMaskedLM.from_pretrained(path_to_latin_bert+\"latin_bert/\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57214ba9-d273-4f39-b48a-342ddf442d93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T02:20:17.507285Z",
     "iopub.status.busy": "2023-03-30T02:20:17.506963Z",
     "iopub.status.idle": "2023-03-30T02:34:35.366336Z",
     "shell.execute_reply": "2023-03-30T02:34:35.365497Z",
     "shell.execute_reply.started": "2023-03-30T02:20:17.507262Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (352139 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text data\n",
    "# encoded_data = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1ee27c-3aa0-41c6-97f6-f65df3204703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T00:23:38.113131Z",
     "iopub.status.busy": "2023-04-14T00:23:38.112442Z",
     "iopub.status.idle": "2023-04-14T00:23:38.118033Z",
     "shell.execute_reply": "2023-04-14T00:23:38.117202Z",
     "shell.execute_reply.started": "2023-04-14T00:23:38.113105Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(item[0]) for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item[1]) for item in batch])\n",
    "    labels = torch.stack([torch.tensor(item[2]) for item in batch])\n",
    "\n",
    "    # Move data to the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e613b4bb-09a5-45bb-bc9d-4fc334fb8c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T00:23:41.961922Z",
     "iopub.status.busy": "2023-04-14T00:23:41.961310Z",
     "iopub.status.idle": "2023-04-14T00:35:13.703862Z",
     "shell.execute_reply": "2023-04-14T00:35:13.703119Z",
     "shell.execute_reply.started": "2023-04-14T00:23:41.961896Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/data/datasets/language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (352137 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "#from multiprocessing import cpu_count\n",
    "# Define the dataset and data collator for training the model\n",
    "dataset = TextDataset(file_path=f\"{author}_data.txt\", tokenizer=tokenizer, block_size=256)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.35)\n",
    "\n",
    "#num_workers = cpu_count()\n",
    "#print(num_workers)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf68eb1-dda6-463b-9560-110cc62ecd97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T00:35:13.706156Z",
     "iopub.status.busy": "2023-04-14T00:35:13.705156Z",
     "iopub.status.idle": "2023-04-14T00:35:13.722295Z",
     "shell.execute_reply": "2023-04-14T00:35:13.721728Z",
     "shell.execute_reply.started": "2023-04-14T00:35:13.706131Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_object(obj, filename):\n",
    "    with open(filename, \"wb\") as outp:\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "save_object(train_dataloader, f\"{author}_dataset.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db139bf-7fa8-4ed9-9a9d-e1cb39616cbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T00:35:26.659650Z",
     "iopub.status.busy": "2023-04-14T00:35:26.658819Z",
     "iopub.status.idle": "2023-04-14T00:35:26.663552Z",
     "shell.execute_reply": "2023-04-14T00:35:26.662999Z",
     "shell.execute_reply.started": "2023-04-14T00:35:26.659624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdebbe4d-4c0e-4c22-ba66-b612329ca0a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T00:35:29.097999Z",
     "iopub.status.busy": "2023-04-14T00:35:29.096893Z",
     "iopub.status.idle": "2023-04-14T02:47:08.930025Z",
     "shell.execute_reply": "2023-04-14T02:47:08.929241Z",
     "shell.execute_reply.started": "2023-04-14T00:35:29.097963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Step [20/174]: Train loss = 3.2789806723594666\n",
      "Epoch [1/200] Step [40/174]: Train loss = 2.9643692851066588\n",
      "Epoch [1/200] Step [60/174]: Train loss = 2.926290047168732\n",
      "Epoch [1/200] Step [80/174]: Train loss = 2.961133122444153\n",
      "Epoch [1/200] Step [100/174]: Train loss = 2.9426745772361755\n",
      "Epoch [1/200] Step [120/174]: Train loss = 2.905635643005371\n",
      "Epoch [1/200] Step [140/174]: Train loss = 2.774307036399841\n",
      "Epoch [1/200] Step [160/174]: Train loss = 2.8412150859832765\n",
      "Epoch [2/200] Step [20/174]: Train loss = 2.785649263858795\n",
      "Epoch [2/200] Step [40/174]: Train loss = 2.7073137521743775\n",
      "Epoch [2/200] Step [60/174]: Train loss = 2.7949732899665833\n",
      "Epoch [2/200] Step [80/174]: Train loss = 2.8498963594436644\n",
      "Epoch [2/200] Step [100/174]: Train loss = 2.8285470962524415\n",
      "Epoch [2/200] Step [120/174]: Train loss = 2.8460800290107726\n",
      "Epoch [2/200] Step [140/174]: Train loss = 2.701103377342224\n",
      "Epoch [2/200] Step [160/174]: Train loss = 2.786465620994568\n",
      "Epoch [3/200] Step [20/174]: Train loss = 2.7302573561668395\n",
      "Epoch [3/200] Step [40/174]: Train loss = 2.6452940821647646\n",
      "Epoch [3/200] Step [60/174]: Train loss = 2.7514673113822936\n",
      "Epoch [3/200] Step [80/174]: Train loss = 2.772434139251709\n",
      "Epoch [3/200] Step [100/174]: Train loss = 2.7236242413520815\n",
      "Epoch [3/200] Step [120/174]: Train loss = 2.7302927017211913\n",
      "Epoch [3/200] Step [140/174]: Train loss = 2.6287145137786867\n",
      "Epoch [3/200] Step [160/174]: Train loss = 2.7026825189590453\n",
      "Epoch [4/200] Step [20/174]: Train loss = 2.6869428038597105\n",
      "Epoch [4/200] Step [40/174]: Train loss = 2.579669737815857\n",
      "Epoch [4/200] Step [60/174]: Train loss = 2.6377797603607176\n",
      "Epoch [4/200] Step [80/174]: Train loss = 2.7230280518531798\n",
      "Epoch [4/200] Step [100/174]: Train loss = 2.7036593198776244\n",
      "Epoch [4/200] Step [120/174]: Train loss = 2.6959808707237243\n",
      "Epoch [4/200] Step [140/174]: Train loss = 2.5460439205169676\n",
      "Epoch [4/200] Step [160/174]: Train loss = 2.598141443729401\n",
      "Epoch [5/200] Step [20/174]: Train loss = 2.6602882027626036\n",
      "Epoch [5/200] Step [40/174]: Train loss = 2.6226170659065247\n",
      "Epoch [5/200] Step [60/174]: Train loss = 2.554038643836975\n",
      "Epoch [5/200] Step [80/174]: Train loss = 2.6504804372787474\n",
      "Epoch [5/200] Step [100/174]: Train loss = 2.6269070982933043\n",
      "Epoch [5/200] Step [120/174]: Train loss = 2.602659595012665\n",
      "Epoch [5/200] Step [140/174]: Train loss = 2.4956178426742555\n",
      "Epoch [5/200] Step [160/174]: Train loss = 2.5078721761703493\n",
      "Epoch [6/200] Step [20/174]: Train loss = 2.589661741256714\n",
      "Epoch [6/200] Step [40/174]: Train loss = 2.5307848334312437\n",
      "Epoch [6/200] Step [60/174]: Train loss = 2.534842336177826\n",
      "Epoch [6/200] Step [80/174]: Train loss = 2.543490242958069\n",
      "Epoch [6/200] Step [100/174]: Train loss = 2.555380606651306\n",
      "Epoch [6/200] Step [120/174]: Train loss = 2.5243367552757263\n",
      "Epoch [6/200] Step [140/174]: Train loss = 2.434780311584473\n",
      "Epoch [6/200] Step [160/174]: Train loss = 2.480631709098816\n",
      "Epoch [7/200] Step [20/174]: Train loss = 2.52474205493927\n",
      "Epoch [7/200] Step [40/174]: Train loss = 2.394958472251892\n",
      "Epoch [7/200] Step [60/174]: Train loss = 2.4737515211105348\n",
      "Epoch [7/200] Step [80/174]: Train loss = 2.516810727119446\n",
      "Epoch [7/200] Step [100/174]: Train loss = 2.431186091899872\n",
      "Epoch [7/200] Step [120/174]: Train loss = 2.4946974515914917\n",
      "Epoch [7/200] Step [140/174]: Train loss = 2.3762027740478517\n",
      "Epoch [7/200] Step [160/174]: Train loss = 2.423420476913452\n",
      "Epoch [8/200] Step [20/174]: Train loss = 2.4983567595481873\n",
      "Epoch [8/200] Step [40/174]: Train loss = 2.4220425486564636\n",
      "Epoch [8/200] Step [60/174]: Train loss = 2.418316388130188\n",
      "Epoch [8/200] Step [80/174]: Train loss = 2.435024690628052\n",
      "Epoch [8/200] Step [100/174]: Train loss = 2.4526495218276976\n",
      "Epoch [8/200] Step [120/174]: Train loss = 2.4790574431419374\n",
      "Epoch [8/200] Step [140/174]: Train loss = 2.348264253139496\n",
      "Epoch [8/200] Step [160/174]: Train loss = 2.3932830452919007\n",
      "Epoch [9/200] Step [20/174]: Train loss = 2.4761961460113526\n",
      "Epoch [9/200] Step [40/174]: Train loss = 2.374747395515442\n",
      "Epoch [9/200] Step [60/174]: Train loss = 2.3980476379394533\n",
      "Epoch [9/200] Step [80/174]: Train loss = 2.44974889755249\n",
      "Epoch [9/200] Step [100/174]: Train loss = 2.3919538617134095\n",
      "Epoch [9/200] Step [120/174]: Train loss = 2.403684175014496\n",
      "Epoch [9/200] Step [140/174]: Train loss = 2.286431574821472\n",
      "Epoch [9/200] Step [160/174]: Train loss = 2.3740617632865906\n",
      "Epoch [10/200] Step [20/174]: Train loss = 2.343262809514999\n",
      "Epoch [10/200] Step [40/174]: Train loss = 2.3427896738052367\n",
      "Epoch [10/200] Step [60/174]: Train loss = 2.291953128576279\n",
      "Epoch [10/200] Step [80/174]: Train loss = 2.3783323884010317\n",
      "Epoch [10/200] Step [100/174]: Train loss = 2.4156522154808044\n",
      "Epoch [10/200] Step [120/174]: Train loss = 2.4062079071998594\n",
      "Epoch [10/200] Step [140/174]: Train loss = 2.228028917312622\n",
      "Epoch [10/200] Step [160/174]: Train loss = 2.3672016143798826\n",
      "Epoch [11/200] Step [20/174]: Train loss = 2.375114357471466\n",
      "Epoch [11/200] Step [40/174]: Train loss = 2.28120738863945\n",
      "Epoch [11/200] Step [60/174]: Train loss = 2.3125499725341796\n",
      "Epoch [11/200] Step [80/174]: Train loss = 2.339468514919281\n",
      "Epoch [11/200] Step [100/174]: Train loss = 2.339581859111786\n",
      "Epoch [11/200] Step [120/174]: Train loss = 2.321711874008179\n",
      "Epoch [11/200] Step [140/174]: Train loss = 2.256866681575775\n",
      "Epoch [11/200] Step [160/174]: Train loss = 2.3233898520469665\n",
      "Epoch [12/200] Step [20/174]: Train loss = 2.3196306347846987\n",
      "Epoch [12/200] Step [40/174]: Train loss = 2.2368377327919005\n",
      "Epoch [12/200] Step [60/174]: Train loss = 2.284712886810303\n",
      "Epoch [12/200] Step [80/174]: Train loss = 2.321808844804764\n",
      "Epoch [12/200] Step [100/174]: Train loss = 2.2642866730690003\n",
      "Epoch [12/200] Step [120/174]: Train loss = 2.290176260471344\n",
      "Epoch [12/200] Step [140/174]: Train loss = 2.2018221855163573\n",
      "Epoch [12/200] Step [160/174]: Train loss = 2.289758974313736\n",
      "Epoch [13/200] Step [20/174]: Train loss = 2.3000285983085633\n",
      "Epoch [13/200] Step [40/174]: Train loss = 2.189625871181488\n",
      "Epoch [13/200] Step [60/174]: Train loss = 2.2571032464504244\n",
      "Epoch [13/200] Step [80/174]: Train loss = 2.2890757083892823\n",
      "Epoch [13/200] Step [100/174]: Train loss = 2.2724569857120516\n",
      "Epoch [13/200] Step [120/174]: Train loss = 2.2482083797454835\n",
      "Epoch [13/200] Step [140/174]: Train loss = 2.1697136402130126\n",
      "Epoch [13/200] Step [160/174]: Train loss = 2.2605648279190063\n",
      "Epoch [14/200] Step [20/174]: Train loss = 2.2406303107738497\n",
      "Epoch [14/200] Step [40/174]: Train loss = 2.214361256361008\n",
      "Epoch [14/200] Step [60/174]: Train loss = 2.2673709630966186\n",
      "Epoch [14/200] Step [80/174]: Train loss = 2.2561397910118104\n",
      "Epoch [14/200] Step [100/174]: Train loss = 2.2870177626609802\n",
      "Epoch [14/200] Step [120/174]: Train loss = 2.1625840604305266\n",
      "Epoch [14/200] Step [140/174]: Train loss = 2.1660552978515626\n",
      "Epoch [14/200] Step [160/174]: Train loss = 2.2317047297954558\n",
      "Epoch [15/200] Step [20/174]: Train loss = 2.207015633583069\n",
      "Epoch [15/200] Step [40/174]: Train loss = 2.147640061378479\n",
      "Epoch [15/200] Step [60/174]: Train loss = 2.15233713388443\n",
      "Epoch [15/200] Step [80/174]: Train loss = 2.162522476911545\n",
      "Epoch [15/200] Step [100/174]: Train loss = 2.1492225170135497\n",
      "Epoch [15/200] Step [120/174]: Train loss = 2.2052006244659426\n",
      "Epoch [15/200] Step [140/174]: Train loss = 2.1464584112167358\n",
      "Epoch [15/200] Step [160/174]: Train loss = 2.1628332674503326\n",
      "Epoch [16/200] Step [20/174]: Train loss = 2.2071965515613554\n",
      "Epoch [16/200] Step [40/174]: Train loss = 2.1627575874328615\n",
      "Epoch [16/200] Step [60/174]: Train loss = 2.06966889500618\n",
      "Epoch [16/200] Step [80/174]: Train loss = 2.1883036732673644\n",
      "Epoch [16/200] Step [100/174]: Train loss = 2.193824851512909\n",
      "Epoch [16/200] Step [120/174]: Train loss = 2.1301524162292482\n",
      "Epoch [16/200] Step [140/174]: Train loss = 2.083630394935608\n",
      "Epoch [16/200] Step [160/174]: Train loss = 2.1467476367950438\n",
      "Epoch [17/200] Step [20/174]: Train loss = 2.2014121651649474\n",
      "Epoch [17/200] Step [40/174]: Train loss = 2.071552014350891\n",
      "Epoch [17/200] Step [60/174]: Train loss = 2.114330530166626\n",
      "Epoch [17/200] Step [80/174]: Train loss = 2.09667022228241\n",
      "Epoch [17/200] Step [100/174]: Train loss = 2.150418680906296\n",
      "Epoch [17/200] Step [120/174]: Train loss = 2.1109585106372832\n",
      "Epoch [17/200] Step [140/174]: Train loss = 2.079084652662277\n",
      "Epoch [17/200] Step [160/174]: Train loss = 2.089378368854523\n",
      "Epoch [18/200] Step [20/174]: Train loss = 2.1251091778278353\n",
      "Epoch [18/200] Step [40/174]: Train loss = 2.093289840221405\n",
      "Epoch [18/200] Step [60/174]: Train loss = 2.1456131279468535\n",
      "Epoch [18/200] Step [80/174]: Train loss = 2.1424938082695006\n",
      "Epoch [18/200] Step [100/174]: Train loss = 2.1219141125679015\n",
      "Epoch [18/200] Step [120/174]: Train loss = 2.0955925285816193\n",
      "Epoch [18/200] Step [140/174]: Train loss = 1.9986093521118165\n",
      "Epoch [18/200] Step [160/174]: Train loss = 2.0913372695446015\n",
      "Epoch [19/200] Step [20/174]: Train loss = 2.11055783033371\n",
      "Epoch [19/200] Step [40/174]: Train loss = 2.000683933496475\n",
      "Epoch [19/200] Step [60/174]: Train loss = 2.0120650053024294\n",
      "Epoch [19/200] Step [80/174]: Train loss = 2.0901153445243836\n",
      "Epoch [19/200] Step [100/174]: Train loss = 2.056764680147171\n",
      "Epoch [19/200] Step [120/174]: Train loss = 2.100643348693848\n",
      "Epoch [19/200] Step [140/174]: Train loss = 1.9422035217285156\n",
      "Epoch [19/200] Step [160/174]: Train loss = 2.0470947563648223\n",
      "Epoch [20/200] Step [20/174]: Train loss = 2.146125018596649\n",
      "Epoch [20/200] Step [40/174]: Train loss = 2.0181589901447294\n",
      "Epoch [20/200] Step [60/174]: Train loss = 2.0351991653442383\n",
      "Epoch [20/200] Step [80/174]: Train loss = 2.020330232381821\n",
      "Epoch [20/200] Step [100/174]: Train loss = 2.013444173336029\n",
      "Epoch [20/200] Step [120/174]: Train loss = 2.0411444544792174\n",
      "Epoch [20/200] Step [140/174]: Train loss = 1.9195709764957427\n",
      "Epoch [20/200] Step [160/174]: Train loss = 1.9589983105659485\n",
      "Epoch [21/200] Step [20/174]: Train loss = 2.095475846529007\n",
      "Epoch [21/200] Step [40/174]: Train loss = 1.9993191301822661\n",
      "Epoch [21/200] Step [60/174]: Train loss = 1.9611258387565613\n",
      "Epoch [21/200] Step [80/174]: Train loss = 2.0606376349925997\n",
      "Epoch [21/200] Step [100/174]: Train loss = 2.0204258620738984\n",
      "Epoch [21/200] Step [120/174]: Train loss = 2.03248992562294\n",
      "Epoch [21/200] Step [140/174]: Train loss = 1.9529416799545287\n",
      "Epoch [21/200] Step [160/174]: Train loss = 2.0366439402103422\n",
      "Epoch [22/200] Step [20/174]: Train loss = 2.077545922994614\n",
      "Epoch [22/200] Step [40/174]: Train loss = 1.9710088670253754\n",
      "Epoch [22/200] Step [60/174]: Train loss = 2.000446003675461\n",
      "Epoch [22/200] Step [80/174]: Train loss = 2.0294287145137786\n",
      "Epoch [22/200] Step [100/174]: Train loss = 2.0110834419727324\n",
      "Epoch [22/200] Step [120/174]: Train loss = 1.960864508152008\n",
      "Epoch [22/200] Step [140/174]: Train loss = 1.8933434009552002\n",
      "Epoch [22/200] Step [160/174]: Train loss = 2.0326794385910034\n",
      "Epoch [23/200] Step [20/174]: Train loss = 1.9683733463287354\n",
      "Epoch [23/200] Step [40/174]: Train loss = 1.9313436448574066\n",
      "Epoch [23/200] Step [60/174]: Train loss = 1.9574847400188446\n",
      "Epoch [23/200] Step [80/174]: Train loss = 1.9491311132907867\n",
      "Epoch [23/200] Step [100/174]: Train loss = 1.9775922298431396\n",
      "Epoch [23/200] Step [120/174]: Train loss = 1.9889471888542176\n",
      "Epoch [23/200] Step [140/174]: Train loss = 1.8746522188186645\n",
      "Epoch [23/200] Step [160/174]: Train loss = 1.926493126153946\n",
      "Epoch [24/200] Step [20/174]: Train loss = 1.9726053655147553\n",
      "Epoch [24/200] Step [40/174]: Train loss = 1.9234017193317414\n",
      "Epoch [24/200] Step [60/174]: Train loss = 1.9866716861724854\n",
      "Epoch [24/200] Step [80/174]: Train loss = 1.9397607743740082\n",
      "Epoch [24/200] Step [100/174]: Train loss = 1.887360805273056\n",
      "Epoch [24/200] Step [120/174]: Train loss = 1.9798909544944763\n",
      "Epoch [24/200] Step [140/174]: Train loss = 1.8341367185115813\n",
      "Epoch [24/200] Step [160/174]: Train loss = 1.900088256597519\n",
      "Epoch [25/200] Step [20/174]: Train loss = 1.9394918441772462\n",
      "Epoch [25/200] Step [40/174]: Train loss = 1.9432661831378937\n",
      "Epoch [25/200] Step [60/174]: Train loss = 1.9023570537567138\n",
      "Epoch [25/200] Step [80/174]: Train loss = 1.9273336708545685\n",
      "Epoch [25/200] Step [100/174]: Train loss = 1.9531256437301636\n",
      "Epoch [25/200] Step [120/174]: Train loss = 1.893826425075531\n",
      "Epoch [25/200] Step [140/174]: Train loss = 1.8528442978858948\n",
      "Epoch [25/200] Step [160/174]: Train loss = 1.9099673449993133\n",
      "Epoch [26/200] Step [20/174]: Train loss = 1.9433201253414154\n",
      "Epoch [26/200] Step [40/174]: Train loss = 1.8357413351535796\n",
      "Epoch [26/200] Step [60/174]: Train loss = 1.8804638028144836\n",
      "Epoch [26/200] Step [80/174]: Train loss = 1.8983336865901947\n",
      "Epoch [26/200] Step [100/174]: Train loss = 1.8930655121803284\n",
      "Epoch [26/200] Step [120/174]: Train loss = 1.8709771096706391\n",
      "Epoch [26/200] Step [140/174]: Train loss = 1.7907254874706269\n",
      "Epoch [26/200] Step [160/174]: Train loss = 1.8436416685581207\n",
      "Epoch [27/200] Step [20/174]: Train loss = 1.8554104685783386\n",
      "Epoch [27/200] Step [40/174]: Train loss = 1.8192637145519257\n",
      "Epoch [27/200] Step [60/174]: Train loss = 1.8796316385269165\n",
      "Epoch [27/200] Step [80/174]: Train loss = 1.8242111146450042\n",
      "Epoch [27/200] Step [100/174]: Train loss = 1.8555750608444215\n",
      "Epoch [27/200] Step [120/174]: Train loss = 1.8558730900287628\n",
      "Epoch [27/200] Step [140/174]: Train loss = 1.7726506531238555\n",
      "Epoch [27/200] Step [160/174]: Train loss = 1.8290810346603394\n",
      "Epoch [28/200] Step [20/174]: Train loss = 1.8629808008670807\n",
      "Epoch [28/200] Step [40/174]: Train loss = 1.811126172542572\n",
      "Epoch [28/200] Step [60/174]: Train loss = 1.8357300937175751\n",
      "Epoch [28/200] Step [80/174]: Train loss = 1.831137979030609\n",
      "Epoch [28/200] Step [100/174]: Train loss = 1.8930976867675782\n",
      "Epoch [28/200] Step [120/174]: Train loss = 1.8635042488574982\n",
      "Epoch [28/200] Step [140/174]: Train loss = 1.7230589807033538\n",
      "Epoch [28/200] Step [160/174]: Train loss = 1.8219406723976135\n",
      "Epoch [29/200] Step [20/174]: Train loss = 1.8392692685127259\n",
      "Epoch [29/200] Step [40/174]: Train loss = 1.7999218225479126\n",
      "Epoch [29/200] Step [60/174]: Train loss = 1.7682537674903869\n",
      "Epoch [29/200] Step [80/174]: Train loss = 1.8889407813549042\n",
      "Epoch [29/200] Step [100/174]: Train loss = 1.797550803422928\n",
      "Epoch [29/200] Step [120/174]: Train loss = 1.791230136156082\n",
      "Epoch [29/200] Step [140/174]: Train loss = 1.7045736014842987\n",
      "Epoch [29/200] Step [160/174]: Train loss = 1.771761989593506\n",
      "Epoch [30/200] Step [20/174]: Train loss = 1.823685073852539\n",
      "Epoch [30/200] Step [40/174]: Train loss = 1.74290491938591\n",
      "Epoch [30/200] Step [60/174]: Train loss = 1.7778188586235046\n",
      "Epoch [30/200] Step [80/174]: Train loss = 1.7750894010066987\n",
      "Epoch [30/200] Step [100/174]: Train loss = 1.7896177887916564\n",
      "Epoch [30/200] Step [120/174]: Train loss = 1.7717089116573335\n",
      "Epoch [30/200] Step [140/174]: Train loss = 1.7337860107421874\n",
      "Epoch [30/200] Step [160/174]: Train loss = 1.7912198841571807\n",
      "Epoch [31/200] Step [20/174]: Train loss = 1.841064214706421\n",
      "Epoch [31/200] Step [40/174]: Train loss = 1.8066118001937865\n",
      "Epoch [31/200] Step [60/174]: Train loss = 1.7490907430648803\n",
      "Epoch [31/200] Step [80/174]: Train loss = 1.7496448874473571\n",
      "Epoch [31/200] Step [100/174]: Train loss = 1.8041314125061034\n",
      "Epoch [31/200] Step [120/174]: Train loss = 1.7615957379341125\n",
      "Epoch [31/200] Step [140/174]: Train loss = 1.7504451751708985\n",
      "Epoch [31/200] Step [160/174]: Train loss = 1.795383447408676\n",
      "Epoch [32/200] Step [20/174]: Train loss = 1.7220762491226196\n",
      "Epoch [32/200] Step [40/174]: Train loss = 1.7682342052459716\n",
      "Epoch [32/200] Step [60/174]: Train loss = 1.733051860332489\n",
      "Epoch [32/200] Step [80/174]: Train loss = 1.7261535584926606\n",
      "Epoch [32/200] Step [100/174]: Train loss = 1.7748258054256438\n",
      "Epoch [32/200] Step [120/174]: Train loss = 1.7108648777008058\n",
      "Epoch [32/200] Step [140/174]: Train loss = 1.6915835797786714\n",
      "Epoch [32/200] Step [160/174]: Train loss = 1.7023363351821899\n",
      "Epoch [33/200] Step [20/174]: Train loss = 1.7278396964073182\n",
      "Epoch [33/200] Step [40/174]: Train loss = 1.7381609797477722\n",
      "Epoch [33/200] Step [60/174]: Train loss = 1.7456842541694642\n",
      "Epoch [33/200] Step [80/174]: Train loss = 1.7422288417816163\n",
      "Epoch [33/200] Step [100/174]: Train loss = 1.7141937911510468\n",
      "Epoch [33/200] Step [120/174]: Train loss = 1.7517756998538971\n",
      "Epoch [33/200] Step [140/174]: Train loss = 1.6617107152938844\n",
      "Epoch [33/200] Step [160/174]: Train loss = 1.6924513936042787\n",
      "Epoch [34/200] Step [20/174]: Train loss = 1.6993436217308044\n",
      "Epoch [34/200] Step [40/174]: Train loss = 1.719848245382309\n",
      "Epoch [34/200] Step [60/174]: Train loss = 1.7015329718589782\n",
      "Epoch [34/200] Step [80/174]: Train loss = 1.7348267495632173\n",
      "Epoch [34/200] Step [100/174]: Train loss = 1.696238821744919\n",
      "Epoch [34/200] Step [120/174]: Train loss = 1.6670052111148834\n",
      "Epoch [34/200] Step [140/174]: Train loss = 1.6551005721092225\n",
      "Epoch [34/200] Step [160/174]: Train loss = 1.7787093222141266\n",
      "Epoch [35/200] Step [20/174]: Train loss = 1.7166312515735627\n",
      "Epoch [35/200] Step [40/174]: Train loss = 1.6201061725616455\n",
      "Epoch [35/200] Step [60/174]: Train loss = 1.7246251046657561\n",
      "Epoch [35/200] Step [80/174]: Train loss = 1.7268779158592225\n",
      "Epoch [35/200] Step [100/174]: Train loss = 1.7302753627300262\n",
      "Epoch [35/200] Step [120/174]: Train loss = 1.6491416096687317\n",
      "Epoch [35/200] Step [140/174]: Train loss = 1.5981638133525848\n",
      "Epoch [35/200] Step [160/174]: Train loss = 1.6258853912353515\n",
      "Epoch [36/200] Step [20/174]: Train loss = 1.6780390083789825\n",
      "Epoch [36/200] Step [40/174]: Train loss = 1.6899442970752716\n",
      "Epoch [36/200] Step [60/174]: Train loss = 1.6450851023197175\n",
      "Epoch [36/200] Step [80/174]: Train loss = 1.6832233130931855\n",
      "Epoch [36/200] Step [100/174]: Train loss = 1.7268016993999482\n",
      "Epoch [36/200] Step [120/174]: Train loss = 1.6849712073802947\n",
      "Epoch [36/200] Step [140/174]: Train loss = 1.6299293220043183\n",
      "Epoch [36/200] Step [160/174]: Train loss = 1.6628120481967925\n",
      "Epoch [37/200] Step [20/174]: Train loss = 1.6734435975551605\n",
      "Epoch [37/200] Step [40/174]: Train loss = 1.6374249517917634\n",
      "Epoch [37/200] Step [60/174]: Train loss = 1.6491295754909516\n",
      "Epoch [37/200] Step [80/174]: Train loss = 1.6692834377288819\n",
      "Epoch [37/200] Step [100/174]: Train loss = 1.7108501255512238\n",
      "Epoch [37/200] Step [120/174]: Train loss = 1.60368971824646\n",
      "Epoch [37/200] Step [140/174]: Train loss = 1.608894944190979\n",
      "Epoch [37/200] Step [160/174]: Train loss = 1.601320505142212\n",
      "Epoch [38/200] Step [20/174]: Train loss = 1.660015445947647\n",
      "Epoch [38/200] Step [40/174]: Train loss = 1.6588142573833466\n",
      "Epoch [38/200] Step [60/174]: Train loss = 1.5686142086982726\n",
      "Epoch [38/200] Step [80/174]: Train loss = 1.6534287750720977\n",
      "Epoch [38/200] Step [100/174]: Train loss = 1.6172988831996917\n",
      "Epoch [38/200] Step [120/174]: Train loss = 1.6108112514019013\n",
      "Epoch [38/200] Step [140/174]: Train loss = 1.535278356075287\n",
      "Epoch [38/200] Step [160/174]: Train loss = 1.6211120247840882\n",
      "Epoch [39/200] Step [20/174]: Train loss = 1.6318530023097992\n",
      "Epoch [39/200] Step [40/174]: Train loss = 1.5597754120826721\n",
      "Epoch [39/200] Step [60/174]: Train loss = 1.6433942854404449\n",
      "Epoch [39/200] Step [80/174]: Train loss = 1.6585473001003266\n",
      "Epoch [39/200] Step [100/174]: Train loss = 1.6454001367092133\n",
      "Epoch [39/200] Step [120/174]: Train loss = 1.6651892721652986\n",
      "Epoch [39/200] Step [140/174]: Train loss = 1.5756158709526062\n",
      "Epoch [39/200] Step [160/174]: Train loss = 1.6386060059070586\n",
      "Epoch [40/200] Step [20/174]: Train loss = 1.587966799736023\n",
      "Epoch [40/200] Step [40/174]: Train loss = 1.5700024783611297\n",
      "Epoch [40/200] Step [60/174]: Train loss = 1.5814200699329377\n",
      "Epoch [40/200] Step [80/174]: Train loss = 1.5860207557678223\n",
      "Epoch [40/200] Step [100/174]: Train loss = 1.6104386627674103\n",
      "Epoch [40/200] Step [120/174]: Train loss = 1.5679635167121888\n",
      "Epoch [40/200] Step [140/174]: Train loss = 1.5127335727214812\n",
      "Epoch [40/200] Step [160/174]: Train loss = 1.566294014453888\n",
      "Epoch [41/200] Step [20/174]: Train loss = 1.575844395160675\n",
      "Epoch [41/200] Step [40/174]: Train loss = 1.5536826968193054\n",
      "Epoch [41/200] Step [60/174]: Train loss = 1.5756793856620788\n",
      "Epoch [41/200] Step [80/174]: Train loss = 1.5686900615692139\n",
      "Epoch [41/200] Step [100/174]: Train loss = 1.5930442810058594\n",
      "Epoch [41/200] Step [120/174]: Train loss = 1.578429639339447\n",
      "Epoch [41/200] Step [140/174]: Train loss = 1.5012969851493836\n",
      "Epoch [41/200] Step [160/174]: Train loss = 1.57244331240654\n",
      "Epoch [42/200] Step [20/174]: Train loss = 1.5990026652812959\n",
      "Epoch [42/200] Step [40/174]: Train loss = 1.5663852095603943\n",
      "Epoch [42/200] Step [60/174]: Train loss = 1.538567101955414\n",
      "Epoch [42/200] Step [80/174]: Train loss = 1.5642479479312896\n",
      "Epoch [42/200] Step [100/174]: Train loss = 1.5119625627994537\n",
      "Epoch [42/200] Step [120/174]: Train loss = 1.5158700883388518\n",
      "Epoch [42/200] Step [140/174]: Train loss = 1.4969721376895904\n",
      "Epoch [42/200] Step [160/174]: Train loss = 1.512750643491745\n",
      "Epoch [43/200] Step [20/174]: Train loss = 1.607306969165802\n",
      "Epoch [43/200] Step [40/174]: Train loss = 1.5544893980026244\n",
      "Epoch [43/200] Step [60/174]: Train loss = 1.5122328579425812\n",
      "Epoch [43/200] Step [80/174]: Train loss = 1.552227908372879\n",
      "Epoch [43/200] Step [100/174]: Train loss = 1.5359859466552734\n",
      "Epoch [43/200] Step [120/174]: Train loss = 1.5649344265460967\n",
      "Epoch [43/200] Step [140/174]: Train loss = 1.4820013105869294\n",
      "Epoch [43/200] Step [160/174]: Train loss = 1.5518764197826385\n",
      "Epoch [44/200] Step [20/174]: Train loss = 1.550952172279358\n",
      "Epoch [44/200] Step [40/174]: Train loss = 1.5060650646686553\n",
      "Epoch [44/200] Step [60/174]: Train loss = 1.4948256075382234\n",
      "Epoch [44/200] Step [80/174]: Train loss = 1.5364959061145782\n",
      "Epoch [44/200] Step [100/174]: Train loss = 1.5243465304374695\n",
      "Epoch [44/200] Step [120/174]: Train loss = 1.4986535131931304\n",
      "Epoch [44/200] Step [140/174]: Train loss = 1.4716874361038208\n",
      "Epoch [44/200] Step [160/174]: Train loss = 1.4968364834785461\n",
      "Epoch [45/200] Step [20/174]: Train loss = 1.5450649380683898\n",
      "Epoch [45/200] Step [40/174]: Train loss = 1.4853707194328307\n",
      "Epoch [45/200] Step [60/174]: Train loss = 1.496393656730652\n",
      "Epoch [45/200] Step [80/174]: Train loss = 1.494102555513382\n",
      "Epoch [45/200] Step [100/174]: Train loss = 1.466278338432312\n",
      "Epoch [45/200] Step [120/174]: Train loss = 1.4858336925506592\n",
      "Epoch [45/200] Step [140/174]: Train loss = 1.429151701927185\n",
      "Epoch [45/200] Step [160/174]: Train loss = 1.5055435955524445\n",
      "Epoch [46/200] Step [20/174]: Train loss = 1.4920938611030579\n",
      "Epoch [46/200] Step [40/174]: Train loss = 1.4939734637737274\n",
      "Epoch [46/200] Step [60/174]: Train loss = 1.4697611212730408\n",
      "Epoch [46/200] Step [80/174]: Train loss = 1.5410252392292023\n",
      "Epoch [46/200] Step [100/174]: Train loss = 1.5221135199069977\n",
      "Epoch [46/200] Step [120/174]: Train loss = 1.532457607984543\n",
      "Epoch [46/200] Step [140/174]: Train loss = 1.351627379655838\n",
      "Epoch [46/200] Step [160/174]: Train loss = 1.4830033957958222\n",
      "Epoch [47/200] Step [20/174]: Train loss = 1.5259338676929475\n",
      "Epoch [47/200] Step [40/174]: Train loss = 1.4668386936187745\n",
      "Epoch [47/200] Step [60/174]: Train loss = 1.4211229801177978\n",
      "Epoch [47/200] Step [80/174]: Train loss = 1.4539375066757203\n",
      "Epoch [47/200] Step [100/174]: Train loss = 1.5002945601940154\n",
      "Epoch [47/200] Step [120/174]: Train loss = 1.4218238055706025\n",
      "Epoch [47/200] Step [140/174]: Train loss = 1.3955983221530914\n",
      "Epoch [47/200] Step [160/174]: Train loss = 1.4784453749656676\n",
      "Epoch [48/200] Step [20/174]: Train loss = 1.4154910564422607\n",
      "Epoch [48/200] Step [40/174]: Train loss = 1.425760453939438\n",
      "Epoch [48/200] Step [60/174]: Train loss = 1.4240618705749513\n",
      "Epoch [48/200] Step [80/174]: Train loss = 1.4480731666088105\n",
      "Epoch [48/200] Step [100/174]: Train loss = 1.461576807498932\n",
      "Epoch [48/200] Step [120/174]: Train loss = 1.3774161458015441\n",
      "Epoch [48/200] Step [140/174]: Train loss = 1.39995276927948\n",
      "Epoch [48/200] Step [160/174]: Train loss = 1.4308600664138793\n",
      "Epoch [49/200] Step [20/174]: Train loss = 1.4222724795341493\n",
      "Epoch [49/200] Step [40/174]: Train loss = 1.4200035452842712\n",
      "Epoch [49/200] Step [60/174]: Train loss = 1.4147105753421783\n",
      "Epoch [49/200] Step [80/174]: Train loss = 1.4430868208408356\n",
      "Epoch [49/200] Step [100/174]: Train loss = 1.4443362712860108\n",
      "Epoch [49/200] Step [120/174]: Train loss = 1.3955546438694\n",
      "Epoch [49/200] Step [140/174]: Train loss = 1.3635339617729187\n",
      "Epoch [49/200] Step [160/174]: Train loss = 1.4445489823818207\n",
      "Epoch [50/200] Step [20/174]: Train loss = 1.440622228384018\n",
      "Epoch [50/200] Step [40/174]: Train loss = 1.4184009373188018\n",
      "Epoch [50/200] Step [60/174]: Train loss = 1.4173519492149353\n",
      "Epoch [50/200] Step [80/174]: Train loss = 1.4104610800743103\n",
      "Epoch [50/200] Step [100/174]: Train loss = 1.463745903968811\n",
      "Epoch [50/200] Step [120/174]: Train loss = 1.3945228099822997\n",
      "Epoch [50/200] Step [140/174]: Train loss = 1.325537806749344\n",
      "Epoch [50/200] Step [160/174]: Train loss = 1.421758246421814\n",
      "Epoch [51/200] Step [20/174]: Train loss = 1.4706176221370697\n",
      "Epoch [51/200] Step [40/174]: Train loss = 1.370033222436905\n",
      "Epoch [51/200] Step [60/174]: Train loss = 1.3578937292098998\n",
      "Epoch [51/200] Step [80/174]: Train loss = 1.406288480758667\n",
      "Epoch [51/200] Step [100/174]: Train loss = 1.4162871181964873\n",
      "Epoch [51/200] Step [120/174]: Train loss = 1.4198428571224213\n",
      "Epoch [51/200] Step [140/174]: Train loss = 1.3925268411636353\n",
      "Epoch [51/200] Step [160/174]: Train loss = 1.4043934524059296\n",
      "Epoch [52/200] Step [20/174]: Train loss = 1.387485110759735\n",
      "Epoch [52/200] Step [40/174]: Train loss = 1.3936492562294007\n",
      "Epoch [52/200] Step [60/174]: Train loss = 1.4021299362182618\n",
      "Epoch [52/200] Step [80/174]: Train loss = 1.3775701522827148\n",
      "Epoch [52/200] Step [100/174]: Train loss = 1.3798147022724152\n",
      "Epoch [52/200] Step [120/174]: Train loss = 1.4107742667198182\n",
      "Epoch [52/200] Step [140/174]: Train loss = 1.3575711607933045\n",
      "Epoch [52/200] Step [160/174]: Train loss = 1.4166827917098999\n",
      "Epoch [53/200] Step [20/174]: Train loss = 1.4145103096961975\n",
      "Epoch [53/200] Step [40/174]: Train loss = 1.3773999452590941\n",
      "Epoch [53/200] Step [60/174]: Train loss = 1.320548665523529\n",
      "Epoch [53/200] Step [80/174]: Train loss = 1.4070885598659515\n",
      "Epoch [53/200] Step [100/174]: Train loss = 1.4591956198215486\n",
      "Epoch [53/200] Step [120/174]: Train loss = 1.4051202833652496\n",
      "Epoch [53/200] Step [140/174]: Train loss = 1.3610463261604309\n",
      "Epoch [53/200] Step [160/174]: Train loss = 1.3680251836776733\n",
      "Epoch [54/200] Step [20/174]: Train loss = 1.4091774225234985\n",
      "Epoch [54/200] Step [40/174]: Train loss = 1.3144642472267152\n",
      "Epoch [54/200] Step [60/174]: Train loss = 1.3205279171466828\n",
      "Epoch [54/200] Step [80/174]: Train loss = 1.321885246038437\n",
      "Epoch [54/200] Step [100/174]: Train loss = 1.363286316394806\n",
      "Epoch [54/200] Step [120/174]: Train loss = 1.2977342426776886\n",
      "Epoch [54/200] Step [140/174]: Train loss = 1.3130279958248139\n",
      "Epoch [54/200] Step [160/174]: Train loss = 1.343725138902664\n",
      "Epoch [55/200] Step [20/174]: Train loss = 1.3788096964359284\n",
      "Epoch [55/200] Step [40/174]: Train loss = 1.3551842033863069\n",
      "Epoch [55/200] Step [60/174]: Train loss = 1.3348992466926575\n",
      "Epoch [55/200] Step [80/174]: Train loss = 1.3532366633415223\n",
      "Epoch [55/200] Step [100/174]: Train loss = 1.3245157122612\n",
      "Epoch [55/200] Step [120/174]: Train loss = 1.3326605677604675\n",
      "Epoch [55/200] Step [140/174]: Train loss = 1.2623321950435638\n",
      "Epoch [55/200] Step [160/174]: Train loss = 1.3381915450096131\n",
      "Epoch [56/200] Step [20/174]: Train loss = 1.3270721793174745\n",
      "Epoch [56/200] Step [40/174]: Train loss = 1.3507159560918809\n",
      "Epoch [56/200] Step [60/174]: Train loss = 1.3301864743232727\n",
      "Epoch [56/200] Step [80/174]: Train loss = 1.3007802665233612\n",
      "Epoch [56/200] Step [100/174]: Train loss = 1.3326278805732727\n",
      "Epoch [56/200] Step [120/174]: Train loss = 1.3532423555850983\n",
      "Epoch [56/200] Step [140/174]: Train loss = 1.2458036482334136\n",
      "Epoch [56/200] Step [160/174]: Train loss = 1.3370187163352967\n",
      "Epoch [57/200] Step [20/174]: Train loss = 1.3241380035877228\n",
      "Epoch [57/200] Step [40/174]: Train loss = 1.3238524079322815\n",
      "Epoch [57/200] Step [60/174]: Train loss = 1.2639640271663666\n",
      "Epoch [57/200] Step [80/174]: Train loss = 1.286411815881729\n",
      "Epoch [57/200] Step [100/174]: Train loss = 1.3191313922405243\n",
      "Epoch [57/200] Step [120/174]: Train loss = 1.257581353187561\n",
      "Epoch [57/200] Step [140/174]: Train loss = 1.2312158405780793\n",
      "Epoch [57/200] Step [160/174]: Train loss = 1.3306414127349853\n",
      "Epoch [58/200] Step [20/174]: Train loss = 1.2525983154773712\n",
      "Epoch [58/200] Step [40/174]: Train loss = 1.2687017977237702\n",
      "Epoch [58/200] Step [60/174]: Train loss = 1.2872524917125703\n",
      "Epoch [58/200] Step [80/174]: Train loss = 1.3105154871940612\n",
      "Epoch [58/200] Step [100/174]: Train loss = 1.2709055125713349\n",
      "Epoch [58/200] Step [120/174]: Train loss = 1.2608414053916932\n",
      "Epoch [58/200] Step [140/174]: Train loss = 1.252263456583023\n",
      "Epoch [58/200] Step [160/174]: Train loss = 1.2696242570877074\n",
      "Epoch [59/200] Step [20/174]: Train loss = 1.308596658706665\n",
      "Epoch [59/200] Step [40/174]: Train loss = 1.2991216599941253\n",
      "Epoch [59/200] Step [60/174]: Train loss = 1.2636275887489319\n",
      "Epoch [59/200] Step [80/174]: Train loss = 1.2794900596141816\n",
      "Epoch [59/200] Step [100/174]: Train loss = 1.2698189675807954\n",
      "Epoch [59/200] Step [120/174]: Train loss = 1.3079083025455476\n",
      "Epoch [59/200] Step [140/174]: Train loss = 1.2255098521709442\n",
      "Epoch [59/200] Step [160/174]: Train loss = 1.2912290334701537\n",
      "Epoch [60/200] Step [20/174]: Train loss = 1.2982650697231293\n",
      "Epoch [60/200] Step [40/174]: Train loss = 1.2855196833610534\n",
      "Epoch [60/200] Step [60/174]: Train loss = 1.2356942653656007\n",
      "Epoch [60/200] Step [80/174]: Train loss = 1.2608005046844482\n",
      "Epoch [60/200] Step [100/174]: Train loss = 1.2398833632469177\n",
      "Epoch [60/200] Step [120/174]: Train loss = 1.2698343098163605\n",
      "Epoch [60/200] Step [140/174]: Train loss = 1.1991878628730774\n",
      "Epoch [60/200] Step [160/174]: Train loss = 1.293397879600525\n",
      "Epoch [61/200] Step [20/174]: Train loss = 1.282930040359497\n",
      "Epoch [61/200] Step [40/174]: Train loss = 1.2592459917068481\n",
      "Epoch [61/200] Step [60/174]: Train loss = 1.2024610787630081\n",
      "Epoch [61/200] Step [80/174]: Train loss = 1.2480416893959045\n",
      "Epoch [61/200] Step [100/174]: Train loss = 1.2298574566841125\n",
      "Epoch [61/200] Step [120/174]: Train loss = 1.21092147231102\n",
      "Epoch [61/200] Step [140/174]: Train loss = 1.2063108801841735\n",
      "Epoch [61/200] Step [160/174]: Train loss = 1.2519026458263398\n",
      "Epoch [62/200] Step [20/174]: Train loss = 1.2422262251377105\n",
      "Epoch [62/200] Step [40/174]: Train loss = 1.2773812234401702\n",
      "Epoch [62/200] Step [60/174]: Train loss = 1.2127711653709412\n",
      "Epoch [62/200] Step [80/174]: Train loss = 1.2457054555416107\n",
      "Epoch [62/200] Step [100/174]: Train loss = 1.1726706206798554\n",
      "Epoch [62/200] Step [120/174]: Train loss = 1.2041537642478943\n",
      "Epoch [62/200] Step [140/174]: Train loss = 1.1702182024717331\n",
      "Epoch [62/200] Step [160/174]: Train loss = 1.2509248733520508\n",
      "Epoch [63/200] Step [20/174]: Train loss = 1.2291969239711762\n",
      "Epoch [63/200] Step [40/174]: Train loss = 1.2633838593959807\n",
      "Epoch [63/200] Step [60/174]: Train loss = 1.204457938671112\n",
      "Epoch [63/200] Step [80/174]: Train loss = 1.2334828197956085\n",
      "Epoch [63/200] Step [100/174]: Train loss = 1.200687700510025\n",
      "Epoch [63/200] Step [120/174]: Train loss = 1.2243998229503632\n",
      "Epoch [63/200] Step [140/174]: Train loss = 1.1575614541769028\n",
      "Epoch [63/200] Step [160/174]: Train loss = 1.2376019418239594\n",
      "Epoch [64/200] Step [20/174]: Train loss = 1.2309908121824265\n",
      "Epoch [64/200] Step [40/174]: Train loss = 1.2348259121179581\n",
      "Epoch [64/200] Step [60/174]: Train loss = 1.192983904480934\n",
      "Epoch [64/200] Step [80/174]: Train loss = 1.1981668174266815\n",
      "Epoch [64/200] Step [100/174]: Train loss = 1.182693663239479\n",
      "Epoch [64/200] Step [120/174]: Train loss = 1.2220333248376847\n",
      "Epoch [64/200] Step [140/174]: Train loss = 1.172884437441826\n",
      "Epoch [64/200] Step [160/174]: Train loss = 1.219272792339325\n",
      "Epoch [65/200] Step [20/174]: Train loss = 1.235434076189995\n",
      "Epoch [65/200] Step [40/174]: Train loss = 1.181022623181343\n",
      "Epoch [65/200] Step [60/174]: Train loss = 1.207211285829544\n",
      "Epoch [65/200] Step [80/174]: Train loss = 1.1823984116315842\n",
      "Epoch [65/200] Step [100/174]: Train loss = 1.1697524458169937\n",
      "Epoch [65/200] Step [120/174]: Train loss = 1.2329495072364807\n",
      "Epoch [65/200] Step [140/174]: Train loss = 1.1399238973855972\n",
      "Epoch [65/200] Step [160/174]: Train loss = 1.2312263906002046\n",
      "Epoch [66/200] Step [20/174]: Train loss = 1.206585732102394\n",
      "Epoch [66/200] Step [40/174]: Train loss = 1.1808093518018723\n",
      "Epoch [66/200] Step [60/174]: Train loss = 1.1923056930303573\n",
      "Epoch [66/200] Step [80/174]: Train loss = 1.1323378384113312\n",
      "Epoch [66/200] Step [100/174]: Train loss = 1.210173824429512\n",
      "Epoch [66/200] Step [120/174]: Train loss = 1.170643538236618\n",
      "Epoch [66/200] Step [140/174]: Train loss = 1.1991362601518631\n",
      "Epoch [66/200] Step [160/174]: Train loss = 1.2232313692569732\n",
      "Epoch [67/200] Step [20/174]: Train loss = 1.2236408948898316\n",
      "Epoch [67/200] Step [40/174]: Train loss = 1.1875516653060914\n",
      "Epoch [67/200] Step [60/174]: Train loss = 1.2099491477012634\n",
      "Epoch [67/200] Step [80/174]: Train loss = 1.2071845829486847\n",
      "Epoch [67/200] Step [100/174]: Train loss = 1.232964527606964\n",
      "Epoch [67/200] Step [120/174]: Train loss = 1.2015723824501037\n",
      "Epoch [67/200] Step [140/174]: Train loss = 1.1455274522304535\n",
      "Epoch [67/200] Step [160/174]: Train loss = 1.1675494194030762\n",
      "Epoch [68/200] Step [20/174]: Train loss = 1.1601665019989014\n",
      "Epoch [68/200] Step [40/174]: Train loss = 1.1468918085098267\n",
      "Epoch [68/200] Step [60/174]: Train loss = 1.2091177374124527\n",
      "Epoch [68/200] Step [80/174]: Train loss = 1.1670752882957458\n",
      "Epoch [68/200] Step [100/174]: Train loss = 1.1809782654047012\n",
      "Epoch [68/200] Step [120/174]: Train loss = 1.1819604843854905\n",
      "Epoch [68/200] Step [140/174]: Train loss = 1.1247405350208282\n",
      "Epoch [68/200] Step [160/174]: Train loss = 1.1669954299926757\n",
      "Epoch [69/200] Step [20/174]: Train loss = 1.1929496824741364\n",
      "Epoch [69/200] Step [40/174]: Train loss = 1.1698071360588074\n",
      "Epoch [69/200] Step [60/174]: Train loss = 1.1333725810050965\n",
      "Epoch [69/200] Step [80/174]: Train loss = 1.165157398581505\n",
      "Epoch [69/200] Step [100/174]: Train loss = 1.1354361832141877\n",
      "Epoch [69/200] Step [120/174]: Train loss = 1.1272705256938935\n",
      "Epoch [69/200] Step [140/174]: Train loss = 1.1258226186037064\n",
      "Epoch [69/200] Step [160/174]: Train loss = 1.1625857889652251\n",
      "Epoch [70/200] Step [20/174]: Train loss = 1.1713227778673172\n",
      "Epoch [70/200] Step [40/174]: Train loss = 1.1637073367834092\n",
      "Epoch [70/200] Step [60/174]: Train loss = 1.0963205248117447\n",
      "Epoch [70/200] Step [80/174]: Train loss = 1.1505398631095887\n",
      "Epoch [70/200] Step [100/174]: Train loss = 1.161009633541107\n",
      "Epoch [70/200] Step [120/174]: Train loss = 1.0851301819086074\n",
      "Epoch [70/200] Step [140/174]: Train loss = 1.066110521554947\n",
      "Epoch [70/200] Step [160/174]: Train loss = 1.1211824685335159\n",
      "Epoch [71/200] Step [20/174]: Train loss = 1.146657133102417\n",
      "Epoch [71/200] Step [40/174]: Train loss = 1.136816817522049\n",
      "Epoch [71/200] Step [60/174]: Train loss = 1.1024554938077926\n",
      "Epoch [71/200] Step [80/174]: Train loss = 1.1636717915534973\n",
      "Epoch [71/200] Step [100/174]: Train loss = 1.145387625694275\n",
      "Epoch [71/200] Step [120/174]: Train loss = 1.1093797206878662\n",
      "Epoch [71/200] Step [140/174]: Train loss = 1.1086875289678573\n",
      "Epoch [71/200] Step [160/174]: Train loss = 1.1021902561187744\n",
      "Epoch [72/200] Step [20/174]: Train loss = 1.163295915722847\n",
      "Epoch [72/200] Step [40/174]: Train loss = 1.1522930711507797\n",
      "Epoch [72/200] Step [60/174]: Train loss = 1.1374608993530273\n",
      "Epoch [72/200] Step [80/174]: Train loss = 1.0569519966840744\n",
      "Epoch [72/200] Step [100/174]: Train loss = 1.094039934873581\n",
      "Epoch [72/200] Step [120/174]: Train loss = 1.1472461521625519\n",
      "Epoch [72/200] Step [140/174]: Train loss = 1.0596501767635345\n",
      "Epoch [72/200] Step [160/174]: Train loss = 1.1238840788602829\n",
      "Epoch [73/200] Step [20/174]: Train loss = 1.091959947347641\n",
      "Epoch [73/200] Step [40/174]: Train loss = 1.0924587786197661\n",
      "Epoch [73/200] Step [60/174]: Train loss = 1.0666476964950562\n",
      "Epoch [73/200] Step [80/174]: Train loss = 1.1058922559022903\n",
      "Epoch [73/200] Step [100/174]: Train loss = 1.1703739017248154\n",
      "Epoch [73/200] Step [120/174]: Train loss = 1.0920311629772186\n",
      "Epoch [73/200] Step [140/174]: Train loss = 1.0894861549139023\n",
      "Epoch [73/200] Step [160/174]: Train loss = 1.1345001876354217\n",
      "Epoch [74/200] Step [20/174]: Train loss = 1.093185418844223\n",
      "Epoch [74/200] Step [40/174]: Train loss = 1.076522743701935\n",
      "Epoch [74/200] Step [60/174]: Train loss = 1.1037920445203782\n",
      "Epoch [74/200] Step [80/174]: Train loss = 1.1148990988731384\n",
      "Epoch [74/200] Step [100/174]: Train loss = 1.1240070939064026\n",
      "Epoch [74/200] Step [120/174]: Train loss = 1.081403848528862\n",
      "Epoch [74/200] Step [140/174]: Train loss = 1.0384668499231338\n",
      "Epoch [74/200] Step [160/174]: Train loss = 1.0932047724723817\n",
      "Epoch [75/200] Step [20/174]: Train loss = 1.0795333176851272\n",
      "Epoch [75/200] Step [40/174]: Train loss = 1.0825740694999695\n",
      "Epoch [75/200] Step [60/174]: Train loss = 1.0433162838220595\n",
      "Epoch [75/200] Step [80/174]: Train loss = 1.0743082821369172\n",
      "Epoch [75/200] Step [100/174]: Train loss = 1.1223795145750046\n",
      "Epoch [75/200] Step [120/174]: Train loss = 1.0757718622684478\n",
      "Epoch [75/200] Step [140/174]: Train loss = 1.0650754421949387\n",
      "Epoch [75/200] Step [160/174]: Train loss = 1.0556680589914322\n",
      "Epoch [76/200] Step [20/174]: Train loss = 1.0972592800855636\n",
      "Epoch [76/200] Step [40/174]: Train loss = 1.1023826211690904\n",
      "Epoch [76/200] Step [60/174]: Train loss = 1.0158564478158951\n",
      "Epoch [76/200] Step [80/174]: Train loss = 1.0639049023389817\n",
      "Epoch [76/200] Step [100/174]: Train loss = 1.0935827046632767\n",
      "Epoch [76/200] Step [120/174]: Train loss = 1.0918451130390168\n",
      "Epoch [76/200] Step [140/174]: Train loss = 1.0409207761287689\n",
      "Epoch [76/200] Step [160/174]: Train loss = 1.0731713145971298\n",
      "Epoch [77/200] Step [20/174]: Train loss = 1.041693463921547\n",
      "Epoch [77/200] Step [40/174]: Train loss = 1.0594888240098954\n",
      "Epoch [77/200] Step [60/174]: Train loss = 0.9835832953453064\n",
      "Epoch [77/200] Step [80/174]: Train loss = 1.0644120037555695\n",
      "Epoch [77/200] Step [100/174]: Train loss = 1.10407994389534\n",
      "Epoch [77/200] Step [120/174]: Train loss = 1.0949872881174088\n",
      "Epoch [77/200] Step [140/174]: Train loss = 1.0165931165218354\n",
      "Epoch [77/200] Step [160/174]: Train loss = 1.0847036093473434\n",
      "Epoch [78/200] Step [20/174]: Train loss = 1.0832678347826004\n",
      "Epoch [78/200] Step [40/174]: Train loss = 1.0412971079349518\n",
      "Epoch [78/200] Step [60/174]: Train loss = 0.9894752860069275\n",
      "Epoch [78/200] Step [80/174]: Train loss = 1.0349778413772583\n",
      "Epoch [78/200] Step [100/174]: Train loss = 1.0734615087509156\n",
      "Epoch [78/200] Step [120/174]: Train loss = 1.0300110459327698\n",
      "Epoch [78/200] Step [140/174]: Train loss = 1.0238893359899521\n",
      "Epoch [78/200] Step [160/174]: Train loss = 1.0639295756816864\n",
      "Epoch [79/200] Step [20/174]: Train loss = 1.049217078089714\n",
      "Epoch [79/200] Step [40/174]: Train loss = 1.0613241255283357\n",
      "Epoch [79/200] Step [60/174]: Train loss = 1.016628050804138\n",
      "Epoch [79/200] Step [80/174]: Train loss = 1.0640009105205537\n",
      "Epoch [79/200] Step [100/174]: Train loss = 1.0647531360387803\n",
      "Epoch [79/200] Step [120/174]: Train loss = 1.0072119057178497\n",
      "Epoch [79/200] Step [140/174]: Train loss = 0.9778959721326828\n",
      "Epoch [79/200] Step [160/174]: Train loss = 1.0829052180051804\n",
      "Epoch [80/200] Step [20/174]: Train loss = 0.9851017504930496\n",
      "Epoch [80/200] Step [40/174]: Train loss = 1.0351116985082627\n",
      "Epoch [80/200] Step [60/174]: Train loss = 1.031160682439804\n",
      "Epoch [80/200] Step [80/174]: Train loss = 0.9947484076023102\n",
      "Epoch [80/200] Step [100/174]: Train loss = 1.0795021265745164\n",
      "Epoch [80/200] Step [120/174]: Train loss = 1.022927361726761\n",
      "Epoch [80/200] Step [140/174]: Train loss = 0.9830325543880463\n",
      "Epoch [80/200] Step [160/174]: Train loss = 1.043926978111267\n",
      "Epoch [81/200] Step [20/174]: Train loss = 1.0832314282655715\n",
      "Epoch [81/200] Step [40/174]: Train loss = 1.0414824932813644\n",
      "Epoch [81/200] Step [60/174]: Train loss = 1.0157172828912735\n",
      "Epoch [81/200] Step [80/174]: Train loss = 1.0153300434350967\n",
      "Epoch [81/200] Step [100/174]: Train loss = 1.0062829315662385\n",
      "Epoch [81/200] Step [120/174]: Train loss = 1.034300258755684\n",
      "Epoch [81/200] Step [140/174]: Train loss = 0.9883641242980957\n",
      "Epoch [81/200] Step [160/174]: Train loss = 1.0212329506874085\n",
      "Epoch [82/200] Step [20/174]: Train loss = 1.0577435940504074\n",
      "Epoch [82/200] Step [40/174]: Train loss = 1.0023519784212112\n",
      "Epoch [82/200] Step [60/174]: Train loss = 1.0655047595500946\n",
      "Epoch [82/200] Step [80/174]: Train loss = 1.0166466116905213\n",
      "Epoch [82/200] Step [100/174]: Train loss = 1.0286798536777497\n",
      "Epoch [82/200] Step [120/174]: Train loss = 1.0137527674436568\n",
      "Epoch [82/200] Step [140/174]: Train loss = 1.0058976262807846\n",
      "Epoch [82/200] Step [160/174]: Train loss = 0.9864427983760834\n",
      "Epoch [83/200] Step [20/174]: Train loss = 1.0392946273088455\n",
      "Epoch [83/200] Step [40/174]: Train loss = 0.9626258343458176\n",
      "Epoch [83/200] Step [60/174]: Train loss = 1.0540633499622345\n",
      "Epoch [83/200] Step [80/174]: Train loss = 1.01241637468338\n",
      "Epoch [83/200] Step [100/174]: Train loss = 0.9673870116472244\n",
      "Epoch [83/200] Step [120/174]: Train loss = 1.0180414706468581\n",
      "Epoch [83/200] Step [140/174]: Train loss = 0.9879461735486984\n",
      "Epoch [83/200] Step [160/174]: Train loss = 0.9786754548549652\n",
      "Epoch [84/200] Step [20/174]: Train loss = 1.0254555374383927\n",
      "Epoch [84/200] Step [40/174]: Train loss = 0.9846827536821365\n",
      "Epoch [84/200] Step [60/174]: Train loss = 0.9397107720375061\n",
      "Epoch [84/200] Step [80/174]: Train loss = 1.0075540781021117\n",
      "Epoch [84/200] Step [100/174]: Train loss = 0.9963077187538147\n",
      "Epoch [84/200] Step [120/174]: Train loss = 1.008213499188423\n",
      "Epoch [84/200] Step [140/174]: Train loss = 0.9578827321529388\n",
      "Epoch [84/200] Step [160/174]: Train loss = 1.0308686643838882\n",
      "Epoch [85/200] Step [20/174]: Train loss = 1.0185797035694122\n",
      "Epoch [85/200] Step [40/174]: Train loss = 0.9906883507966995\n",
      "Epoch [85/200] Step [60/174]: Train loss = 0.9935126155614853\n",
      "Epoch [85/200] Step [80/174]: Train loss = 0.9905607521533966\n",
      "Epoch [85/200] Step [100/174]: Train loss = 0.9762214422225952\n",
      "Epoch [85/200] Step [120/174]: Train loss = 1.0089495748281478\n",
      "Epoch [85/200] Step [140/174]: Train loss = 0.971546471118927\n",
      "Epoch [85/200] Step [160/174]: Train loss = 0.9825381636619568\n",
      "Epoch [86/200] Step [20/174]: Train loss = 1.001658409833908\n",
      "Epoch [86/200] Step [40/174]: Train loss = 0.9735787272453308\n",
      "Epoch [86/200] Step [60/174]: Train loss = 1.0354676425457001\n",
      "Epoch [86/200] Step [80/174]: Train loss = 0.9682068049907684\n",
      "Epoch [86/200] Step [100/174]: Train loss = 0.9871007174253463\n",
      "Epoch [86/200] Step [120/174]: Train loss = 0.9896715164184571\n",
      "Epoch [86/200] Step [140/174]: Train loss = 0.9405623078346252\n",
      "Epoch [86/200] Step [160/174]: Train loss = 0.9969759970903397\n",
      "Epoch [87/200] Step [20/174]: Train loss = 0.9564937978982926\n",
      "Epoch [87/200] Step [40/174]: Train loss = 0.9531422883272171\n",
      "Epoch [87/200] Step [60/174]: Train loss = 0.9613960444927215\n",
      "Epoch [87/200] Step [80/174]: Train loss = 0.9905528038740158\n",
      "Epoch [87/200] Step [100/174]: Train loss = 0.983686438202858\n",
      "Epoch [87/200] Step [120/174]: Train loss = 0.9692314594984055\n",
      "Epoch [87/200] Step [140/174]: Train loss = 0.9368995368480683\n",
      "Epoch [87/200] Step [160/174]: Train loss = 0.968085765838623\n",
      "Epoch [88/200] Step [20/174]: Train loss = 1.0117888480424881\n",
      "Epoch [88/200] Step [40/174]: Train loss = 0.946523341536522\n",
      "Epoch [88/200] Step [60/174]: Train loss = 0.9680649071931839\n",
      "Epoch [88/200] Step [80/174]: Train loss = 0.9477187395095825\n",
      "Epoch [88/200] Step [100/174]: Train loss = 0.9920502543449402\n",
      "Epoch [88/200] Step [120/174]: Train loss = 0.9841259211301804\n",
      "Epoch [88/200] Step [140/174]: Train loss = 0.8875069379806518\n",
      "Epoch [88/200] Step [160/174]: Train loss = 0.9523346066474915\n",
      "Epoch [89/200] Step [20/174]: Train loss = 0.9087559729814529\n",
      "Epoch [89/200] Step [40/174]: Train loss = 0.9919309109449387\n",
      "Epoch [89/200] Step [60/174]: Train loss = 0.9280135154724121\n",
      "Epoch [89/200] Step [80/174]: Train loss = 1.0100837409496308\n",
      "Epoch [89/200] Step [100/174]: Train loss = 0.9630072116851807\n",
      "Epoch [89/200] Step [120/174]: Train loss = 0.9620491176843643\n",
      "Epoch [89/200] Step [140/174]: Train loss = 0.9485752522945404\n",
      "Epoch [89/200] Step [160/174]: Train loss = 0.9806051135063172\n",
      "Epoch [90/200] Step [20/174]: Train loss = 0.9347539961338043\n",
      "Epoch [90/200] Step [40/174]: Train loss = 0.9362443029880524\n",
      "Epoch [90/200] Step [60/174]: Train loss = 0.9491981387138366\n",
      "Epoch [90/200] Step [80/174]: Train loss = 0.9528830945491791\n",
      "Epoch [90/200] Step [100/174]: Train loss = 0.9457954466342926\n",
      "Epoch [90/200] Step [120/174]: Train loss = 0.9350623995065689\n",
      "Epoch [90/200] Step [140/174]: Train loss = 0.8784592419862747\n",
      "Epoch [90/200] Step [160/174]: Train loss = 0.932880488038063\n",
      "Epoch [91/200] Step [20/174]: Train loss = 0.8957717806100846\n",
      "Epoch [91/200] Step [40/174]: Train loss = 0.9392203748226166\n",
      "Epoch [91/200] Step [60/174]: Train loss = 0.9523679465055466\n",
      "Epoch [91/200] Step [80/174]: Train loss = 0.9280149936676025\n",
      "Epoch [91/200] Step [100/174]: Train loss = 0.9384985327720642\n",
      "Epoch [91/200] Step [120/174]: Train loss = 0.9378250539302826\n",
      "Epoch [91/200] Step [140/174]: Train loss = 0.8961219638586044\n",
      "Epoch [91/200] Step [160/174]: Train loss = 0.9673158615827561\n",
      "Epoch [92/200] Step [20/174]: Train loss = 0.957203722000122\n",
      "Epoch [92/200] Step [40/174]: Train loss = 0.9474873632192612\n",
      "Epoch [92/200] Step [60/174]: Train loss = 0.8759401053190231\n",
      "Epoch [92/200] Step [80/174]: Train loss = 0.8931063652038574\n",
      "Epoch [92/200] Step [100/174]: Train loss = 0.9599104911088944\n",
      "Epoch [92/200] Step [120/174]: Train loss = 0.9317346811294556\n",
      "Epoch [92/200] Step [140/174]: Train loss = 0.8660522669553756\n",
      "Epoch [92/200] Step [160/174]: Train loss = 0.9849071949720383\n",
      "Epoch [93/200] Step [20/174]: Train loss = 0.8919137328863144\n",
      "Epoch [93/200] Step [40/174]: Train loss = 0.8480498671531678\n",
      "Epoch [93/200] Step [60/174]: Train loss = 0.8819828450679779\n",
      "Epoch [93/200] Step [80/174]: Train loss = 0.9151573479175568\n",
      "Epoch [93/200] Step [100/174]: Train loss = 0.9651147603988648\n",
      "Epoch [93/200] Step [120/174]: Train loss = 0.9369918465614319\n",
      "Epoch [93/200] Step [140/174]: Train loss = 0.8881948530673981\n",
      "Epoch [93/200] Step [160/174]: Train loss = 0.9265460669994354\n",
      "Epoch [94/200] Step [20/174]: Train loss = 0.9180398017168045\n",
      "Epoch [94/200] Step [40/174]: Train loss = 0.9172152578830719\n",
      "Epoch [94/200] Step [60/174]: Train loss = 0.909824737906456\n",
      "Epoch [94/200] Step [80/174]: Train loss = 0.9703824371099472\n",
      "Epoch [94/200] Step [100/174]: Train loss = 0.8940491735935211\n",
      "Epoch [94/200] Step [120/174]: Train loss = 0.9210326492786407\n",
      "Epoch [94/200] Step [140/174]: Train loss = 0.8884612321853638\n",
      "Epoch [94/200] Step [160/174]: Train loss = 0.8919577181339264\n",
      "Epoch [95/200] Step [20/174]: Train loss = 0.9068705886602402\n",
      "Epoch [95/200] Step [40/174]: Train loss = 0.8862921893596649\n",
      "Epoch [95/200] Step [60/174]: Train loss = 0.8830278098583222\n",
      "Epoch [95/200] Step [80/174]: Train loss = 0.9525581538677216\n",
      "Epoch [95/200] Step [100/174]: Train loss = 0.9337719708681107\n",
      "Epoch [95/200] Step [120/174]: Train loss = 0.8866773754358291\n",
      "Epoch [95/200] Step [140/174]: Train loss = 0.8930007129907608\n",
      "Epoch [95/200] Step [160/174]: Train loss = 0.9094760417938232\n",
      "Epoch [96/200] Step [20/174]: Train loss = 0.9328153938055038\n",
      "Epoch [96/200] Step [40/174]: Train loss = 0.8756031304597854\n",
      "Epoch [96/200] Step [60/174]: Train loss = 0.8914998203516007\n",
      "Epoch [96/200] Step [80/174]: Train loss = 0.8950242549180984\n",
      "Epoch [96/200] Step [100/174]: Train loss = 0.9172961860895157\n",
      "Epoch [96/200] Step [120/174]: Train loss = 0.9034344792366028\n",
      "Epoch [96/200] Step [140/174]: Train loss = 0.8814966380596161\n",
      "Epoch [96/200] Step [160/174]: Train loss = 0.9428124248981475\n",
      "Epoch [97/200] Step [20/174]: Train loss = 0.9070451408624649\n",
      "Epoch [97/200] Step [40/174]: Train loss = 0.9108100861310959\n",
      "Epoch [97/200] Step [60/174]: Train loss = 0.884981918334961\n",
      "Epoch [97/200] Step [80/174]: Train loss = 0.8806789755821228\n",
      "Epoch [97/200] Step [100/174]: Train loss = 0.8909879505634308\n",
      "Epoch [97/200] Step [120/174]: Train loss = 0.8872495979070664\n",
      "Epoch [97/200] Step [140/174]: Train loss = 0.8534190446138382\n",
      "Epoch [97/200] Step [160/174]: Train loss = 0.9011530697345733\n",
      "Epoch [98/200] Step [20/174]: Train loss = 0.9017143696546555\n",
      "Epoch [98/200] Step [40/174]: Train loss = 0.8494272798299789\n",
      "Epoch [98/200] Step [60/174]: Train loss = 0.8399731695652009\n",
      "Epoch [98/200] Step [80/174]: Train loss = 0.8928047716617584\n",
      "Epoch [98/200] Step [100/174]: Train loss = 0.8592642962932586\n",
      "Epoch [98/200] Step [120/174]: Train loss = 0.8837444514036179\n",
      "Epoch [98/200] Step [140/174]: Train loss = 0.8490546971559525\n",
      "Epoch [98/200] Step [160/174]: Train loss = 0.8658607393503189\n",
      "Epoch [99/200] Step [20/174]: Train loss = 0.8867532879114151\n",
      "Epoch [99/200] Step [40/174]: Train loss = 0.9021746963262558\n",
      "Epoch [99/200] Step [60/174]: Train loss = 0.876311331987381\n",
      "Epoch [99/200] Step [80/174]: Train loss = 0.8718322783708572\n",
      "Epoch [99/200] Step [100/174]: Train loss = 0.9071227252483368\n",
      "Epoch [99/200] Step [120/174]: Train loss = 0.8795178920030594\n",
      "Epoch [99/200] Step [140/174]: Train loss = 0.8277094602584839\n",
      "Epoch [99/200] Step [160/174]: Train loss = 0.8400459378957749\n",
      "Epoch [100/200] Step [20/174]: Train loss = 0.8751163721084595\n",
      "Epoch [100/200] Step [40/174]: Train loss = 0.9143376529216767\n",
      "Epoch [100/200] Step [60/174]: Train loss = 0.8981580853462219\n",
      "Epoch [100/200] Step [80/174]: Train loss = 0.8824515670537949\n",
      "Epoch [100/200] Step [100/174]: Train loss = 0.8589689195156097\n",
      "Epoch [100/200] Step [120/174]: Train loss = 0.8542475134134293\n",
      "Epoch [100/200] Step [140/174]: Train loss = 0.7829247146844864\n",
      "Epoch [100/200] Step [160/174]: Train loss = 0.8691127449274063\n",
      "Epoch [101/200] Step [20/174]: Train loss = 0.8671462833881378\n",
      "Epoch [101/200] Step [40/174]: Train loss = 0.8587280243635178\n",
      "Epoch [101/200] Step [60/174]: Train loss = 0.8249898135662079\n",
      "Epoch [101/200] Step [80/174]: Train loss = 0.8615404337644577\n",
      "Epoch [101/200] Step [100/174]: Train loss = 0.8834367275238038\n",
      "Epoch [101/200] Step [120/174]: Train loss = 0.8215201795101166\n",
      "Epoch [101/200] Step [140/174]: Train loss = 0.7887876778841019\n",
      "Epoch [101/200] Step [160/174]: Train loss = 0.8587189227342605\n",
      "Epoch [102/200] Step [20/174]: Train loss = 0.848006084561348\n",
      "Epoch [102/200] Step [40/174]: Train loss = 0.8550942093133926\n",
      "Epoch [102/200] Step [60/174]: Train loss = 0.8233362197875976\n",
      "Epoch [102/200] Step [80/174]: Train loss = 0.8908592492341996\n",
      "Epoch [102/200] Step [100/174]: Train loss = 0.8610845923423767\n",
      "Epoch [102/200] Step [120/174]: Train loss = 0.8283076703548431\n",
      "Epoch [102/200] Step [140/174]: Train loss = 0.8280190467834473\n",
      "Epoch [102/200] Step [160/174]: Train loss = 0.8390920549631119\n",
      "Epoch [103/200] Step [20/174]: Train loss = 0.829279437661171\n",
      "Epoch [103/200] Step [40/174]: Train loss = 0.882681030035019\n",
      "Epoch [103/200] Step [60/174]: Train loss = 0.8301195979118348\n",
      "Epoch [103/200] Step [80/174]: Train loss = 0.8321486949920655\n",
      "Epoch [103/200] Step [100/174]: Train loss = 0.8325212866067886\n",
      "Epoch [103/200] Step [120/174]: Train loss = 0.8099060863256454\n",
      "Epoch [103/200] Step [140/174]: Train loss = 0.8040947288274765\n",
      "Epoch [103/200] Step [160/174]: Train loss = 0.8618397831916809\n",
      "Epoch [104/200] Step [20/174]: Train loss = 0.8453303903341294\n",
      "Epoch [104/200] Step [40/174]: Train loss = 0.8023663491010666\n",
      "Epoch [104/200] Step [60/174]: Train loss = 0.8294780045747757\n",
      "Epoch [104/200] Step [80/174]: Train loss = 0.8615118741989136\n",
      "Epoch [104/200] Step [100/174]: Train loss = 0.8707194596529007\n",
      "Epoch [104/200] Step [120/174]: Train loss = 0.8378481328487396\n",
      "Epoch [104/200] Step [140/174]: Train loss = 0.7738230466842652\n",
      "Epoch [104/200] Step [160/174]: Train loss = 0.855762493610382\n",
      "Epoch [105/200] Step [20/174]: Train loss = 0.8584217816591263\n",
      "Epoch [105/200] Step [40/174]: Train loss = 0.8472236454486847\n",
      "Epoch [105/200] Step [60/174]: Train loss = 0.7917300641536713\n",
      "Epoch [105/200] Step [80/174]: Train loss = 0.8540438026189804\n",
      "Epoch [105/200] Step [100/174]: Train loss = 0.8391655415296555\n",
      "Epoch [105/200] Step [120/174]: Train loss = 0.8107748717069626\n",
      "Epoch [105/200] Step [140/174]: Train loss = 0.7946952700614929\n",
      "Epoch [105/200] Step [160/174]: Train loss = 0.8329009354114533\n",
      "Epoch [106/200] Step [20/174]: Train loss = 0.8279288381338119\n",
      "Epoch [106/200] Step [40/174]: Train loss = 0.8596362859010697\n",
      "Epoch [106/200] Step [60/174]: Train loss = 0.7998675376176834\n",
      "Epoch [106/200] Step [80/174]: Train loss = 0.817323237657547\n",
      "Epoch [106/200] Step [100/174]: Train loss = 0.8503437578678131\n",
      "Epoch [106/200] Step [120/174]: Train loss = 0.8083581387996673\n",
      "Epoch [106/200] Step [140/174]: Train loss = 0.8244040459394455\n",
      "Epoch [106/200] Step [160/174]: Train loss = 0.8357803434133529\n",
      "Epoch [107/200] Step [20/174]: Train loss = 0.8046849966049194\n",
      "Epoch [107/200] Step [40/174]: Train loss = 0.8270270556211472\n",
      "Epoch [107/200] Step [60/174]: Train loss = 0.788328617811203\n",
      "Epoch [107/200] Step [80/174]: Train loss = 0.8098125398159027\n",
      "Epoch [107/200] Step [100/174]: Train loss = 0.8282466381788254\n",
      "Epoch [107/200] Step [120/174]: Train loss = 0.7885837554931641\n",
      "Epoch [107/200] Step [140/174]: Train loss = 0.8203336149454117\n",
      "Epoch [107/200] Step [160/174]: Train loss = 0.8535549074411393\n",
      "Epoch [108/200] Step [20/174]: Train loss = 0.8597317069768906\n",
      "Epoch [108/200] Step [40/174]: Train loss = 0.791587033867836\n",
      "Epoch [108/200] Step [60/174]: Train loss = 0.7892439872026443\n",
      "Epoch [108/200] Step [80/174]: Train loss = 0.8236276596784592\n",
      "Epoch [108/200] Step [100/174]: Train loss = 0.8022075533866883\n",
      "Epoch [108/200] Step [120/174]: Train loss = 0.8109444886445999\n",
      "Epoch [108/200] Step [140/174]: Train loss = 0.7928890585899353\n",
      "Epoch [108/200] Step [160/174]: Train loss = 0.8167504876852035\n",
      "Epoch [109/200] Step [20/174]: Train loss = 0.8150305300951004\n",
      "Epoch [109/200] Step [40/174]: Train loss = 0.8381431370973587\n",
      "Epoch [109/200] Step [60/174]: Train loss = 0.8297959744930268\n",
      "Epoch [109/200] Step [80/174]: Train loss = 0.792030081152916\n",
      "Epoch [109/200] Step [100/174]: Train loss = 0.8278397113084793\n",
      "Epoch [109/200] Step [120/174]: Train loss = 0.7839535772800446\n",
      "Epoch [109/200] Step [140/174]: Train loss = 0.7446858614683152\n",
      "Epoch [109/200] Step [160/174]: Train loss = 0.8460441172122956\n",
      "Epoch [110/200] Step [20/174]: Train loss = 0.7804546475410461\n",
      "Epoch [110/200] Step [40/174]: Train loss = 0.7839185357093811\n",
      "Epoch [110/200] Step [60/174]: Train loss = 0.762384682893753\n",
      "Epoch [110/200] Step [80/174]: Train loss = 0.8057633757591247\n",
      "Epoch [110/200] Step [100/174]: Train loss = 0.8203526020050049\n",
      "Epoch [110/200] Step [120/174]: Train loss = 0.8087315052747727\n",
      "Epoch [110/200] Step [140/174]: Train loss = 0.7498063802719116\n",
      "Epoch [110/200] Step [160/174]: Train loss = 0.7989858746528625\n",
      "Epoch [111/200] Step [20/174]: Train loss = 0.7978356420993805\n",
      "Epoch [111/200] Step [40/174]: Train loss = 0.781099608540535\n",
      "Epoch [111/200] Step [60/174]: Train loss = 0.8277885317802429\n",
      "Epoch [111/200] Step [80/174]: Train loss = 0.8203684747219085\n",
      "Epoch [111/200] Step [100/174]: Train loss = 0.7771998286247254\n",
      "Epoch [111/200] Step [120/174]: Train loss = 0.7652450501918793\n",
      "Epoch [111/200] Step [140/174]: Train loss = 0.7436732709407806\n",
      "Epoch [111/200] Step [160/174]: Train loss = 0.7817747205495834\n",
      "Epoch [112/200] Step [20/174]: Train loss = 0.8283178359270096\n",
      "Epoch [112/200] Step [40/174]: Train loss = 0.7825151085853577\n",
      "Epoch [112/200] Step [60/174]: Train loss = 0.7833012938499451\n",
      "Epoch [112/200] Step [80/174]: Train loss = 0.8477069050073623\n",
      "Epoch [112/200] Step [100/174]: Train loss = 0.7579016715288163\n",
      "Epoch [112/200] Step [120/174]: Train loss = 0.7629346281290055\n",
      "Epoch [112/200] Step [140/174]: Train loss = 0.7643225073814393\n",
      "Epoch [112/200] Step [160/174]: Train loss = 0.8439007043838501\n",
      "Epoch [113/200] Step [20/174]: Train loss = 0.7678697377443313\n",
      "Epoch [113/200] Step [40/174]: Train loss = 0.7849723786115647\n",
      "Epoch [113/200] Step [60/174]: Train loss = 0.7378508657217026\n",
      "Epoch [113/200] Step [80/174]: Train loss = 0.7885012477636337\n",
      "Epoch [113/200] Step [100/174]: Train loss = 0.7707482814788819\n",
      "Epoch [113/200] Step [120/174]: Train loss = 0.759251070022583\n",
      "Epoch [113/200] Step [140/174]: Train loss = 0.7586197942495346\n",
      "Epoch [113/200] Step [160/174]: Train loss = 0.7736791580915451\n",
      "Epoch [114/200] Step [20/174]: Train loss = 0.7690888702869415\n",
      "Epoch [114/200] Step [40/174]: Train loss = 0.7521756500005722\n",
      "Epoch [114/200] Step [60/174]: Train loss = 0.7605048269033432\n",
      "Epoch [114/200] Step [80/174]: Train loss = 0.7384679049253464\n",
      "Epoch [114/200] Step [100/174]: Train loss = 0.7757606655359268\n",
      "Epoch [114/200] Step [120/174]: Train loss = 0.7542770445346832\n",
      "Epoch [114/200] Step [140/174]: Train loss = 0.7433986276388168\n",
      "Epoch [114/200] Step [160/174]: Train loss = 0.7984698832035064\n",
      "Epoch [115/200] Step [20/174]: Train loss = 0.7535840928554535\n",
      "Epoch [115/200] Step [40/174]: Train loss = 0.8086916297674179\n",
      "Epoch [115/200] Step [60/174]: Train loss = 0.7538733333349228\n",
      "Epoch [115/200] Step [80/174]: Train loss = 0.7704092532396316\n",
      "Epoch [115/200] Step [100/174]: Train loss = 0.7779239520430565\n",
      "Epoch [115/200] Step [120/174]: Train loss = 0.787419056892395\n",
      "Epoch [115/200] Step [140/174]: Train loss = 0.7445525914430619\n",
      "Epoch [115/200] Step [160/174]: Train loss = 0.7883007407188416\n",
      "Epoch [116/200] Step [20/174]: Train loss = 0.7748561054468155\n",
      "Epoch [116/200] Step [40/174]: Train loss = 0.75308056473732\n",
      "Epoch [116/200] Step [60/174]: Train loss = 0.7559048742055893\n",
      "Epoch [116/200] Step [80/174]: Train loss = 0.7158364191651344\n",
      "Epoch [116/200] Step [100/174]: Train loss = 0.7300297409296036\n",
      "Epoch [116/200] Step [120/174]: Train loss = 0.7155233711004257\n",
      "Epoch [116/200] Step [140/174]: Train loss = 0.7069297522306442\n",
      "Epoch [116/200] Step [160/174]: Train loss = 0.7596518337726593\n",
      "Epoch [117/200] Step [20/174]: Train loss = 0.7697868168354034\n",
      "Epoch [117/200] Step [40/174]: Train loss = 0.742047181725502\n",
      "Epoch [117/200] Step [60/174]: Train loss = 0.742980134487152\n",
      "Epoch [117/200] Step [80/174]: Train loss = 0.7533781290054321\n",
      "Epoch [117/200] Step [100/174]: Train loss = 0.7660456538200379\n",
      "Epoch [117/200] Step [120/174]: Train loss = 0.7552373513579369\n",
      "Epoch [117/200] Step [140/174]: Train loss = 0.7068655520677567\n",
      "Epoch [117/200] Step [160/174]: Train loss = 0.7511074006557464\n",
      "Epoch [118/200] Step [20/174]: Train loss = 0.7376719206571579\n",
      "Epoch [118/200] Step [40/174]: Train loss = 0.7599025547504425\n",
      "Epoch [118/200] Step [60/174]: Train loss = 0.7052693575620651\n",
      "Epoch [118/200] Step [80/174]: Train loss = 0.7342812716960907\n",
      "Epoch [118/200] Step [100/174]: Train loss = 0.7705175250768661\n",
      "Epoch [118/200] Step [120/174]: Train loss = 0.7491886138916015\n",
      "Epoch [118/200] Step [140/174]: Train loss = 0.7250665932893753\n",
      "Epoch [118/200] Step [160/174]: Train loss = 0.7757761001586914\n",
      "Epoch [119/200] Step [20/174]: Train loss = 0.7928141206502914\n",
      "Epoch [119/200] Step [40/174]: Train loss = 0.7406897217035293\n",
      "Epoch [119/200] Step [60/174]: Train loss = 0.7247584193944931\n",
      "Epoch [119/200] Step [80/174]: Train loss = 0.7096682831645011\n",
      "Epoch [119/200] Step [100/174]: Train loss = 0.7728643387556076\n",
      "Epoch [119/200] Step [120/174]: Train loss = 0.7362578451633454\n",
      "Epoch [119/200] Step [140/174]: Train loss = 0.6999956995248795\n",
      "Epoch [119/200] Step [160/174]: Train loss = 0.7506180256605148\n",
      "Epoch [120/200] Step [20/174]: Train loss = 0.7586183816194534\n",
      "Epoch [120/200] Step [40/174]: Train loss = 0.7334728479385376\n",
      "Epoch [120/200] Step [60/174]: Train loss = 0.709998682141304\n",
      "Epoch [120/200] Step [80/174]: Train loss = 0.6954201459884644\n",
      "Epoch [120/200] Step [100/174]: Train loss = 0.7590638220310211\n",
      "Epoch [120/200] Step [120/174]: Train loss = 0.7226517856121063\n",
      "Epoch [120/200] Step [140/174]: Train loss = 0.7072022840380668\n",
      "Epoch [120/200] Step [160/174]: Train loss = 0.7330169588327408\n",
      "Epoch [121/200] Step [20/174]: Train loss = 0.7411749273538589\n",
      "Epoch [121/200] Step [40/174]: Train loss = 0.7819435447454453\n",
      "Epoch [121/200] Step [60/174]: Train loss = 0.7147599786520005\n",
      "Epoch [121/200] Step [80/174]: Train loss = 0.7186923146247863\n",
      "Epoch [121/200] Step [100/174]: Train loss = 0.7408818632364274\n",
      "Epoch [121/200] Step [120/174]: Train loss = 0.7081810474395752\n",
      "Epoch [121/200] Step [140/174]: Train loss = 0.6645532563328743\n",
      "Epoch [121/200] Step [160/174]: Train loss = 0.7275838971138\n",
      "Epoch [122/200] Step [20/174]: Train loss = 0.7085877060890198\n",
      "Epoch [122/200] Step [40/174]: Train loss = 0.6956540554761886\n",
      "Epoch [122/200] Step [60/174]: Train loss = 0.7079858928918839\n",
      "Epoch [122/200] Step [80/174]: Train loss = 0.7332635700702668\n",
      "Epoch [122/200] Step [100/174]: Train loss = 0.7396289259195328\n",
      "Epoch [122/200] Step [120/174]: Train loss = 0.7402698904275894\n",
      "Epoch [122/200] Step [140/174]: Train loss = 0.6793333113193512\n",
      "Epoch [122/200] Step [160/174]: Train loss = 0.6990079134702682\n",
      "Epoch [123/200] Step [20/174]: Train loss = 0.7430300951004029\n",
      "Epoch [123/200] Step [40/174]: Train loss = 0.7151402860879899\n",
      "Epoch [123/200] Step [60/174]: Train loss = 0.7058064118027687\n",
      "Epoch [123/200] Step [80/174]: Train loss = 0.681120154261589\n",
      "Epoch [123/200] Step [100/174]: Train loss = 0.7390312671661377\n",
      "Epoch [123/200] Step [120/174]: Train loss = 0.7228426307439804\n",
      "Epoch [123/200] Step [140/174]: Train loss = 0.6509857594966888\n",
      "Epoch [123/200] Step [160/174]: Train loss = 0.6950514823198318\n",
      "Epoch [124/200] Step [20/174]: Train loss = 0.7209567040205002\n",
      "Epoch [124/200] Step [40/174]: Train loss = 0.7304897099733353\n",
      "Epoch [124/200] Step [60/174]: Train loss = 0.6738534152507782\n",
      "Epoch [124/200] Step [80/174]: Train loss = 0.6720284253358841\n",
      "Epoch [124/200] Step [100/174]: Train loss = 0.7264885812997818\n",
      "Epoch [124/200] Step [120/174]: Train loss = 0.7068075269460679\n",
      "Epoch [124/200] Step [140/174]: Train loss = 0.6893363714218139\n",
      "Epoch [124/200] Step [160/174]: Train loss = 0.7217571645975113\n",
      "Epoch [125/200] Step [20/174]: Train loss = 0.7396517246961594\n",
      "Epoch [125/200] Step [40/174]: Train loss = 0.7220721125602723\n",
      "Epoch [125/200] Step [60/174]: Train loss = 0.6977156817913055\n",
      "Epoch [125/200] Step [80/174]: Train loss = 0.7024207353591919\n",
      "Epoch [125/200] Step [100/174]: Train loss = 0.7101006478071212\n",
      "Epoch [125/200] Step [120/174]: Train loss = 0.6897926032543182\n",
      "Epoch [125/200] Step [140/174]: Train loss = 0.6563862651586533\n",
      "Epoch [125/200] Step [160/174]: Train loss = 0.7198302000761032\n",
      "Epoch [126/200] Step [20/174]: Train loss = 0.6999014914035797\n",
      "Epoch [126/200] Step [40/174]: Train loss = 0.7008790343999862\n",
      "Epoch [126/200] Step [60/174]: Train loss = 0.7207226976752281\n",
      "Epoch [126/200] Step [80/174]: Train loss = 0.7179207175970077\n",
      "Epoch [126/200] Step [100/174]: Train loss = 0.7461386740207672\n",
      "Epoch [126/200] Step [120/174]: Train loss = 0.6975412428379059\n",
      "Epoch [126/200] Step [140/174]: Train loss = 0.6523724138736725\n",
      "Epoch [126/200] Step [160/174]: Train loss = 0.7002958655357361\n",
      "Epoch [127/200] Step [20/174]: Train loss = 0.7000247180461884\n",
      "Epoch [127/200] Step [40/174]: Train loss = 0.7149100184440613\n",
      "Epoch [127/200] Step [60/174]: Train loss = 0.6719829201698303\n",
      "Epoch [127/200] Step [80/174]: Train loss = 0.6768632635474205\n",
      "Epoch [127/200] Step [100/174]: Train loss = 0.693678817152977\n",
      "Epoch [127/200] Step [120/174]: Train loss = 0.6790418893098831\n",
      "Epoch [127/200] Step [140/174]: Train loss = 0.6768258720636368\n",
      "Epoch [127/200] Step [160/174]: Train loss = 0.68400689214468\n",
      "Epoch [128/200] Step [20/174]: Train loss = 0.7145167529582978\n",
      "Epoch [128/200] Step [40/174]: Train loss = 0.7137570917606354\n",
      "Epoch [128/200] Step [60/174]: Train loss = 0.6584856450557709\n",
      "Epoch [128/200] Step [80/174]: Train loss = 0.6491661995649338\n",
      "Epoch [128/200] Step [100/174]: Train loss = 0.723004749417305\n",
      "Epoch [128/200] Step [120/174]: Train loss = 0.6350903153419495\n",
      "Epoch [128/200] Step [140/174]: Train loss = 0.6859179139137268\n",
      "Epoch [128/200] Step [160/174]: Train loss = 0.6945306316018105\n",
      "Epoch [129/200] Step [20/174]: Train loss = 0.6770181357860565\n",
      "Epoch [129/200] Step [40/174]: Train loss = 0.6829017877578736\n",
      "Epoch [129/200] Step [60/174]: Train loss = 0.6918787121772766\n",
      "Epoch [129/200] Step [80/174]: Train loss = 0.6427925765514374\n",
      "Epoch [129/200] Step [100/174]: Train loss = 0.679382535815239\n",
      "Epoch [129/200] Step [120/174]: Train loss = 0.6980601221323013\n",
      "Epoch [129/200] Step [140/174]: Train loss = 0.6685006380081177\n",
      "Epoch [129/200] Step [160/174]: Train loss = 0.6475277110934258\n",
      "Epoch [130/200] Step [20/174]: Train loss = 0.7156686604022979\n",
      "Epoch [130/200] Step [40/174]: Train loss = 0.6743107467889786\n",
      "Epoch [130/200] Step [60/174]: Train loss = 0.6434728145599365\n",
      "Epoch [130/200] Step [80/174]: Train loss = 0.6635084018111229\n",
      "Epoch [130/200] Step [100/174]: Train loss = 0.6710011512041092\n",
      "Epoch [130/200] Step [120/174]: Train loss = 0.6896348863840103\n",
      "Epoch [130/200] Step [140/174]: Train loss = 0.6518004760146141\n",
      "Epoch [130/200] Step [160/174]: Train loss = 0.6905240833759307\n",
      "Epoch [131/200] Step [20/174]: Train loss = 0.6890426069498062\n",
      "Epoch [131/200] Step [40/174]: Train loss = 0.6705689668655396\n",
      "Epoch [131/200] Step [60/174]: Train loss = 0.6549295425415039\n",
      "Epoch [131/200] Step [80/174]: Train loss = 0.6775732666254044\n",
      "Epoch [131/200] Step [100/174]: Train loss = 0.6657635062932968\n",
      "Epoch [131/200] Step [120/174]: Train loss = 0.6799209445714951\n",
      "Epoch [131/200] Step [140/174]: Train loss = 0.6363056197762489\n",
      "Epoch [131/200] Step [160/174]: Train loss = 0.6613696843385697\n",
      "Epoch [132/200] Step [20/174]: Train loss = 0.6942005693912506\n",
      "Epoch [132/200] Step [40/174]: Train loss = 0.6621882557868958\n",
      "Epoch [132/200] Step [60/174]: Train loss = 0.6555755630135536\n",
      "Epoch [132/200] Step [80/174]: Train loss = 0.6930466324090958\n",
      "Epoch [132/200] Step [100/174]: Train loss = 0.6872497826814652\n",
      "Epoch [132/200] Step [120/174]: Train loss = 0.7035239845514297\n",
      "Epoch [132/200] Step [140/174]: Train loss = 0.6205985054373742\n",
      "Epoch [132/200] Step [160/174]: Train loss = 0.6639565244317055\n",
      "Epoch [133/200] Step [20/174]: Train loss = 0.637473015487194\n",
      "Epoch [133/200] Step [40/174]: Train loss = 0.6412625223398208\n",
      "Epoch [133/200] Step [60/174]: Train loss = 0.6219055980443955\n",
      "Epoch [133/200] Step [80/174]: Train loss = 0.6378054678440094\n",
      "Epoch [133/200] Step [100/174]: Train loss = 0.6545532315969467\n",
      "Epoch [133/200] Step [120/174]: Train loss = 0.6903596431016922\n",
      "Epoch [133/200] Step [140/174]: Train loss = 0.6601366013288498\n",
      "Epoch [133/200] Step [160/174]: Train loss = 0.6634552761912346\n",
      "Epoch [134/200] Step [20/174]: Train loss = 0.6468419685959816\n",
      "Epoch [134/200] Step [40/174]: Train loss = 0.6308849260210991\n",
      "Epoch [134/200] Step [60/174]: Train loss = 0.6502308905124664\n",
      "Epoch [134/200] Step [80/174]: Train loss = 0.6829234629869461\n",
      "Epoch [134/200] Step [100/174]: Train loss = 0.6354924440383911\n",
      "Epoch [134/200] Step [120/174]: Train loss = 0.6612253248691559\n",
      "Epoch [134/200] Step [140/174]: Train loss = 0.6416370138525963\n",
      "Epoch [134/200] Step [160/174]: Train loss = 0.6417818099260331\n",
      "Epoch [135/200] Step [20/174]: Train loss = 0.6447521835565567\n",
      "Epoch [135/200] Step [40/174]: Train loss = 0.6386006593704223\n",
      "Epoch [135/200] Step [60/174]: Train loss = 0.6381647750735283\n",
      "Epoch [135/200] Step [80/174]: Train loss = 0.6459775492548943\n",
      "Epoch [135/200] Step [100/174]: Train loss = 0.6614048063755036\n",
      "Epoch [135/200] Step [120/174]: Train loss = 0.6767582729458809\n",
      "Epoch [135/200] Step [140/174]: Train loss = 0.6524409800767899\n",
      "Epoch [135/200] Step [160/174]: Train loss = 0.6377781331539154\n",
      "Epoch [136/200] Step [20/174]: Train loss = 0.6643669664859772\n",
      "Epoch [136/200] Step [40/174]: Train loss = 0.632834455370903\n",
      "Epoch [136/200] Step [60/174]: Train loss = 0.639263704419136\n",
      "Epoch [136/200] Step [80/174]: Train loss = 0.6247974559664726\n",
      "Epoch [136/200] Step [100/174]: Train loss = 0.6411691963672638\n",
      "Epoch [136/200] Step [120/174]: Train loss = 0.6586211696267128\n",
      "Epoch [136/200] Step [140/174]: Train loss = 0.6291644439101219\n",
      "Epoch [136/200] Step [160/174]: Train loss = 0.6599693924188614\n",
      "Epoch [137/200] Step [20/174]: Train loss = 0.6406144693493843\n",
      "Epoch [137/200] Step [40/174]: Train loss = 0.6700713887810708\n",
      "Epoch [137/200] Step [60/174]: Train loss = 0.5976117148995399\n",
      "Epoch [137/200] Step [80/174]: Train loss = 0.6563419908285141\n",
      "Epoch [137/200] Step [100/174]: Train loss = 0.6121026113629341\n",
      "Epoch [137/200] Step [120/174]: Train loss = 0.6542873680591583\n",
      "Epoch [137/200] Step [140/174]: Train loss = 0.6129396706819534\n",
      "Epoch [137/200] Step [160/174]: Train loss = 0.6567628413438797\n",
      "Epoch [138/200] Step [20/174]: Train loss = 0.6804497390985489\n",
      "Epoch [138/200] Step [40/174]: Train loss = 0.6168984308838844\n",
      "Epoch [138/200] Step [60/174]: Train loss = 0.6250394105911254\n",
      "Epoch [138/200] Step [80/174]: Train loss = 0.6516910582780838\n",
      "Epoch [138/200] Step [100/174]: Train loss = 0.6251728102564812\n",
      "Epoch [138/200] Step [120/174]: Train loss = 0.6118677452206611\n",
      "Epoch [138/200] Step [140/174]: Train loss = 0.577933070063591\n",
      "Epoch [138/200] Step [160/174]: Train loss = 0.6389725178480148\n",
      "Epoch [139/200] Step [20/174]: Train loss = 0.6198116064071655\n",
      "Epoch [139/200] Step [40/174]: Train loss = 0.592692919075489\n",
      "Epoch [139/200] Step [60/174]: Train loss = 0.6019868209958077\n",
      "Epoch [139/200] Step [80/174]: Train loss = 0.6538417115807533\n",
      "Epoch [139/200] Step [100/174]: Train loss = 0.6736464232206345\n",
      "Epoch [139/200] Step [120/174]: Train loss = 0.6369022190570831\n",
      "Epoch [139/200] Step [140/174]: Train loss = 0.6198655679821968\n",
      "Epoch [139/200] Step [160/174]: Train loss = 0.6206566646695137\n",
      "Epoch [140/200] Step [20/174]: Train loss = 0.6613214999437332\n",
      "Epoch [140/200] Step [40/174]: Train loss = 0.659146498143673\n",
      "Epoch [140/200] Step [60/174]: Train loss = 0.6187099516391754\n",
      "Epoch [140/200] Step [80/174]: Train loss = 0.6398751050233841\n",
      "Epoch [140/200] Step [100/174]: Train loss = 0.6336155593395233\n",
      "Epoch [140/200] Step [120/174]: Train loss = 0.6557199165225029\n",
      "Epoch [140/200] Step [140/174]: Train loss = 0.6132820457220077\n",
      "Epoch [140/200] Step [160/174]: Train loss = 0.6184988558292389\n",
      "Epoch [141/200] Step [20/174]: Train loss = 0.6386573925614357\n",
      "Epoch [141/200] Step [40/174]: Train loss = 0.608098316192627\n",
      "Epoch [141/200] Step [60/174]: Train loss = 0.606326000392437\n",
      "Epoch [141/200] Step [80/174]: Train loss = 0.5838459029793739\n",
      "Epoch [141/200] Step [100/174]: Train loss = 0.6202658176422119\n",
      "Epoch [141/200] Step [120/174]: Train loss = 0.6541187480092049\n",
      "Epoch [141/200] Step [140/174]: Train loss = 0.5907964140176774\n",
      "Epoch [141/200] Step [160/174]: Train loss = 0.6316227197647095\n",
      "Epoch [142/200] Step [20/174]: Train loss = 0.639913396537304\n",
      "Epoch [142/200] Step [40/174]: Train loss = 0.6284069329500198\n",
      "Epoch [142/200] Step [60/174]: Train loss = 0.6407331019639969\n",
      "Epoch [142/200] Step [80/174]: Train loss = 0.588642780482769\n",
      "Epoch [142/200] Step [100/174]: Train loss = 0.621354840695858\n",
      "Epoch [142/200] Step [120/174]: Train loss = 0.5927542880177498\n",
      "Epoch [142/200] Step [140/174]: Train loss = 0.5898066550493241\n",
      "Epoch [142/200] Step [160/174]: Train loss = 0.597937761247158\n",
      "Epoch [143/200] Step [20/174]: Train loss = 0.6539272487163543\n",
      "Epoch [143/200] Step [40/174]: Train loss = 0.6143684417009354\n",
      "Epoch [143/200] Step [60/174]: Train loss = 0.6049713775515556\n",
      "Epoch [143/200] Step [80/174]: Train loss = 0.6415946751832962\n",
      "Epoch [143/200] Step [100/174]: Train loss = 0.6006959214806556\n",
      "Epoch [143/200] Step [120/174]: Train loss = 0.6156263515353203\n",
      "Epoch [143/200] Step [140/174]: Train loss = 0.5622815951704979\n",
      "Epoch [143/200] Step [160/174]: Train loss = 0.6238522931933403\n",
      "Epoch [144/200] Step [20/174]: Train loss = 0.6185191482305527\n",
      "Epoch [144/200] Step [40/174]: Train loss = 0.6844873368740082\n",
      "Epoch [144/200] Step [60/174]: Train loss = 0.5597537592053413\n",
      "Epoch [144/200] Step [80/174]: Train loss = 0.6153787106275559\n",
      "Epoch [144/200] Step [100/174]: Train loss = 0.6312333375215531\n",
      "Epoch [144/200] Step [120/174]: Train loss = 0.5968713879585266\n",
      "Epoch [144/200] Step [140/174]: Train loss = 0.5971996620297432\n",
      "Epoch [144/200] Step [160/174]: Train loss = 0.6279836922883988\n",
      "Epoch [145/200] Step [20/174]: Train loss = 0.633565291762352\n",
      "Epoch [145/200] Step [40/174]: Train loss = 0.6337830170989036\n",
      "Epoch [145/200] Step [60/174]: Train loss = 0.626824428141117\n",
      "Epoch [145/200] Step [80/174]: Train loss = 0.5952913865447045\n",
      "Epoch [145/200] Step [100/174]: Train loss = 0.6053924486041069\n",
      "Epoch [145/200] Step [120/174]: Train loss = 0.5900723189115524\n",
      "Epoch [145/200] Step [140/174]: Train loss = 0.5516223236918449\n",
      "Epoch [145/200] Step [160/174]: Train loss = 0.6026381686329841\n",
      "Epoch [146/200] Step [20/174]: Train loss = 0.6018058314919472\n",
      "Epoch [146/200] Step [40/174]: Train loss = 0.6202827677130699\n",
      "Epoch [146/200] Step [60/174]: Train loss = 0.5630717813968659\n",
      "Epoch [146/200] Step [80/174]: Train loss = 0.6016866907477378\n",
      "Epoch [146/200] Step [100/174]: Train loss = 0.5935577288269996\n",
      "Epoch [146/200] Step [120/174]: Train loss = 0.5854608625173569\n",
      "Epoch [146/200] Step [140/174]: Train loss = 0.598491969704628\n",
      "Epoch [146/200] Step [160/174]: Train loss = 0.65084910094738\n",
      "Epoch [147/200] Step [20/174]: Train loss = 0.581245943903923\n",
      "Epoch [147/200] Step [40/174]: Train loss = 0.596229687333107\n",
      "Epoch [147/200] Step [60/174]: Train loss = 0.6027145639061928\n",
      "Epoch [147/200] Step [80/174]: Train loss = 0.5828922316431999\n",
      "Epoch [147/200] Step [100/174]: Train loss = 0.5888116165995598\n",
      "Epoch [147/200] Step [120/174]: Train loss = 0.621230748295784\n",
      "Epoch [147/200] Step [140/174]: Train loss = 0.591100312769413\n",
      "Epoch [147/200] Step [160/174]: Train loss = 0.6088086634874343\n",
      "Epoch [148/200] Step [20/174]: Train loss = 0.6010065376758575\n",
      "Epoch [148/200] Step [40/174]: Train loss = 0.5838805466890336\n",
      "Epoch [148/200] Step [60/174]: Train loss = 0.5988912761211396\n",
      "Epoch [148/200] Step [80/174]: Train loss = 0.570850683748722\n",
      "Epoch [148/200] Step [100/174]: Train loss = 0.6302217558026314\n",
      "Epoch [148/200] Step [120/174]: Train loss = 0.6072128102183342\n",
      "Epoch [148/200] Step [140/174]: Train loss = 0.5748043566942215\n",
      "Epoch [148/200] Step [160/174]: Train loss = 0.5836675599217415\n",
      "Epoch [149/200] Step [20/174]: Train loss = 0.59384795576334\n",
      "Epoch [149/200] Step [40/174]: Train loss = 0.5872427552938462\n",
      "Epoch [149/200] Step [60/174]: Train loss = 0.5917719379067421\n",
      "Epoch [149/200] Step [80/174]: Train loss = 0.5920273527503014\n",
      "Epoch [149/200] Step [100/174]: Train loss = 0.6262837156653405\n",
      "Epoch [149/200] Step [120/174]: Train loss = 0.5542096585035324\n",
      "Epoch [149/200] Step [140/174]: Train loss = 0.5759927868843079\n",
      "Epoch [149/200] Step [160/174]: Train loss = 0.5917371720075607\n",
      "Epoch [150/200] Step [20/174]: Train loss = 0.5694181367754936\n",
      "Epoch [150/200] Step [40/174]: Train loss = 0.5923743292689323\n",
      "Epoch [150/200] Step [60/174]: Train loss = 0.5540582507848739\n",
      "Epoch [150/200] Step [80/174]: Train loss = 0.5932748034596443\n",
      "Epoch [150/200] Step [100/174]: Train loss = 0.5563104897737503\n",
      "Epoch [150/200] Step [120/174]: Train loss = 0.6298623979091644\n",
      "Epoch [150/200] Step [140/174]: Train loss = 0.5344479277729988\n",
      "Epoch [150/200] Step [160/174]: Train loss = 0.581194706261158\n",
      "Epoch [151/200] Step [20/174]: Train loss = 0.5481383740901947\n",
      "Epoch [151/200] Step [40/174]: Train loss = 0.5795524626970291\n",
      "Epoch [151/200] Step [60/174]: Train loss = 0.5601759895682334\n",
      "Epoch [151/200] Step [80/174]: Train loss = 0.5675954818725586\n",
      "Epoch [151/200] Step [100/174]: Train loss = 0.5795586764812469\n",
      "Epoch [151/200] Step [120/174]: Train loss = 0.5853368192911148\n",
      "Epoch [151/200] Step [140/174]: Train loss = 0.5238405898213386\n",
      "Epoch [151/200] Step [160/174]: Train loss = 0.5957058951258659\n",
      "Epoch [152/200] Step [20/174]: Train loss = 0.5504717752337456\n",
      "Epoch [152/200] Step [40/174]: Train loss = 0.5778318077325821\n",
      "Epoch [152/200] Step [60/174]: Train loss = 0.5917944371700287\n",
      "Epoch [152/200] Step [80/174]: Train loss = 0.569817753136158\n",
      "Epoch [152/200] Step [100/174]: Train loss = 0.5866347342729569\n",
      "Epoch [152/200] Step [120/174]: Train loss = 0.5996282115578652\n",
      "Epoch [152/200] Step [140/174]: Train loss = 0.5769812300801277\n",
      "Epoch [152/200] Step [160/174]: Train loss = 0.5812857642769813\n",
      "Epoch [153/200] Step [20/174]: Train loss = 0.5480806827545166\n",
      "Epoch [153/200] Step [40/174]: Train loss = 0.6052809685468674\n",
      "Epoch [153/200] Step [60/174]: Train loss = 0.5387863829731941\n",
      "Epoch [153/200] Step [80/174]: Train loss = 0.5797745451331139\n",
      "Epoch [153/200] Step [100/174]: Train loss = 0.535241337120533\n",
      "Epoch [153/200] Step [120/174]: Train loss = 0.5541139051318169\n",
      "Epoch [153/200] Step [140/174]: Train loss = 0.5486489400267601\n",
      "Epoch [153/200] Step [160/174]: Train loss = 0.5867319494485855\n",
      "Epoch [154/200] Step [20/174]: Train loss = 0.6176243990659713\n",
      "Epoch [154/200] Step [40/174]: Train loss = 0.5707483172416687\n",
      "Epoch [154/200] Step [60/174]: Train loss = 0.5348337322473526\n",
      "Epoch [154/200] Step [80/174]: Train loss = 0.5937594816088676\n",
      "Epoch [154/200] Step [100/174]: Train loss = 0.5927478596568108\n",
      "Epoch [154/200] Step [120/174]: Train loss = 0.5920612722635269\n",
      "Epoch [154/200] Step [140/174]: Train loss = 0.5508325368165969\n",
      "Epoch [154/200] Step [160/174]: Train loss = 0.5813019022345542\n",
      "Epoch [155/200] Step [20/174]: Train loss = 0.5641824468970299\n",
      "Epoch [155/200] Step [40/174]: Train loss = 0.5628011927008629\n",
      "Epoch [155/200] Step [60/174]: Train loss = 0.5641956597566604\n",
      "Epoch [155/200] Step [80/174]: Train loss = 0.5914104416966438\n",
      "Epoch [155/200] Step [100/174]: Train loss = 0.5892872229218483\n",
      "Epoch [155/200] Step [120/174]: Train loss = 0.572932206094265\n",
      "Epoch [155/200] Step [140/174]: Train loss = 0.5365172952413559\n",
      "Epoch [155/200] Step [160/174]: Train loss = 0.5923231288790702\n",
      "Epoch [156/200] Step [20/174]: Train loss = 0.5355535209178924\n",
      "Epoch [156/200] Step [40/174]: Train loss = 0.5736214742064476\n",
      "Epoch [156/200] Step [60/174]: Train loss = 0.532568134367466\n",
      "Epoch [156/200] Step [80/174]: Train loss = 0.5701361924409867\n",
      "Epoch [156/200] Step [100/174]: Train loss = 0.5529131963849068\n",
      "Epoch [156/200] Step [120/174]: Train loss = 0.579889951646328\n",
      "Epoch [156/200] Step [140/174]: Train loss = 0.5370143041014671\n",
      "Epoch [156/200] Step [160/174]: Train loss = 0.5765883699059486\n",
      "Epoch [157/200] Step [20/174]: Train loss = 0.5504706144332886\n",
      "Epoch [157/200] Step [40/174]: Train loss = 0.5498245596885681\n",
      "Epoch [157/200] Step [60/174]: Train loss = 0.5700127720832825\n",
      "Epoch [157/200] Step [80/174]: Train loss = 0.558133551478386\n",
      "Epoch [157/200] Step [100/174]: Train loss = 0.5954919278621673\n",
      "Epoch [157/200] Step [120/174]: Train loss = 0.578178933262825\n",
      "Epoch [157/200] Step [140/174]: Train loss = 0.5175244435667992\n",
      "Epoch [157/200] Step [160/174]: Train loss = 0.5493634283542633\n",
      "Epoch [158/200] Step [20/174]: Train loss = 0.5455228418111802\n",
      "Epoch [158/200] Step [40/174]: Train loss = 0.5915534913539886\n",
      "Epoch [158/200] Step [60/174]: Train loss = 0.5410914868116379\n",
      "Epoch [158/200] Step [80/174]: Train loss = 0.579641281068325\n",
      "Epoch [158/200] Step [100/174]: Train loss = 0.5538239806890488\n",
      "Epoch [158/200] Step [120/174]: Train loss = 0.5633942902088165\n",
      "Epoch [158/200] Step [140/174]: Train loss = 0.5345323845744133\n",
      "Epoch [158/200] Step [160/174]: Train loss = 0.5777587279677391\n",
      "Epoch [159/200] Step [20/174]: Train loss = 0.5456439331173897\n",
      "Epoch [159/200] Step [40/174]: Train loss = 0.5964841291308403\n",
      "Epoch [159/200] Step [60/174]: Train loss = 0.5073486372828484\n",
      "Epoch [159/200] Step [80/174]: Train loss = 0.5484059274196624\n",
      "Epoch [159/200] Step [100/174]: Train loss = 0.5746988177299499\n",
      "Epoch [159/200] Step [120/174]: Train loss = 0.5612489446997643\n",
      "Epoch [159/200] Step [140/174]: Train loss = 0.5361743614077568\n",
      "Epoch [159/200] Step [160/174]: Train loss = 0.5738771989941597\n",
      "Epoch [160/200] Step [20/174]: Train loss = 0.5455987915396691\n",
      "Epoch [160/200] Step [40/174]: Train loss = 0.5289402201771736\n",
      "Epoch [160/200] Step [60/174]: Train loss = 0.49382787495851516\n",
      "Epoch [160/200] Step [80/174]: Train loss = 0.5428091362118721\n",
      "Epoch [160/200] Step [100/174]: Train loss = 0.539127641916275\n",
      "Epoch [160/200] Step [120/174]: Train loss = 0.5310058444738388\n",
      "Epoch [160/200] Step [140/174]: Train loss = 0.5368811994791031\n",
      "Epoch [160/200] Step [160/174]: Train loss = 0.516556116938591\n",
      "Epoch [161/200] Step [20/174]: Train loss = 0.5807288199663162\n",
      "Epoch [161/200] Step [40/174]: Train loss = 0.5507606461644172\n",
      "Epoch [161/200] Step [60/174]: Train loss = 0.5441603407263755\n",
      "Epoch [161/200] Step [80/174]: Train loss = 0.5784602075815201\n",
      "Epoch [161/200] Step [100/174]: Train loss = 0.5437956601381302\n",
      "Epoch [161/200] Step [120/174]: Train loss = 0.5296050861477852\n",
      "Epoch [161/200] Step [140/174]: Train loss = 0.5009380131959915\n",
      "Epoch [161/200] Step [160/174]: Train loss = 0.535427300632\n",
      "Epoch [162/200] Step [20/174]: Train loss = 0.5358371615409852\n",
      "Epoch [162/200] Step [40/174]: Train loss = 0.590390907227993\n",
      "Epoch [162/200] Step [60/174]: Train loss = 0.5043594852089882\n",
      "Epoch [162/200] Step [80/174]: Train loss = 0.49005185812711716\n",
      "Epoch [162/200] Step [100/174]: Train loss = 0.573906421661377\n",
      "Epoch [162/200] Step [120/174]: Train loss = 0.5319221019744873\n",
      "Epoch [162/200] Step [140/174]: Train loss = 0.5362178519368171\n",
      "Epoch [162/200] Step [160/174]: Train loss = 0.5078007876873016\n",
      "Epoch [163/200] Step [20/174]: Train loss = 0.5663183361291886\n",
      "Epoch [163/200] Step [40/174]: Train loss = 0.5306048959493637\n",
      "Epoch [163/200] Step [60/174]: Train loss = 0.547864793241024\n",
      "Epoch [163/200] Step [80/174]: Train loss = 0.5386596336960793\n",
      "Epoch [163/200] Step [100/174]: Train loss = 0.5724197879433632\n",
      "Epoch [163/200] Step [120/174]: Train loss = 0.5440099313855171\n",
      "Epoch [163/200] Step [140/174]: Train loss = 0.5054627537727356\n",
      "Epoch [163/200] Step [160/174]: Train loss = 0.5358389377593994\n",
      "Epoch [164/200] Step [20/174]: Train loss = 0.557394702732563\n",
      "Epoch [164/200] Step [40/174]: Train loss = 0.5395874366164207\n",
      "Epoch [164/200] Step [60/174]: Train loss = 0.4955619782209396\n",
      "Epoch [164/200] Step [80/174]: Train loss = 0.5451817154884339\n",
      "Epoch [164/200] Step [100/174]: Train loss = 0.5280568957328796\n",
      "Epoch [164/200] Step [120/174]: Train loss = 0.5210431531071663\n",
      "Epoch [164/200] Step [140/174]: Train loss = 0.5159929826855659\n",
      "Epoch [164/200] Step [160/174]: Train loss = 0.5354103475809098\n",
      "Epoch [165/200] Step [20/174]: Train loss = 0.5377385184168816\n",
      "Epoch [165/200] Step [40/174]: Train loss = 0.5273301675915718\n",
      "Epoch [165/200] Step [60/174]: Train loss = 0.5282964572310448\n",
      "Epoch [165/200] Step [80/174]: Train loss = 0.5095297455787658\n",
      "Epoch [165/200] Step [100/174]: Train loss = 0.5369298622012139\n",
      "Epoch [165/200] Step [120/174]: Train loss = 0.5336087271571159\n",
      "Epoch [165/200] Step [140/174]: Train loss = 0.4902616381645203\n",
      "Epoch [165/200] Step [160/174]: Train loss = 0.534977063536644\n",
      "Epoch [166/200] Step [20/174]: Train loss = 0.5429995730519295\n",
      "Epoch [166/200] Step [40/174]: Train loss = 0.541567987203598\n",
      "Epoch [166/200] Step [60/174]: Train loss = 0.5266158238053322\n",
      "Epoch [166/200] Step [80/174]: Train loss = 0.5309957802295685\n",
      "Epoch [166/200] Step [100/174]: Train loss = 0.5290076896548271\n",
      "Epoch [166/200] Step [120/174]: Train loss = 0.5260543555021286\n",
      "Epoch [166/200] Step [140/174]: Train loss = 0.5135871276259423\n",
      "Epoch [166/200] Step [160/174]: Train loss = 0.5305346980690956\n",
      "Epoch [167/200] Step [20/174]: Train loss = 0.5231353014707565\n",
      "Epoch [167/200] Step [40/174]: Train loss = 0.552212481200695\n",
      "Epoch [167/200] Step [60/174]: Train loss = 0.5189719870686531\n",
      "Epoch [167/200] Step [80/174]: Train loss = 0.5217246919870376\n",
      "Epoch [167/200] Step [100/174]: Train loss = 0.5249839022755622\n",
      "Epoch [167/200] Step [120/174]: Train loss = 0.53594880849123\n",
      "Epoch [167/200] Step [140/174]: Train loss = 0.5184430316090584\n",
      "Epoch [167/200] Step [160/174]: Train loss = 0.5730199754238129\n",
      "Epoch [168/200] Step [20/174]: Train loss = 0.5194443568587304\n",
      "Epoch [168/200] Step [40/174]: Train loss = 0.49996903389692304\n",
      "Epoch [168/200] Step [60/174]: Train loss = 0.5138282537460327\n",
      "Epoch [168/200] Step [80/174]: Train loss = 0.5234003022313118\n",
      "Epoch [168/200] Step [100/174]: Train loss = 0.5312978386878967\n",
      "Epoch [168/200] Step [120/174]: Train loss = 0.506517319381237\n",
      "Epoch [168/200] Step [140/174]: Train loss = 0.528551472723484\n",
      "Epoch [168/200] Step [160/174]: Train loss = 0.5342803359031677\n",
      "Epoch [169/200] Step [20/174]: Train loss = 0.5173653200268745\n",
      "Epoch [169/200] Step [40/174]: Train loss = 0.5050707906484604\n",
      "Epoch [169/200] Step [60/174]: Train loss = 0.506812646985054\n",
      "Epoch [169/200] Step [80/174]: Train loss = 0.5209253907203675\n",
      "Epoch [169/200] Step [100/174]: Train loss = 0.5241880550980568\n",
      "Epoch [169/200] Step [120/174]: Train loss = 0.4964655086398125\n",
      "Epoch [169/200] Step [140/174]: Train loss = 0.49234011471271516\n",
      "Epoch [169/200] Step [160/174]: Train loss = 0.5161206424236298\n",
      "Epoch [170/200] Step [20/174]: Train loss = 0.5063340902328491\n",
      "Epoch [170/200] Step [40/174]: Train loss = 0.49991722255945203\n",
      "Epoch [170/200] Step [60/174]: Train loss = 0.5297553330659867\n",
      "Epoch [170/200] Step [80/174]: Train loss = 0.4924264743924141\n",
      "Epoch [170/200] Step [100/174]: Train loss = 0.4971971794962883\n",
      "Epoch [170/200] Step [120/174]: Train loss = 0.5529791578650475\n",
      "Epoch [170/200] Step [140/174]: Train loss = 0.4833177298307419\n",
      "Epoch [170/200] Step [160/174]: Train loss = 0.5107515394687653\n",
      "Epoch [171/200] Step [20/174]: Train loss = 0.5114582061767579\n",
      "Epoch [171/200] Step [40/174]: Train loss = 0.5248738825321198\n",
      "Epoch [171/200] Step [60/174]: Train loss = 0.513798913359642\n",
      "Epoch [171/200] Step [80/174]: Train loss = 0.5054475903511048\n",
      "Epoch [171/200] Step [100/174]: Train loss = 0.5347384974360466\n",
      "Epoch [171/200] Step [120/174]: Train loss = 0.5030303105711937\n",
      "Epoch [171/200] Step [140/174]: Train loss = 0.4853016555309296\n",
      "Epoch [171/200] Step [160/174]: Train loss = 0.48847944140434263\n",
      "Epoch [172/200] Step [20/174]: Train loss = 0.4863110288977623\n",
      "Epoch [172/200] Step [40/174]: Train loss = 0.5286923184990883\n",
      "Epoch [172/200] Step [60/174]: Train loss = 0.4764826089143753\n",
      "Epoch [172/200] Step [80/174]: Train loss = 0.5537509590387344\n",
      "Epoch [172/200] Step [100/174]: Train loss = 0.47100079208612444\n",
      "Epoch [172/200] Step [120/174]: Train loss = 0.4988973453640938\n",
      "Epoch [172/200] Step [140/174]: Train loss = 0.48796441704034804\n",
      "Epoch [172/200] Step [160/174]: Train loss = 0.5135470166802406\n",
      "Epoch [173/200] Step [20/174]: Train loss = 0.5261637315154075\n",
      "Epoch [173/200] Step [40/174]: Train loss = 0.5214155405759812\n",
      "Epoch [173/200] Step [60/174]: Train loss = 0.4979460328817368\n",
      "Epoch [173/200] Step [80/174]: Train loss = 0.5109955772757531\n",
      "Epoch [173/200] Step [100/174]: Train loss = 0.5048324495553971\n",
      "Epoch [173/200] Step [120/174]: Train loss = 0.497096075117588\n",
      "Epoch [173/200] Step [140/174]: Train loss = 0.490385602414608\n",
      "Epoch [173/200] Step [160/174]: Train loss = 0.49390612095594405\n",
      "Epoch [174/200] Step [20/174]: Train loss = 0.5094155058264732\n",
      "Epoch [174/200] Step [40/174]: Train loss = 0.47005052119493484\n",
      "Epoch [174/200] Step [60/174]: Train loss = 0.4830814316868782\n",
      "Epoch [174/200] Step [80/174]: Train loss = 0.49354568868875504\n",
      "Epoch [174/200] Step [100/174]: Train loss = 0.5096114233136178\n",
      "Epoch [174/200] Step [120/174]: Train loss = 0.5013415113091468\n",
      "Epoch [174/200] Step [140/174]: Train loss = 0.4999553844332695\n",
      "Epoch [174/200] Step [160/174]: Train loss = 0.5425475373864174\n",
      "Epoch [175/200] Step [20/174]: Train loss = 0.5080372974276542\n",
      "Epoch [175/200] Step [40/174]: Train loss = 0.5011732429265976\n",
      "Epoch [175/200] Step [60/174]: Train loss = 0.46654871106147766\n",
      "Epoch [175/200] Step [80/174]: Train loss = 0.5225061729550362\n",
      "Epoch [175/200] Step [100/174]: Train loss = 0.5183466598391533\n",
      "Epoch [175/200] Step [120/174]: Train loss = 0.4957390621304512\n",
      "Epoch [175/200] Step [140/174]: Train loss = 0.49339033514261244\n",
      "Epoch [175/200] Step [160/174]: Train loss = 0.5198079347610474\n",
      "Epoch [176/200] Step [20/174]: Train loss = 0.48044489324092865\n",
      "Epoch [176/200] Step [40/174]: Train loss = 0.5211918175220489\n",
      "Epoch [176/200] Step [60/174]: Train loss = 0.4782732754945755\n",
      "Epoch [176/200] Step [80/174]: Train loss = 0.5283532485365867\n",
      "Epoch [176/200] Step [100/174]: Train loss = 0.51084965467453\n",
      "Epoch [176/200] Step [120/174]: Train loss = 0.475650380551815\n",
      "Epoch [176/200] Step [140/174]: Train loss = 0.47450254708528516\n",
      "Epoch [176/200] Step [160/174]: Train loss = 0.48213901966810224\n",
      "Epoch [177/200] Step [20/174]: Train loss = 0.48134979903697966\n",
      "Epoch [177/200] Step [40/174]: Train loss = 0.48404031097888944\n",
      "Epoch [177/200] Step [60/174]: Train loss = 0.49532587826251984\n",
      "Epoch [177/200] Step [80/174]: Train loss = 0.48403224647045134\n",
      "Epoch [177/200] Step [100/174]: Train loss = 0.4955447196960449\n",
      "Epoch [177/200] Step [120/174]: Train loss = 0.5016675442457199\n",
      "Epoch [177/200] Step [140/174]: Train loss = 0.4909437447786331\n",
      "Epoch [177/200] Step [160/174]: Train loss = 0.4949697941541672\n",
      "Epoch [178/200] Step [20/174]: Train loss = 0.4699406027793884\n",
      "Epoch [178/200] Step [40/174]: Train loss = 0.5394701048731804\n",
      "Epoch [178/200] Step [60/174]: Train loss = 0.48343063741922376\n",
      "Epoch [178/200] Step [80/174]: Train loss = 0.4899488493800163\n",
      "Epoch [178/200] Step [100/174]: Train loss = 0.4957886040210724\n",
      "Epoch [178/200] Step [120/174]: Train loss = 0.4672489568591118\n",
      "Epoch [178/200] Step [140/174]: Train loss = 0.476607745885849\n",
      "Epoch [178/200] Step [160/174]: Train loss = 0.47877705544233323\n",
      "Epoch [179/200] Step [20/174]: Train loss = 0.4793559432029724\n",
      "Epoch [179/200] Step [40/174]: Train loss = 0.49512423425912855\n",
      "Epoch [179/200] Step [60/174]: Train loss = 0.4683398887515068\n",
      "Epoch [179/200] Step [80/174]: Train loss = 0.4981533497571945\n",
      "Epoch [179/200] Step [100/174]: Train loss = 0.4665184661746025\n",
      "Epoch [179/200] Step [120/174]: Train loss = 0.46919809728860856\n",
      "Epoch [179/200] Step [140/174]: Train loss = 0.449956576526165\n",
      "Epoch [179/200] Step [160/174]: Train loss = 0.5044458419084549\n",
      "Epoch [180/200] Step [20/174]: Train loss = 0.48911253064870835\n",
      "Epoch [180/200] Step [40/174]: Train loss = 0.4734416127204895\n",
      "Epoch [180/200] Step [60/174]: Train loss = 0.48512653112411497\n",
      "Epoch [180/200] Step [80/174]: Train loss = 0.460915869474411\n",
      "Epoch [180/200] Step [100/174]: Train loss = 0.4637066230177879\n",
      "Epoch [180/200] Step [120/174]: Train loss = 0.4728926241397858\n",
      "Epoch [180/200] Step [140/174]: Train loss = 0.457522514462471\n",
      "Epoch [180/200] Step [160/174]: Train loss = 0.4865746572613716\n",
      "Epoch [181/200] Step [20/174]: Train loss = 0.47711228877305983\n",
      "Epoch [181/200] Step [40/174]: Train loss = 0.4843964159488678\n",
      "Epoch [181/200] Step [60/174]: Train loss = 0.4374138370156288\n",
      "Epoch [181/200] Step [80/174]: Train loss = 0.47851511389017104\n",
      "Epoch [181/200] Step [100/174]: Train loss = 0.4760798141360283\n",
      "Epoch [181/200] Step [120/174]: Train loss = 0.4637588128447533\n",
      "Epoch [181/200] Step [140/174]: Train loss = 0.4352459967136383\n",
      "Epoch [181/200] Step [160/174]: Train loss = 0.49322460740804674\n",
      "Epoch [182/200] Step [20/174]: Train loss = 0.4608414053916931\n",
      "Epoch [182/200] Step [40/174]: Train loss = 0.4747073605656624\n",
      "Epoch [182/200] Step [60/174]: Train loss = 0.45785983055830004\n",
      "Epoch [182/200] Step [80/174]: Train loss = 0.4676953673362732\n",
      "Epoch [182/200] Step [100/174]: Train loss = 0.4860377907752991\n",
      "Epoch [182/200] Step [120/174]: Train loss = 0.4763944000005722\n",
      "Epoch [182/200] Step [140/174]: Train loss = 0.4285111725330353\n",
      "Epoch [182/200] Step [160/174]: Train loss = 0.4897420108318329\n",
      "Epoch [183/200] Step [20/174]: Train loss = 0.4775308549404144\n",
      "Epoch [183/200] Step [40/174]: Train loss = 0.52247004956007\n",
      "Epoch [183/200] Step [60/174]: Train loss = 0.4694994404911995\n",
      "Epoch [183/200] Step [80/174]: Train loss = 0.4751549050211906\n",
      "Epoch [183/200] Step [100/174]: Train loss = 0.48568726927042005\n",
      "Epoch [183/200] Step [120/174]: Train loss = 0.49045692980289457\n",
      "Epoch [183/200] Step [140/174]: Train loss = 0.47428932338953017\n",
      "Epoch [183/200] Step [160/174]: Train loss = 0.47051075994968417\n",
      "Epoch [184/200] Step [20/174]: Train loss = 0.4659125044941902\n",
      "Epoch [184/200] Step [40/174]: Train loss = 0.455551715195179\n",
      "Epoch [184/200] Step [60/174]: Train loss = 0.43908820748329164\n",
      "Epoch [184/200] Step [80/174]: Train loss = 0.4931872233748436\n",
      "Epoch [184/200] Step [100/174]: Train loss = 0.49331592619419096\n",
      "Epoch [184/200] Step [120/174]: Train loss = 0.48433118909597395\n",
      "Epoch [184/200] Step [140/174]: Train loss = 0.44980548620224\n",
      "Epoch [184/200] Step [160/174]: Train loss = 0.48792659789323806\n",
      "Epoch [185/200] Step [20/174]: Train loss = 0.47528479546308516\n",
      "Epoch [185/200] Step [40/174]: Train loss = 0.5047509267926216\n",
      "Epoch [185/200] Step [60/174]: Train loss = 0.4828603208065033\n",
      "Epoch [185/200] Step [80/174]: Train loss = 0.46550202965736387\n",
      "Epoch [185/200] Step [100/174]: Train loss = 0.5007644578814506\n",
      "Epoch [185/200] Step [120/174]: Train loss = 0.4538597196340561\n",
      "Epoch [185/200] Step [140/174]: Train loss = 0.42501351833343504\n",
      "Epoch [185/200] Step [160/174]: Train loss = 0.46731110513210294\n",
      "Epoch [186/200] Step [20/174]: Train loss = 0.46006172001361845\n",
      "Epoch [186/200] Step [40/174]: Train loss = 0.4660265102982521\n",
      "Epoch [186/200] Step [60/174]: Train loss = 0.4329203173518181\n",
      "Epoch [186/200] Step [80/174]: Train loss = 0.44109964519739153\n",
      "Epoch [186/200] Step [100/174]: Train loss = 0.451824414730072\n",
      "Epoch [186/200] Step [120/174]: Train loss = 0.45341850221157076\n",
      "Epoch [186/200] Step [140/174]: Train loss = 0.43496456146240237\n",
      "Epoch [186/200] Step [160/174]: Train loss = 0.44733681827783583\n",
      "Epoch [187/200] Step [20/174]: Train loss = 0.4518313854932785\n",
      "Epoch [187/200] Step [40/174]: Train loss = 0.46572080701589585\n",
      "Epoch [187/200] Step [60/174]: Train loss = 0.45386490821838377\n",
      "Epoch [187/200] Step [80/174]: Train loss = 0.4601781517267227\n",
      "Epoch [187/200] Step [100/174]: Train loss = 0.44991594851016997\n",
      "Epoch [187/200] Step [120/174]: Train loss = 0.4615289315581322\n",
      "Epoch [187/200] Step [140/174]: Train loss = 0.4180379301309586\n",
      "Epoch [187/200] Step [160/174]: Train loss = 0.47349192649126054\n",
      "Epoch [188/200] Step [20/174]: Train loss = 0.46272130608558654\n",
      "Epoch [188/200] Step [40/174]: Train loss = 0.42934814244508746\n",
      "Epoch [188/200] Step [60/174]: Train loss = 0.4521435558795929\n",
      "Epoch [188/200] Step [80/174]: Train loss = 0.4702028721570969\n",
      "Epoch [188/200] Step [100/174]: Train loss = 0.4770128667354584\n",
      "Epoch [188/200] Step [120/174]: Train loss = 0.46383619159460066\n",
      "Epoch [188/200] Step [140/174]: Train loss = 0.4324765756726265\n",
      "Epoch [188/200] Step [160/174]: Train loss = 0.43192372620105746\n",
      "Epoch [189/200] Step [20/174]: Train loss = 0.46722710132598877\n",
      "Epoch [189/200] Step [40/174]: Train loss = 0.4268075928092003\n",
      "Epoch [189/200] Step [60/174]: Train loss = 0.4452026471495628\n",
      "Epoch [189/200] Step [80/174]: Train loss = 0.446065753698349\n",
      "Epoch [189/200] Step [100/174]: Train loss = 0.4561066865921021\n",
      "Epoch [189/200] Step [120/174]: Train loss = 0.44108507633209226\n",
      "Epoch [189/200] Step [140/174]: Train loss = 0.44843671917915345\n",
      "Epoch [189/200] Step [160/174]: Train loss = 0.4272375702857971\n",
      "Epoch [190/200] Step [20/174]: Train loss = 0.4398725777864456\n",
      "Epoch [190/200] Step [40/174]: Train loss = 0.4505317732691765\n",
      "Epoch [190/200] Step [60/174]: Train loss = 0.45955619812011717\n",
      "Epoch [190/200] Step [80/174]: Train loss = 0.4316476285457611\n",
      "Epoch [190/200] Step [100/174]: Train loss = 0.44267001152038576\n",
      "Epoch [190/200] Step [120/174]: Train loss = 0.4497864380478859\n",
      "Epoch [190/200] Step [140/174]: Train loss = 0.4394439861178398\n",
      "Epoch [190/200] Step [160/174]: Train loss = 0.4443523272871971\n",
      "Epoch [191/200] Step [20/174]: Train loss = 0.46777805835008623\n",
      "Epoch [191/200] Step [40/174]: Train loss = 0.4044406250119209\n",
      "Epoch [191/200] Step [60/174]: Train loss = 0.4662471279501915\n",
      "Epoch [191/200] Step [80/174]: Train loss = 0.4392452657222748\n",
      "Epoch [191/200] Step [100/174]: Train loss = 0.4786656066775322\n",
      "Epoch [191/200] Step [120/174]: Train loss = 0.4647290036082268\n",
      "Epoch [191/200] Step [140/174]: Train loss = 0.4150906652212143\n",
      "Epoch [191/200] Step [160/174]: Train loss = 0.4282461583614349\n",
      "Epoch [192/200] Step [20/174]: Train loss = 0.44154525101184844\n",
      "Epoch [192/200] Step [40/174]: Train loss = 0.44868467152118685\n",
      "Epoch [192/200] Step [60/174]: Train loss = 0.47791700065135956\n",
      "Epoch [192/200] Step [80/174]: Train loss = 0.4415931612253189\n",
      "Epoch [192/200] Step [100/174]: Train loss = 0.4554889649152756\n",
      "Epoch [192/200] Step [120/174]: Train loss = 0.46984865665435793\n",
      "Epoch [192/200] Step [140/174]: Train loss = 0.4272456899285316\n",
      "Epoch [192/200] Step [160/174]: Train loss = 0.4521152377128601\n",
      "Epoch [193/200] Step [20/174]: Train loss = 0.45767267793416977\n",
      "Epoch [193/200] Step [40/174]: Train loss = 0.4511054143309593\n",
      "Epoch [193/200] Step [60/174]: Train loss = 0.44113912582397463\n",
      "Epoch [193/200] Step [80/174]: Train loss = 0.44729231894016264\n",
      "Epoch [193/200] Step [100/174]: Train loss = 0.44279688149690627\n",
      "Epoch [193/200] Step [120/174]: Train loss = 0.4488730564713478\n",
      "Epoch [193/200] Step [140/174]: Train loss = 0.4196929469704628\n",
      "Epoch [193/200] Step [160/174]: Train loss = 0.46898943483829497\n",
      "Epoch [194/200] Step [20/174]: Train loss = 0.4593165501952171\n",
      "Epoch [194/200] Step [40/174]: Train loss = 0.46170713007450104\n",
      "Epoch [194/200] Step [60/174]: Train loss = 0.43099997490644454\n",
      "Epoch [194/200] Step [80/174]: Train loss = 0.4856815040111542\n",
      "Epoch [194/200] Step [100/174]: Train loss = 0.45519098490476606\n",
      "Epoch [194/200] Step [120/174]: Train loss = 0.4120643734931946\n",
      "Epoch [194/200] Step [140/174]: Train loss = 0.4259446546435356\n",
      "Epoch [194/200] Step [160/174]: Train loss = 0.41864276826381686\n",
      "Epoch [195/200] Step [20/174]: Train loss = 0.4254057705402374\n",
      "Epoch [195/200] Step [40/174]: Train loss = 0.4292719930410385\n",
      "Epoch [195/200] Step [60/174]: Train loss = 0.4514904454350471\n",
      "Epoch [195/200] Step [80/174]: Train loss = 0.4362852945923805\n",
      "Epoch [195/200] Step [100/174]: Train loss = 0.46398031860589983\n",
      "Epoch [195/200] Step [120/174]: Train loss = 0.4517154946923256\n",
      "Epoch [195/200] Step [140/174]: Train loss = 0.42391215413808825\n",
      "Epoch [195/200] Step [160/174]: Train loss = 0.45736629217863084\n",
      "Epoch [196/200] Step [20/174]: Train loss = 0.44479550421237946\n",
      "Epoch [196/200] Step [40/174]: Train loss = 0.4352817863225937\n",
      "Epoch [196/200] Step [60/174]: Train loss = 0.40865400731563567\n",
      "Epoch [196/200] Step [80/174]: Train loss = 0.42951608896255494\n",
      "Epoch [196/200] Step [100/174]: Train loss = 0.43565377593040466\n",
      "Epoch [196/200] Step [120/174]: Train loss = 0.4268390521407127\n",
      "Epoch [196/200] Step [140/174]: Train loss = 0.4434508889913559\n",
      "Epoch [196/200] Step [160/174]: Train loss = 0.42989297658205033\n",
      "Epoch [197/200] Step [20/174]: Train loss = 0.44537612348794936\n",
      "Epoch [197/200] Step [40/174]: Train loss = 0.4336354658007622\n",
      "Epoch [197/200] Step [60/174]: Train loss = 0.40769096463918686\n",
      "Epoch [197/200] Step [80/174]: Train loss = 0.41118512600660323\n",
      "Epoch [197/200] Step [100/174]: Train loss = 0.44070169180631635\n",
      "Epoch [197/200] Step [120/174]: Train loss = 0.4020629473030567\n",
      "Epoch [197/200] Step [140/174]: Train loss = 0.40120711028575895\n",
      "Epoch [197/200] Step [160/174]: Train loss = 0.3978170156478882\n",
      "Epoch [198/200] Step [20/174]: Train loss = 0.4376953959465027\n",
      "Epoch [198/200] Step [40/174]: Train loss = 0.40557200610637667\n",
      "Epoch [198/200] Step [60/174]: Train loss = 0.4172071240842342\n",
      "Epoch [198/200] Step [80/174]: Train loss = 0.43753446638584137\n",
      "Epoch [198/200] Step [100/174]: Train loss = 0.42949800193309784\n",
      "Epoch [198/200] Step [120/174]: Train loss = 0.4397102363407612\n",
      "Epoch [198/200] Step [140/174]: Train loss = 0.4135299578309059\n",
      "Epoch [198/200] Step [160/174]: Train loss = 0.4326033294200897\n",
      "Epoch [199/200] Step [20/174]: Train loss = 0.4135201096534729\n",
      "Epoch [199/200] Step [40/174]: Train loss = 0.4285233348608017\n",
      "Epoch [199/200] Step [60/174]: Train loss = 0.3830264151096344\n",
      "Epoch [199/200] Step [80/174]: Train loss = 0.4279264107346535\n",
      "Epoch [199/200] Step [100/174]: Train loss = 0.46683857291936875\n",
      "Epoch [199/200] Step [120/174]: Train loss = 0.4286441907286644\n",
      "Epoch [199/200] Step [140/174]: Train loss = 0.40478423833847044\n",
      "Epoch [199/200] Step [160/174]: Train loss = 0.42642317712306976\n",
      "Epoch [200/200] Step [20/174]: Train loss = 0.4293401002883911\n",
      "Epoch [200/200] Step [40/174]: Train loss = 0.46632828563451767\n",
      "Epoch [200/200] Step [60/174]: Train loss = 0.41664905697107313\n",
      "Epoch [200/200] Step [80/174]: Train loss = 0.4147277161478996\n",
      "Epoch [200/200] Step [100/174]: Train loss = 0.4162426397204399\n",
      "Epoch [200/200] Step [120/174]: Train loss = 0.44203648567199705\n",
      "Epoch [200/200] Step [140/174]: Train loss = 0.4161409676074982\n",
      "Epoch [200/200] Step [160/174]: Train loss = 0.4720524877309799\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    epoch_train_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # forward pass\n",
    "        #print(batch[\"input_ids\"].size())\n",
    "        #batch[\"input_ids\"].to(device)\n",
    "        #batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=batch['input_ids'].to(device),labels=batch['labels'].to(device))\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        epoch_train_loss+=train_loss\n",
    "        train_loss_step.append(loss.item())\n",
    "        \n",
    "        # log training progress\n",
    "        if (step + 1) % log_interval == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] Step [{step + 1}/{len(train_dataloader)}]: Train loss = {train_loss / log_interval}\")\n",
    "            train_loss = 0\n",
    "            \n",
    "    train_loss_epoch.append(epoch_train_loss / len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c69aab81-a93d-4344-9d81-d5c07a296df6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T02:47:09.514374Z",
     "iopub.status.busy": "2023-04-14T02:47:09.514167Z",
     "iopub.status.idle": "2023-04-14T02:47:10.019582Z",
     "shell.execute_reply": "2023-04-14T02:47:10.018927Z",
     "shell.execute_reply.started": "2023-04-14T02:47:09.514353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss_step, label='Training Loss (Step)')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f\"{author} Step Learning Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"learning_curves/{author}_Step_Learning_Curve.png\")\n",
    "plt.show()\n",
    "plt.plot(train_loss_epoch, label='Training Loss (Epoch)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f\"{author} Epoch Learning Curve\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"learning_curves/{author}_Epoch_Learning_Curve.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f228762-803b-4858-b548-4322bf84bbcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-14T02:47:08.936040Z",
     "iopub.status.busy": "2023-04-14T02:47:08.935855Z",
     "iopub.status.idle": "2023-04-14T02:47:09.509050Z",
     "shell.execute_reply": "2023-04-14T02:47:09.508120Z",
     "shell.execute_reply.started": "2023-04-14T02:47:08.936021Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"./{author}-finetuned/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3bbf016b-9fdc-4794-8cc8-90602bf6d6c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T04:27:20.931314Z",
     "iopub.status.busy": "2023-03-30T04:27:20.930623Z",
     "iopub.status.idle": "2023-03-30T04:27:21.689731Z",
     "shell.execute_reply": "2023-03-30T04:27:21.688644Z",
     "shell.execute_reply.started": "2023-03-30T04:27:20.931287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at pater infelix, nec iam pater, icare, dixit, icare, dixit ubi es? qua te regione requiram? icare dicebat :'fluctibus ille nomen.'omnibus est, ait, tellus : pater est locus illi. quid facerat? quem fugiens inimicus amabat, proximus huic terrae frater erat. fluctibus actum erat. omnibus hoc visum est : venit enim, mox \" nominat \" unus, \" et vidi? \" an \" dixi \" quod lumen habebat? \" \" hoc\n"
     ]
    }
   ],
   "source": [
    "bee_input_text = \"Qualis apes aestate nova per florea rura exercet sub sole labor, cum gentis adultos educunt fetus,\".lower()\n",
    "input_text = \"At pater infelix, nec iam pater, Icare, dixit, Icare, dixit ubi es? qua te regione requiram? Icare dicebat:\".lower()\n",
    "input_text_2 = \"Rapidi vicinia solis mollit odoratas, pennarum vincula, ceras. Tabuerant cerae: nudos quatit ille lacertos, remigioque carens non ullas percipit auras, oraque caerulea patrium clamantia nomen excipiuntur aqua: quae nomen traxit ab illo.\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "outputs = model.generate(input_ids=input_ids.to(device), max_length=50, do_sample=True)\n",
    "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "090fad48-aa7e-4b7e-a7e3-d3e141c9f702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T04:26:04.037219Z",
     "iopub.status.busy": "2023-03-30T04:26:04.036182Z",
     "iopub.status.idle": "2023-03-30T04:26:04.043391Z",
     "shell.execute_reply": "2023-03-30T04:26:04.042412Z",
     "shell.execute_reply.started": "2023-03-30T04:26:04.037191Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(f\"Results/{author}_bert_gen.txt\",\"w+\") as f:\n",
    "    f.write(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0206e3a-36df-420d-ac4c-766d84ccb142",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-30T01:15:18.807878Z",
     "iopub.status.busy": "2023-03-30T01:15:18.807530Z",
     "iopub.status.idle": "2023-03-30T01:15:18.814020Z",
     "shell.execute_reply": "2023-03-30T01:15:18.813254Z",
     "shell.execute_reply.started": "2023-03-30T01:15:18.807851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'gallia_',\n",
       " 'est_',\n",
       " 'omnis_',\n",
       " 'divisa_',\n",
       " 'in_',\n",
       " 'partes_',\n",
       " 'tres_',\n",
       " ',_',\n",
       " 'quarum_',\n",
       " 'unam_',\n",
       " 'incolu',\n",
       " 'nt_',\n",
       " 'bel',\n",
       " 'gae_',\n",
       " ',_',\n",
       " 'aliam_',\n",
       " 'aqui',\n",
       " 'tani_',\n",
       " ',_',\n",
       " 'tertiam_',\n",
       " 'qui_',\n",
       " 'ipsorum_',\n",
       " 'lingua_',\n",
       " 'celt',\n",
       " 'ae_',\n",
       " ',_',\n",
       " 'nostra_',\n",
       " 'galli_',\n",
       " 'appellantur_',\n",
       " '._',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791747ab-d943-4657-90dc-0402957c1e56",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b78cfc-b584-4480-8cc2-2f621c73a8c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T04:28:07.946666Z",
     "iopub.status.busy": "2023-05-13T04:28:07.945972Z",
     "iopub.status.idle": "2023-05-13T04:28:12.847801Z",
     "shell.execute_reply": "2023-05-13T04:28:12.847165Z",
     "shell.execute_reply.started": "2023-05-13T04:28:07.946640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32900, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32900, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./LatinBERT/\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"./Caesar-finetuned/\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5462dd3-d9c2-4c23-a506-8ec2a1d01984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-13T04:30:23.315867Z",
     "iopub.status.busy": "2023-05-13T04:30:23.315552Z",
     "iopub.status.idle": "2023-05-13T04:30:23.618820Z",
     "shell.execute_reply": "2023-05-13T04:30:23.617792Z",
     "shell.execute_reply.started": "2023-05-13T04:30:23.315842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertForMaskedLM'>\n",
      "olim nomine rufo iuvenis ingeniosus erat, sed numero hominum, quos illi attribuit, equos attribuit, arma, equos, equos attribuit. huic se atque suos attribuit ; huic marcum brutum praeficit. his copiis quintum um praeficit ; huic marcum\n"
     ]
    }
   ],
   "source": [
    "prompt = \"olim nomine rufo iuvenis ingeniosus erat,\".lower()\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "outputs = model.generate(input_ids=input_ids.to(device), max_length=50, do_sample=True)\n",
    "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(type(model))\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d6ad4-d9ea-4bc4-b49b-8a5debb736f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
