{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50580ed9-28e6-4edf-aba8-c908cf76d750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:13:27.513659Z",
     "iopub.status.busy": "2023-03-26T15:13:27.513048Z",
     "iopub.status.idle": "2023-03-26T15:13:32.168484Z",
     "shell.execute_reply": "2023-03-26T15:13:32.167773Z",
     "shell.execute_reply.started": "2023-03-26T15:13:27.513635Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, re, warnings\n",
    "from Data import dataExp\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from LatinBERT.LatinTok import LatinTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "    from cltk.tokenizers.lat.lat import LatinWordTokenizer as WordTokenizer\n",
    "    from cltk.tokenizers.lat.lat import LatinPunktSentenceTokenizer as SentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24f46b20-4991-4f23-916c-3fa1887f7d80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:48:06.107129Z",
     "iopub.status.busy": "2023-03-26T15:48:06.106339Z",
     "iopub.status.idle": "2023-03-26T15:48:06.110267Z",
     "shell.execute_reply": "2023-03-26T15:48:06.109920Z",
     "shell.execute_reply.started": "2023-03-26T15:48:06.107093Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 \n",
    "context_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ece4dd6-8edd-4d84-9020-da086e1989ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:13:37.885962Z",
     "iopub.status.busy": "2023-03-26T15:13:37.885412Z",
     "iopub.status.idle": "2023-03-26T15:13:41.635368Z",
     "shell.execute_reply": "2023-03-26T15:13:41.634825Z",
     "shell.execute_reply.started": "2023-03-26T15:13:37.885934Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /notebooks/LatinBERT/latin_bert were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(32900, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=32900, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load in the tokenizer and Bert Model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizerPath = os.getcwd()+\"/LatinBERT/latin.subword.encoder\"\n",
    "bertPath = os.getcwd()+\"/LatinBERT/latin_bert\"\n",
    "encoder = text_encoder.SubwordTextEncoder(tokenizerPath)\n",
    "tokenizer = LatinTokenizer(encoder)\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bertPath)\n",
    "model.to(device)\n",
    "print(model.eval())\n",
    "\n",
    "#st = SentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47526a6-3c1c-415b-ac6c-5727d74b5847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:14:41.038017Z",
     "iopub.status.busy": "2023-03-26T15:14:41.037353Z",
     "iopub.status.idle": "2023-03-26T15:14:43.513657Z",
     "shell.execute_reply": "2023-03-26T15:14:43.513042Z",
     "shell.execute_reply.started": "2023-03-26T15:14:41.037991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the existing corpus\n",
      "abbofloracensis had 1 pieces of work with a total of 5403 characters of text\n",
      "abelard had 1 pieces of work with a total of 13784 characters of text\n",
      "acticussincerius had 1 pieces of work with a total of 340 characters of text\n",
      "addison had 1 pieces of work with a total of 296 characters of text\n",
      "adso had 1 pieces of work with a total of 2351 characters of text\n",
      "aelredus had 1 pieces of work with a total of 20901 characters of text\n",
      "agnes had 1 pieces of work with a total of 11811 characters of text\n",
      "alanus had 1 pieces of work with a total of 34012 characters of text\n",
      "albericodamarcellise had 1 pieces of work with a total of 27 characters of text\n",
      "albertanus had 1 pieces of work with a total of 2869 characters of text\n",
      "albertofaix had 1 pieces of work with a total of 14123 characters of text\n",
      "alcuin had 1 pieces of work with a total of 487 characters of text\n",
      "aleandrogerolamo had 1 pieces of work with a total of 654 characters of text\n",
      "alfonsi had 1 pieces of work with a total of 17894 characters of text\n",
      "ambrose had 1 pieces of work with a total of 1246 characters of text\n",
      "ammianus had 18 pieces of work with a total of 152277 characters of text\n",
      "ampelius had 1 pieces of work with a total of 8667 characters of text\n",
      "andecavis had 1 pieces of work with a total of 161 characters of text\n",
      "andreasbergoma had 1 pieces of work with a total of 4834 characters of text\n",
      "andronicus had 1 pieces of work with a total of 557 characters of text\n",
      "angeloambrogini had 1 pieces of work with a total of 458 characters of text\n",
      "angelopoliziano had 1 pieces of work with a total of 469 characters of text\n",
      "angilbert had 1 pieces of work with a total of 382 characters of text\n",
      "annalesregnifrancorum had 1 pieces of work with a total of 25076 characters of text\n",
      "annalesvedastini had 1 pieces of work with a total of 8907 characters of text\n",
      "anon had 1 pieces of work with a total of 8621 characters of text\n",
      "anonymous had 1 pieces of work with a total of 1039 characters of text\n",
      "anselmepistula had 1 pieces of work with a total of 811 characters of text\n",
      "anselmproslogion had 1 pieces of work with a total of 7910 characters of text\n",
      "apicius had 1 pieces of work with a total of 1654 characters of text\n",
      "appverg had 1 pieces of work with a total of 12 characters of text\n",
      "appvergcomp had 1 pieces of work with a total of 18951 characters of text\n",
      "appvergculex had 1 pieces of work with a total of 3123 characters of text\n",
      "apuleius had 13 pieces of work with a total of 96788 characters of text\n",
      "aquinas had 1 pieces of work with a total of 1126 characters of text\n",
      "arbroath had 1 pieces of work with a total of 1194 characters of text\n",
      "archpoet had 1 pieces of work with a total of 5323 characters of text\n",
      "aristotle had 1 pieces of work with a total of 1952 characters of text\n",
      "arnobius had 1 pieces of work with a total of 6772 characters of text\n",
      "arnulf had 1 pieces of work with a total of 236 characters of text\n",
      "asconius had 1 pieces of work with a total of 19216 characters of text\n",
      "asserius had 1 pieces of work with a total of 14621 characters of text\n",
      "augustine had 7 pieces of work with a total of 69080 characters of text\n",
      "aureliusvictor had 2 pieces of work with a total of 23206 characters of text\n",
      "aus had 1 pieces of work with a total of 428 characters of text\n",
      "ausonius had 24 pieces of work with a total of 51304 characters of text\n",
      "ave had 1 pieces of work with a total of 1136 characters of text\n",
      "avianus had 1 pieces of work with a total of 5264 characters of text\n",
      "avienus had 1 pieces of work with a total of 4552 characters of text\n",
      "axio had 1 pieces of work with a total of 2845 characters of text\n",
      "bacon had 1 pieces of work with a total of 7934 characters of text\n",
      "balbus had 1 pieces of work with a total of 2662 characters of text\n",
      "balde had 1 pieces of work with a total of 127 characters of text\n",
      "baldo had 1 pieces of work with a total of 11374 characters of text\n",
      "bebel had 1 pieces of work with a total of 22436 characters of text\n",
      "bede had 5 pieces of work with a total of 95877 characters of text\n",
      "benedict had 1 pieces of work with a total of 15529 characters of text\n",
      "berengar had 1 pieces of work with a total of 6440 characters of text\n",
      "bernardcluny had 1 pieces of work with a total of 9267 characters of text\n",
      "bible had 1 pieces of work with a total of 1902 characters of text\n",
      "biggs had 1 pieces of work with a total of 7315 characters of text\n",
      "bill had 1 pieces of work with a total of 540 characters of text\n",
      "blesensis had 1 pieces of work with a total of 9500 characters of text\n",
      "boethius had 10 pieces of work with a total of 43252 characters of text\n",
      "boethiusdacia had 1 pieces of work with a total of 9053 characters of text\n",
      "bonaventura had 1 pieces of work with a total of 12067 characters of text\n",
      "boskovic had 1 pieces of work with a total of 5088 characters of text\n",
      "brevechronicon had 1 pieces of work with a total of 1427 characters of text\n",
      "buchanan had 1 pieces of work with a total of 6163 characters of text\n",
      "bultelius had 1 pieces of work with a total of 5655 characters of text\n",
      "caeciliusbalbus had 1 pieces of work with a total of 8511 characters of text\n",
      "caesar had 11 pieces of work with a total of 96121 characters of text\n",
      "caesaraugustus had 1 pieces of work with a total of 2930 characters of text\n",
      "calpurniusflaccus had 1 pieces of work with a total of 8733 characters of text\n",
      "calpurniussiculus had 1 pieces of work with a total of 6110 characters of text\n",
      "campion had 1 pieces of work with a total of 3287 characters of text\n",
      "capellanus had 1 pieces of work with a total of 38711 characters of text\n",
      "carm had 1 pieces of work with a total of 300 characters of text\n",
      "carmenarvale had 1 pieces of work with a total of 97 characters of text\n",
      "carmeninvictoriam had 1 pieces of work with a total of 2342 characters of text\n",
      "carmensaliare had 1 pieces of work with a total of 57 characters of text\n",
      "cassiodorus had 1 pieces of work with a total of 10326 characters of text\n",
      "catalogueliberien had 1 pieces of work with a total of 1541 characters of text\n",
      "cato had 1 pieces of work with a total of 2762 characters of text\n",
      "catullus had 1 pieces of work with a total of 15439 characters of text\n",
      "celsus had 8 pieces of work with a total of 125627 characters of text\n",
      "celtis had 1 pieces of work with a total of 286 characters of text\n",
      "censorinus had 1 pieces of work with a total of 12234 characters of text\n",
      "cicero had 122 pieces of work with a total of 1303304 characters of text\n",
      "cinna had 1 pieces of work with a total of 134 characters of text\n",
      "claud had 1 pieces of work with a total of 911 characters of text\n",
      "claudian had 19 pieces of work with a total of 71409 characters of text\n",
      "clitophon had 1 pieces of work with a total of 1796 characters of text\n",
      "colman had 1 pieces of work with a total of 289 characters of text\n",
      "columba had 1 pieces of work with a total of 6402 characters of text\n",
      "columbus had 1 pieces of work with a total of 2703 characters of text\n",
      "columella had 9 pieces of work with a total of 92306 characters of text\n",
      "comes had 1 pieces of work with a total of 86 characters of text\n",
      "commodianus had 1 pieces of work with a total of 189 characters of text\n",
      "corippus had 8 pieces of work with a total of 38301 characters of text\n",
      "corneliopaoloamalteo had 1 pieces of work with a total of 98 characters of text\n",
      "corvinus had 1 pieces of work with a total of 793 characters of text\n",
      "cotta had 1 pieces of work with a total of 2364 characters of text\n",
      "creeds had 1 pieces of work with a total of 1114 characters of text\n",
      "curtius had 1 pieces of work with a total of 11655 characters of text\n",
      "curtiusrufus had 8 pieces of work with a total of 89377 characters of text\n",
      "dante had 1 pieces of work with a total of 7043 characters of text\n",
      "dantealighieri had 1 pieces of work with a total of 21 characters of text\n",
      "dares had 1 pieces of work with a total of 9495 characters of text\n",
      "debury had 1 pieces of work with a total of 17429 characters of text\n",
      "declaratio had 1 pieces of work with a total of 1609 characters of text\n",
      "decretum had 1 pieces of work with a total of 2288 characters of text\n",
      "descartes had 1 pieces of work with a total of 2922 characters of text\n",
      "dicquid had 1 pieces of work with a total of 64 characters of text\n",
      "diesirae had 1 pieces of work with a total of 275 characters of text\n",
      "diravi had 1 pieces of work with a total of 153 characters of text\n",
      "don had 1 pieces of work with a total of 5180 characters of text\n",
      "donation had 1 pieces of work with a total of 3147 characters of text\n",
      "dracontius had 16 pieces of work with a total of 45692 characters of text\n",
      "dumdiane had 1 pieces of work with a total of 259 characters of text\n",
      "dumdomus had 1 pieces of work with a total of 65 characters of text\n",
      "ebulo had 1 pieces of work with a total of 14004 characters of text\n",
      "egeria had 1 pieces of work with a total of 11242 characters of text\n",
      "ein had 1 pieces of work with a total of 8987 characters of text\n",
      "enn had 1 pieces of work with a total of 1439 characters of text\n",
      "ennius had 1 pieces of work with a total of 3811 characters of text\n",
      "ennodius had 1 pieces of work with a total of 13006 characters of text\n",
      "ep had 1 pieces of work with a total of 3291 characters of text\n",
      "epistaustras had 1 pieces of work with a total of 5319 characters of text\n",
      "epitaphs had 1 pieces of work with a total of 2235 characters of text\n",
      "epitomecononiana had 1 pieces of work with a total of 6930 characters of text\n",
      "epitomefeliciana had 1 pieces of work with a total of 7764 characters of text\n",
      "erasmus had 1 pieces of work with a total of 5576 characters of text\n",
      "erchempert had 1 pieces of work with a total of 14564 characters of text\n",
      "estas had 1 pieces of work with a total of 105 characters of text\n",
      "eucherius had 1 pieces of work with a total of 4755 characters of text\n",
      "eugenius had 1 pieces of work with a total of 100 characters of text\n",
      "eugippius had 1 pieces of work with a total of 13762 characters of text\n",
      "eutropius had 1 pieces of work with a total of 21868 characters of text\n",
      "exivi had 1 pieces of work with a total of 4912 characters of text\n",
      "fabe had 1 pieces of work with a total of 6053 characters of text\n",
      "falcandus had 1 pieces of work with a total of 41670 characters of text\n",
      "falcone had 1 pieces of work with a total of 44690 characters of text\n",
      "faustoandrelino had 1 pieces of work with a total of 801 characters of text\n",
      "ferraria had 1 pieces of work with a total of 3220 characters of text\n",
      "ficino had 1 pieces of work with a total of 3983 characters of text\n",
      "fletcher had 1 pieces of work with a total of 7096 characters of text\n",
      "florus had 1 pieces of work with a total of 30973 characters of text\n",
      "foedusaeternum had 1 pieces of work with a total of 561 characters of text\n",
      "forsett had 1 pieces of work with a total of 6118 characters of text\n",
      "fortunat had 1 pieces of work with a total of 116 characters of text\n",
      "fragmentumlaurentianum had 1 pieces of work with a total of 954 characters of text\n",
      "fredegarius had 1 pieces of work with a total of 27300 characters of text\n",
      "frodebertus had 1 pieces of work with a total of 1227 characters of text\n",
      "frontinus had 1 pieces of work with a total of 421 characters of text\n",
      "fronto had 1 pieces of work with a total of 54465 characters of text\n",
      "fulbert had 1 pieces of work with a total of 123 characters of text\n",
      "fulgentius had 1 pieces of work with a total of 5993 characters of text\n",
      "gaius had 1 pieces of work with a total of 15402 characters of text\n",
      "galileo had 1 pieces of work with a total of 12487 characters of text\n",
      "garcilaso had 1 pieces of work with a total of 1165 characters of text\n",
      "garland had 1 pieces of work with a total of 3760 characters of text\n",
      "gaud had 1 pieces of work with a total of 154 characters of text\n",
      "gauss had 1 pieces of work with a total of 12162 characters of text\n",
      "gellius had 21 pieces of work with a total of 137854 characters of text\n",
      "germanicus had 1 pieces of work with a total of 5622 characters of text\n",
      "gestafrancorum had 1 pieces of work with a total of 1403 characters of text\n",
      "gestarom had 1 pieces of work with a total of 19688 characters of text\n",
      "gioacchino had 1 pieces of work with a total of 25292 characters of text\n",
      "girolamoaccelini had 1 pieces of work with a total of 38723 characters of text\n",
      "girolamoamaseo had 1 pieces of work with a total of 2798 characters of text\n",
      "glass had 1 pieces of work with a total of 31324 characters of text\n",
      "godfrey had 1 pieces of work with a total of 1822 characters of text\n",
      "grattius had 1 pieces of work with a total of 4243 characters of text\n",
      "gravissimas had 1 pieces of work with a total of 2096 characters of text\n",
      "greg had 1 pieces of work with a total of 897 characters of text\n",
      "gregdecretals had 1 pieces of work with a total of 37625 characters of text\n",
      "gregory had 1 pieces of work with a total of 18319 characters of text\n",
      "gregorytours had 1 pieces of work with a total of 9668 characters of text\n",
      "gwinne had 1 pieces of work with a total of 8072 characters of text\n",
      "halley had 1 pieces of work with a total of 369 characters of text\n",
      "hebet had 1 pieces of work with a total of 121 characters of text\n",
      "henry had 1 pieces of work with a total of 1371 characters of text\n",
      "henrysettimello had 1 pieces of work with a total of 8194 characters of text\n",
      "hipp had 1 pieces of work with a total of 2792 characters of text\n",
      "histapoll had 1 pieces of work with a total of 14613 characters of text\n",
      "histbrit had 1 pieces of work with a total of 11314 characters of text\n",
      "holberg had 1 pieces of work with a total of 34977 characters of text\n",
      "horace had 10 pieces of work with a total of 52219 characters of text\n",
      "hrabanus had 1 pieces of work with a total of 206 characters of text\n",
      "hugo had 1 pieces of work with a total of 6453 characters of text\n",
      "hydatiuschronicon had 1 pieces of work with a total of 6571 characters of text\n",
      "hydatiusfasti had 1 pieces of work with a total of 10086 characters of text\n",
      "hyginus had 1 pieces of work with a total of 3878 characters of text\n",
      "hymni had 1 pieces of work with a total of 2136 characters of text\n",
      "iabervocius had 1 pieces of work with a total of 770 characters of text\n",
      "iamdulcis had 1 pieces of work with a total of 232 characters of text\n",
      "ilias had 1 pieces of work with a total of 7615 characters of text\n",
      "index had 1 pieces of work with a total of 213 characters of text\n",
      "indices had 1 pieces of work with a total of 246 characters of text\n",
      "innocent had 1 pieces of work with a total of 18725 characters of text\n",
      "inquisitio had 1 pieces of work with a total of 4941 characters of text\n",
      "inscriptions had 1 pieces of work with a total of 1215 characters of text\n",
      "iordanes had 1 pieces of work with a total of 21090 characters of text\n",
      "ipsavivere had 1 pieces of work with a total of 99 characters of text\n",
      "isidore had 1 pieces of work with a total of 23566 characters of text\n",
      "italicus had 1 pieces of work with a total of 7605 characters of text\n",
      "jacopoallegretti had 1 pieces of work with a total of 837 characters of text\n",
      "janus had 1 pieces of work with a total of 943 characters of text\n",
      "jerome had 80 pieces of work with a total of 1041707 characters of text\n",
      "jfkhonor had 1 pieces of work with a total of 272 characters of text\n",
      "johannes had 1 pieces of work with a total of 14602 characters of text\n",
      "junillus had 1 pieces of work with a total of 13641 characters of text\n",
      "justin had 1 pieces of work with a total of 1203 characters of text\n",
      "justinian had 1 pieces of work with a total of 22680 characters of text\n",
      "juvenal had 1 pieces of work with a total of 29189 characters of text\n",
      "juvencus had 4 pieces of work with a total of 23355 characters of text\n",
      "kalila had 1 pieces of work with a total of 36502 characters of text\n",
      "kempis had 1 pieces of work with a total of 6191 characters of text\n",
      "kepler had 1 pieces of work with a total of 7555 characters of text\n",
      "lactantius had 1 pieces of work with a total of 13175 characters of text\n",
      "landor had 1 pieces of work with a total of 3837 characters of text\n",
      "legenda had 1 pieces of work with a total of 3821 characters of text\n",
      "leo had 1 pieces of work with a total of 6667 characters of text\n",
      "leothegreat had 1 pieces of work with a total of 1205 characters of text\n",
      "letabundus had 1 pieces of work with a total of 162 characters of text\n",
      "levis had 1 pieces of work with a total of 114 characters of text\n",
      "lhomond had 1 pieces of work with a total of 33933 characters of text\n",
      "liberpontificalis had 1 pieces of work with a total of 19308 characters of text\n",
      "livy had 4 pieces of work with a total of 592961 characters of text\n",
      "lotichius had 1 pieces of work with a total of 182 characters of text\n",
      "lucan had 10 pieces of work with a total of 62938 characters of text\n",
      "lucernarium had 1 pieces of work with a total of 90 characters of text\n",
      "lucretius had 6 pieces of work with a total of 55778 characters of text\n",
      "luther had 1 pieces of work with a total of 894 characters of text\n",
      "macarius had 1 pieces of work with a total of 6172 characters of text\n",
      "macrobius had 8 pieces of work with a total of 102569 characters of text\n",
      "magnacarta had 1 pieces of work with a total of 4287 characters of text\n",
      "maidstone had 1 pieces of work with a total of 2047 characters of text\n",
      "malaterra had 1 pieces of work with a total of 11862 characters of text\n",
      "manilius had 5 pieces of work with a total of 31644 characters of text\n",
      "mapps had 1 pieces of work with a total of 1983 characters of text\n",
      "marbodus had 1 pieces of work with a total of 2107 characters of text\n",
      "marcantonioaldegati had 1 pieces of work with a total of 8863 characters of text\n",
      "marcellinus had 1 pieces of work with a total of 11561 characters of text\n",
      "marcusmincuiusfelix had 1 pieces of work with a total of 13821 characters of text\n",
      "martial had 14 pieces of work with a total of 50707 characters of text\n",
      "martinbraga had 1 pieces of work with a total of 1472 characters of text\n",
      "marullo had 1 pieces of work with a total of 470 characters of text\n",
      "marx had 1 pieces of work with a total of 1017 characters of text\n",
      "maximianus had 1 pieces of work with a total of 5099 characters of text\n",
      "may had 1 pieces of work with a total of 3897 characters of text\n",
      "melanchthon had 1 pieces of work with a total of 2106 characters of text\n",
      "milton had 1 pieces of work with a total of 1761 characters of text\n",
      "minucius had 1 pieces of work with a total of 14018 characters of text\n",
      "mirabilia had 1 pieces of work with a total of 5182 characters of text\n",
      "mirandola had 1 pieces of work with a total of 65 characters of text\n",
      "montanus had 1 pieces of work with a total of 966 characters of text\n",
      "more had 1 pieces of work with a total of 33136 characters of text\n",
      "musavenit had 1 pieces of work with a total of 117 characters of text\n",
      "naevius had 1 pieces of work with a total of 1036 characters of text\n",
      "navagero had 1 pieces of work with a total of 8263 characters of text\n",
      "nemesianus had 1 pieces of work with a total of 690 characters of text\n",
      "nepos had 1 pieces of work with a total of 33582 characters of text\n",
      "newton had 1 pieces of work with a total of 669 characters of text\n",
      "nithardus had 1 pieces of work with a total of 3559 characters of text\n",
      "nobilis had 1 pieces of work with a total of 114 characters of text\n",
      "notitia had 1 pieces of work with a total of 6058 characters of text\n",
      "novatian had 1 pieces of work with a total of 24333 characters of text\n",
      "obsequens had 1 pieces of work with a total of 6643 characters of text\n",
      "omnegenus had 1 pieces of work with a total of 216 characters of text\n",
      "oratio had 1 pieces of work with a total of 1028 characters of text\n",
      "oresmius had 1 pieces of work with a total of 11491 characters of text\n",
      "origo had 1 pieces of work with a total of 1229 characters of text\n",
      "orosius had 1 pieces of work with a total of 12399 characters of text\n",
      "ottofreising had 1 pieces of work with a total of 18211 characters of text\n",
      "ovid had 41 pieces of work with a total of 268300 characters of text\n",
      "owen had 1 pieces of work with a total of 604 characters of text\n",
      "paris had 1 pieces of work with a total of 832 characters of text\n",
      "pascoli had 1 pieces of work with a total of 1094 characters of text\n",
      "passerat had 1 pieces of work with a total of 607 characters of text\n",
      "patricius had 1 pieces of work with a total of 4333 characters of text\n",
      "pauldeacon had 1 pieces of work with a total of 2228 characters of text\n",
      "paulinus had 1 pieces of work with a total of 30411 characters of text\n",
      "paulusdiaconus had 1 pieces of work with a total of 11632 characters of text\n",
      "perp had 1 pieces of work with a total of 4314 characters of text\n",
      "persius had 1 pieces of work with a total of 5596 characters of text\n",
      "pervig had 1 pieces of work with a total of 772 characters of text\n",
      "petrarch had 1 pieces of work with a total of 1088 characters of text\n",
      "petrarchmedicus had 1 pieces of work with a total of 27546 characters of text\n",
      "petronius had 1 pieces of work with a total of 36066 characters of text\n",
      "petroniusfrag had 1 pieces of work with a total of 1455 characters of text\n",
      "phaedr had 1 pieces of work with a total of 1220 characters of text\n",
      "phaedrapp had 1 pieces of work with a total of 3279 characters of text\n",
      "piccolomini had 1 pieces of work with a total of 731 characters of text\n",
      "planctus had 1 pieces of work with a total of 541 characters of text\n",
      "plautus had 20 pieces of work with a total of 210574 characters of text\n",
      "pliny had 1 pieces of work with a total of 7723 characters of text\n",
      "plinytheelder had 7 pieces of work with a total of 475768 characters of text\n",
      "plinytheyounger had 10 pieces of work with a total of 79798 characters of text\n",
      "poggio had 1 pieces of work with a total of 17048 characters of text\n",
      "polignac had 3 pieces of work with a total of 152574 characters of text\n",
      "pomponius had 1 pieces of work with a total of 7085 characters of text\n",
      "pontano had 1 pieces of work with a total of 1159 characters of text\n",
      "poree had 1 pieces of work with a total of 11176 characters of text\n",
      "porphyrius had 1 pieces of work with a total of 6102 characters of text\n",
      "potatores had 1 pieces of work with a total of 188 characters of text\n",
      "prataiam had 1 pieces of work with a total of 149 characters of text\n",
      "prec had 1 pieces of work with a total of 265 characters of text\n",
      "precatio had 1 pieces of work with a total of 148 characters of text\n",
      "priapea had 1 pieces of work with a total of 4429 characters of text\n",
      "priscian had 4 pieces of work with a total of 20747 characters of text\n",
      "professio had 1 pieces of work with a total of 1385 characters of text\n",
      "prop had 1 pieces of work with a total of 7315 characters of text\n",
      "propertius had 1 pieces of work with a total of 30354 characters of text\n",
      "prosperus had 1 pieces of work with a total of 5264 characters of text\n",
      "protospatarius had 1 pieces of work with a total of 5234 characters of text\n",
      "prudentius had 7 pieces of work with a total of 38688 characters of text\n",
      "pseudocicero had 1 pieces of work with a total of 2195 characters of text\n",
      "pseudoquintilian had 1 pieces of work with a total of 83064 characters of text\n",
      "psplato had 1 pieces of work with a total of 1768 characters of text\n",
      "pulchracomis had 1 pieces of work with a total of 37 characters of text\n",
      "qcicero had 1 pieces of work with a total of 4988 characters of text\n",
      "quintilian had 12 pieces of work with a total of 207913 characters of text\n",
      "quum had 1 pieces of work with a total of 368 characters of text\n",
      "raoul had 1 pieces of work with a total of 45260 characters of text\n",
      "regula had 1 pieces of work with a total of 7139 characters of text\n",
      "reposianus had 1 pieces of work with a total of 1542 characters of text\n",
      "resgestae had 1 pieces of work with a total of 3096 characters of text\n",
      "rhetores had 1 pieces of work with a total of 113 characters of text\n",
      "richerus had 1 pieces of work with a total of 14512 characters of text\n",
      "rimbaud had 1 pieces of work with a total of 502 characters of text\n",
      "ruaeus had 1 pieces of work with a total of 3041 characters of text\n",
      "rumor had 1 pieces of work with a total of 157 characters of text\n",
      "rutilius had 1 pieces of work with a total of 4643 characters of text\n",
      "rutiliuslupus had 1 pieces of work with a total of 5502 characters of text\n",
      "sabinus had 1 pieces of work with a total of 895 characters of text\n",
      "sall had 1 pieces of work with a total of 25681 characters of text\n",
      "sallust had 3 pieces of work with a total of 42735 characters of text\n",
      "sannazaro had 1 pieces of work with a total of 11445 characters of text\n",
      "scaliger had 1 pieces of work with a total of 49 characters of text\n",
      "scbaccanalibus had 1 pieces of work with a total of 1093 characters of text\n",
      "scottus had 1 pieces of work with a total of 289 characters of text\n",
      "scriptoreshistoriaeaugustae had 4 pieces of work with a total of 86648 characters of text\n",
      "sedulius had 1 pieces of work with a total of 2324 characters of text\n",
      "sen had 1 pieces of work with a total of 8864 characters of text\n",
      "seneca had 42 pieces of work with a total of 385030 characters of text\n",
      "senecatheelder had 8 pieces of work with a total of 119085 characters of text\n",
      "septsap had 1 pieces of work with a total of 9427 characters of text\n",
      "serviushonoratus had 4 pieces of work with a total of 65032 characters of text\n",
      "sha had 1 pieces of work with a total of 6856 characters of text\n",
      "sicmeafata had 1 pieces of work with a total of 155 characters of text\n",
      "sidonius had 1 pieces of work with a total of 3027 characters of text\n",
      "sigebert had 1 pieces of work with a total of 1729 characters of text\n",
      "silius had 1 pieces of work with a total of 6763 characters of text\n",
      "siliusitalicus had 17 pieces of work with a total of 88590 characters of text\n",
      "simedignetur had 1 pieces of work with a total of 58 characters of text\n",
      "smarius had 1 pieces of work with a total of 1251 characters of text\n",
      "solet had 1 pieces of work with a total of 1975 characters of text\n",
      "solinus had 1 pieces of work with a total of 10207 characters of text\n",
      "spinoza had 1 pieces of work with a total of 14848 characters of text\n",
      "statius had 18 pieces of work with a total of 110251 characters of text\n",
      "suetonius had 12 pieces of work with a total of 80771 characters of text\n",
      "sulpicia had 1 pieces of work with a total of 326 characters of text\n",
      "sulpiciusseveruschron had 1 pieces of work with a total of 16313 characters of text\n",
      "sulpiciusseverusmartin had 1 pieces of work with a total of 8498 characters of text\n",
      "suscipeflos had 1 pieces of work with a total of 85 characters of text\n",
      "syrus had 1 pieces of work with a total of 10369 characters of text\n",
      "tacitus had 20 pieces of work with a total of 188319 characters of text\n",
      "tempusest had 1 pieces of work with a total of 247 characters of text\n",
      "ter had 1 pieces of work with a total of 10143 characters of text\n",
      "terence had 6 pieces of work with a total of 63599 characters of text\n",
      "terraiam had 1 pieces of work with a total of 198 characters of text\n",
      "tertullian had 2 pieces of work with a total of 31118 characters of text\n",
      "testamentum had 1 pieces of work with a total of 396 characters of text\n",
      "tevigilans had 1 pieces of work with a total of 71 characters of text\n",
      "theganus had 1 pieces of work with a total of 7630 characters of text\n",
      "theodolus had 1 pieces of work with a total of 2754 characters of text\n",
      "theodosius had 1 pieces of work with a total of 12411 characters of text\n",
      "theophanes had 1 pieces of work with a total of 347 characters of text\n",
      "thesauro had 1 pieces of work with a total of 2112 characters of text\n",
      "thomasedessa had 1 pieces of work with a total of 12859 characters of text\n",
      "tibullus had 1 pieces of work with a total of 14712 characters of text\n",
      "tunger had 1 pieces of work with a total of 15315 characters of text\n",
      "valeriusflaccus had 8 pieces of work with a total of 42426 characters of text\n",
      "valeriusmaximus had 9 pieces of work with a total of 90973 characters of text\n",
      "valesianus had 1 pieces of work with a total of 3594 characters of text\n",
      "valmax had 1 pieces of work with a total of 8787 characters of text\n",
      "varro had 1 pieces of work with a total of 7636 characters of text\n",
      "vegetius had 1 pieces of work with a total of 5459 characters of text\n",
      "vegius had 1 pieces of work with a total of 5138 characters of text\n",
      "vell had 1 pieces of work with a total of 27392 characters of text\n",
      "venantius had 1 pieces of work with a total of 875 characters of text\n",
      "vergil had 17 pieces of work with a total of 99997 characters of text\n",
      "vicentius had 1 pieces of work with a total of 20418 characters of text\n",
      "vico had 1 pieces of work with a total of 3727 characters of text\n",
      "victor had 1 pieces of work with a total of 11244 characters of text\n",
      "vida had 1 pieces of work with a total of 5155 characters of text\n",
      "vitacaroli had 1 pieces of work with a total of 17896 characters of text\n",
      "vitruvius had 10 pieces of work with a total of 67461 characters of text\n",
      "volovirum had 1 pieces of work with a total of 197 characters of text\n",
      "voragine had 1 pieces of work with a total of 512 characters of text\n",
      "waardenburg had 1 pieces of work with a total of 668 characters of text\n",
      "waltarius had 1 pieces of work with a total of 2988 characters of text\n",
      "walter had 1 pieces of work with a total of 498 characters of text\n",
      "walton had 1 pieces of work with a total of 4541 characters of text\n",
      "williamapulia had 1 pieces of work with a total of 20635 characters of text\n",
      "williamtyre had 1 pieces of work with a total of 1194 characters of text\n",
      "withof had 1 pieces of work with a total of 274 characters of text\n",
      "wmconchesdogma had 1 pieces of work with a total of 16159 characters of text\n",
      "wmconchesphil had 1 pieces of work with a total of 7051 characters of text\n",
      "xanten had 1 pieces of work with a total of 8094 characters of text\n",
      "xylander had 1 pieces of work with a total of 17623 characters of text\n",
      "zonaras had 1 pieces of work with a total of 12912 characters of text\n"
     ]
    }
   ],
   "source": [
    "# Load in the raw dataset\n",
    "CI = dataExp.CorpusInterface(corpus_name=\"tokenized_corpus.pickle\", shouldTokenize = True)\n",
    "#text = CI.get_total_data().replace(\"\\t\",\"\")\n",
    "text = CI.get_text_for_author(author=\"Caesar\",shouldShuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46706d81-600d-4a4e-909c-2b21cd9d7023",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:53:46.072899Z",
     "iopub.status.busy": "2023-03-26T15:53:46.072627Z",
     "iopub.status.idle": "2023-03-26T15:53:46.076085Z",
     "shell.execute_reply": "2023-03-26T15:53:46.075614Z",
     "shell.execute_reply.started": "2023-03-26T15:53:46.072879Z"
    }
   },
   "outputs": [],
   "source": [
    "def encodeWithBert(text):\n",
    "    vals = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "edeec94c-f7e4-45cc-962c-bd5257e3c5c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:54:12.186579Z",
     "iopub.status.busy": "2023-03-26T15:54:12.185667Z",
     "iopub.status.idle": "2023-03-26T15:54:13.021807Z",
     "shell.execute_reply": "2023-03-26T15:54:13.021133Z",
     "shell.execute_reply.started": "2023-03-26T15:54:12.186562Z"
    }
   },
   "outputs": [],
   "source": [
    "data = torch.tensor(encodeWithBert(text),dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fc732a5-1dee-4368-ae03-e4c44c60a33e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:54:48.176473Z",
     "iopub.status.busy": "2023-03-26T15:54:48.175775Z",
     "iopub.status.idle": "2023-03-26T15:54:48.180107Z",
     "shell.execute_reply": "2023-03-26T15:54:48.179390Z",
     "shell.execute_reply.started": "2023-03-26T15:54:48.176442Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a train/val split\n",
    "n = int(.8*len(data)) \n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b2b338-141a-4fff-9418-1453ccc43ea3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:15:33.546261Z",
     "iopub.status.busy": "2023-03-26T15:15:33.545557Z",
     "iopub.status.idle": "2023-03-26T15:15:33.551419Z",
     "shell.execute_reply": "2023-03-26T15:15:33.550821Z",
     "shell.execute_reply.started": "2023-03-26T15:15:33.546235Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a Dataset that masks each following word in input sequence\n",
    "class NextWordDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "        sents = re.split('(?<=[\\.\\?\\!])\\s*', text)\n",
    "        for line in sents:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            for i in range(1, len(tokens)):\n",
    "                prev_tokens = tokens[:i]\n",
    "                next_token = tokens[i]\n",
    "                self.examples.append((prev_tokens, next_token))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        prev_tokens, next_token = self.examples[idx]\n",
    "        prev_ids = self.tokenizer.convert_tokens_to_ids(prev_tokens)\n",
    "        next_id = self.tokenizer.convert_tokens_to_ids([next_token])[0]\n",
    "        return torch.tensor(prev_ids), torch.tensor(next_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6e2dc84-bcf5-4d63-baaf-391de05206c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:40:14.157035Z",
     "iopub.status.busy": "2023-03-26T15:40:14.156761Z",
     "iopub.status.idle": "2023-03-26T15:40:16.201057Z",
     "shell.execute_reply": "2023-03-26T15:40:16.200571Z",
     "shell.execute_reply.started": "2023-03-26T15:40:14.157016Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"arma virumque cano\"))\n",
    "\n",
    "torch_tokenids=torch.LongTensor(tokenids).unsqueeze(0)\n",
    "torch_tokenids=torch_tokenids.to(\"cuda\")\n",
    "total_text = \"\"\n",
    "with torch.no_grad():\n",
    "    preds = model(torch_tokenids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b10589b4-d405-43b0-ba27-4d6e7f1f5846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:40:26.678646Z",
     "iopub.status.busy": "2023-03-26T15:40:26.678018Z",
     "iopub.status.idle": "2023-03-26T15:40:26.701608Z",
     "shell.execute_reply": "2023-03-26T15:40:26.701152Z",
     "shell.execute_reply.started": "2023-03-26T15:40:26.678623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedLMOutput(loss=None, logits=tensor([[[-3.0104, -5.5644, -5.4879,  ..., -4.9033, -5.5426, -5.5480],\n",
      "         [-3.7047, -4.6934, -4.6515,  ..., -6.7923, -5.2240, -3.8521],\n",
      "         [-7.3019, -5.7173, -4.2949,  ..., -8.8850, -7.4146, -5.5154],\n",
      "         [-5.8219, -5.9337, -3.8385,  ..., -8.9431, -7.5980, -9.6204]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(preds)\n",
    "#dataset = NextWordDataset(text, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d90a1280-6031-415b-a721-67c56583ffef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:54:55.227366Z",
     "iopub.status.busy": "2023-03-26T15:54:55.227102Z",
     "iopub.status.idle": "2023-03-26T15:54:55.231463Z",
     "shell.execute_reply": "2023-03-26T15:54:55.231000Z",
     "shell.execute_reply.started": "2023-03-26T15:54:55.227347Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - context_size, (batch_size,))\n",
    "    # create a batch by context size tensor of the data\n",
    "    x = torch.stack([data[i:i+context_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+context_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "46154d96-36c1-46d1-b36b-724a3fafee14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T16:03:57.667964Z",
     "iopub.status.busy": "2023-03-26T16:03:57.667416Z",
     "iopub.status.idle": "2023-03-26T16:03:57.678210Z",
     "shell.execute_reply": "2023-03-26T16:03:57.677605Z",
     "shell.execute_reply.started": "2023-03-26T16:03:57.667944Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_x, tmp_y = get_batch(\"val\")\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9a71f504-9a48-4342-8ff1-d6e4b6ee366f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T16:09:39.462436Z",
     "iopub.status.busy": "2023-03-26T16:09:39.461828Z",
     "iopub.status.idle": "2023-03-26T16:09:39.699219Z",
     "shell.execute_reply": "2023-03-26T16:09:39.698246Z",
     "shell.execute_reply.started": "2023-03-26T16:09:39.462411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  313,  3947,    12,   939,  5516,    24,  7710,    51,    65,  2399,\n",
      "         7710,    51,   108,   887,   278,   168,  1550,  2804,    24,    10,\n",
      "         2009,   390,  7710,    51,    38,    63,    37, 14531,    24,   313,\n",
      "        20722,    24,  7710,    51, 13745, 25155, 10674,    24,   488,  8691,\n",
      "           24,  7710,    51,    38,  1390,  7704,   960,  8137,   198,  4118,\n",
      "         6793,   359, 13748,  7710,    51,  2474,   772,  6328,    24,  7710,\n",
      "           51,    40,   215,   215,    30,   198,  4118,    63, 12020,    34,\n",
      "        20317,     9,    82,  1673,  6484,    24,  7710,    51,   583,    22,\n",
      "         4277,  7127,    24, 11919,    36,    90,   795,  5137,   319,  7710,\n",
      "           51,    38,   544,   255,  9575,    34,   108, 11875,    34,  1817,\n",
      "        20317,     9,  5741,    24,    10,    22,  6210,  9169,    24,    10,\n",
      "           30,  9560,   198,  1009,  7330,    24, 13122,    24,   313,  7710,\n",
      "           51,  4277,    38,  1829,   313, 18098,  1267,   313,    41,   743,\n",
      "        10419,    24,  2877, 20793,    24,  7710,    51,    30,   198,  4118,\n",
      "          143,    42,  3499,  8366,    20, 13426,    24,  7710,    51, 25068,\n",
      "           14, 23951,    14,    63,   168,  5096,  7909,    24,  7710,    51,\n",
      "          112,   166,  8753, 12253,  2474,  7710,    51,    38,   198, 15714,\n",
      "         9578,     9,  7710,    51,   166,  3045, 14052,    24,  7710,    51,\n",
      "          146,  1360,    37,   903, 25838,  7710,    51,    38, 10095,    24,\n",
      "           10,    38, 24936,    34, 23910,     9, 22215,    24,  7710,    51,\n",
      "         1480,    10, 21493,    27,    10,  3105,  7710,    51, 15388,  2513,\n",
      "           18,  1673,   147,  6484,    24,  7710,    51,  5190,    24,   343,\n",
      "           12, 27262,    24,   108,    12,  2205,   198, 12618,  8359,    24,\n",
      "         7710,    51,    63, 19491,  4367,   108, 18175,     9, 14052,    24,\n",
      "         7710,    51,    38, 23963,  1192, 30509,   198,  9852,    24,  7710,\n",
      "           51,  3281,   137,  8137, 16004,  3081], device='cuda:0')\n",
      "tensor([ 3947,    12,   939,  5516,    24,  7710,    51,    65,  2399,  7710,\n",
      "           51,   108,   887,   278,   168,  1550,  2804,    24,    10,  2009,\n",
      "          390,  7710,    51,    38,    63,    37, 14531,    24,   313, 20722,\n",
      "           24,  7710,    51, 13745, 25155, 10674,    24,   488,  8691,    24,\n",
      "         7710,    51,    38,  1390,  7704,   960,  8137,   198,  4118,  6793,\n",
      "          359, 13748,  7710,    51,  2474,   772,  6328,    24,  7710,    51,\n",
      "           40,   215,   215,    30,   198,  4118,    63, 12020,    34, 20317,\n",
      "            9,    82,  1673,  6484,    24,  7710,    51,   583,    22,  4277,\n",
      "         7127,    24, 11919,    36,    90,   795,  5137,   319,  7710,    51,\n",
      "           38,   544,   255,  9575,    34,   108, 11875,    34,  1817, 20317,\n",
      "            9,  5741,    24,    10,    22,  6210,  9169,    24,    10,    30,\n",
      "         9560,   198,  1009,  7330,    24, 13122,    24,   313,  7710,    51,\n",
      "         4277,    38,  1829,   313, 18098,  1267,   313,    41,   743, 10419,\n",
      "           24,  2877, 20793,    24,  7710,    51,    30,   198,  4118,   143,\n",
      "           42,  3499,  8366,    20, 13426,    24,  7710,    51, 25068,    14,\n",
      "        23951,    14,    63,   168,  5096,  7909,    24,  7710,    51,   112,\n",
      "          166,  8753, 12253,  2474,  7710,    51,    38,   198, 15714,  9578,\n",
      "            9,  7710,    51,   166,  3045, 14052,    24,  7710,    51,   146,\n",
      "         1360,    37,   903, 25838,  7710,    51,    38, 10095,    24,    10,\n",
      "           38, 24936,    34, 23910,     9, 22215,    24,  7710,    51,  1480,\n",
      "           10, 21493,    27,    10,  3105,  7710,    51, 15388,  2513,    18,\n",
      "         1673,   147,  6484,    24,  7710,    51,  5190,    24,   343,    12,\n",
      "        27262,    24,   108,    12,  2205,   198, 12618,  8359,    24,  7710,\n",
      "           51,    63, 19491,  4367,   108, 18175,     9, 14052,    24,  7710,\n",
      "           51,    38, 23963,  1192, 30509,   198,  9852,    24,  7710,    51,\n",
      "         3281,   137,  8137, 16004,  3081,  7710], device='cuda:0')\n",
      "64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.74 GiB total capacity; 14.13 GiB already allocated; 3.56 MiB free; 14.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tmp_x))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tmp_x)):\n\u001b[0;32m----> 6\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m     sortedVals\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39margsort(preds[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m256\u001b[39m], descending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     p \u001b[38;5;241m=\u001b[39m sortedVals[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py:1351\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1351\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1366\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py:1018\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1009\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1011\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1012\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1013\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1017\u001b[0m )\n\u001b[0;32m-> 1018\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1031\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    598\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    599\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    600\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    483\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    492\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py:432\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    415\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    422\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    423\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    424\u001b[0m         hidden_states,\n\u001b[1;32m    425\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m         output_attentions,\n\u001b[1;32m    431\u001b[0m     )\n\u001b[0;32m--> 432\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py:384\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    382\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    383\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 384\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:2503\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2501\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2502\u001b[0m     )\n\u001b[0;32m-> 2503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 15.74 GiB total capacity; 14.13 GiB already allocated; 3.56 MiB free; 14.55 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "print(tmp_x[0])\n",
    "print(tmp_y[0])\n",
    "assert(len(tmp_x)==len(tmp_y))\n",
    "print(len(tmp_x))\n",
    "for b in range(len(tmp_x)):\n",
    "    preds = model(tmp_x[b].unsqueeze(0))[0]\n",
    "    sortedVals=torch.argsort(preds[0][256], descending=True)\n",
    "    p = sortedVals[0]\n",
    "    print(p)\n",
    "    loss = loss_fn(preds, tmp_y[b])\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d2596e-35ff-4d9a-8e5a-e50b7540e09a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T15:15:44.945698Z",
     "iopub.status.busy": "2023-03-26T15:15:44.944878Z",
     "iopub.status.idle": "2023-03-26T15:15:45.126675Z",
     "shell.execute_reply": "2023-03-26T15:15:45.125871Z",
     "shell.execute_reply.started": "2023-03-26T15:15:44.945674Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [729] at entry 0 and [3638] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     15\u001b[0m     prev_ids, next_id \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [729] at entry 0 and [3638] at entry 1"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer.zero_grad()\n",
    "# Train the model\n",
    "for iter_val in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter_val % eval_interval == 0 or iter_val == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    logits = model(input_ids=prev_ids.unsqueeze(0))[0][:, -1, :]  # predict next word based on the previous tokens\n",
    "    loss = loss_fn(logits, yb)\n",
    "    for b in range(len(tmp_x)):\n",
    "        logits = model(tmp_x[b].unsqueeze(0))[0][0]\n",
    "        loss = loss_fn(res, tmp_y[b])\n",
    "        \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9e639-9829-4529-903e-3fd938b57cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
