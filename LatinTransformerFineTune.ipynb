{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "697388e1",
   "metadata": {},
   "source": [
    "# Generating New Latin Text with the Transformer\n",
    "\n",
    "After training the desired author weights using the transformer.py file,, we can load in those author weights to perform new generation.\n",
    "\n",
    "First, let us important the necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bdd74ad-76f2-4c1d-b3de-0497fe73c5bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:04:30.269497Z",
     "iopub.status.busy": "2023-05-12T13:04:30.269195Z",
     "iopub.status.idle": "2023-05-12T13:04:35.497244Z",
     "shell.execute_reply": "2023-05-12T13:04:35.496211Z",
     "shell.execute_reply.started": "2023-05-12T13:04:30.269477Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 13:04:32.175261: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 13:04:33.044554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-05-12 13:04:34.360282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-12 13:04:34.446666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-12 13:04:34.446896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, re\n",
    "from Data import dataExp\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8977790",
   "metadata": {},
   "source": [
    "Then, we select the author we wish to generate in the style of and load in the Corpus Interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e284a-b5ad-4c84-baba-0ffef5ab8d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:57:08.634721Z",
     "iopub.status.busy": "2023-05-12T13:57:08.634053Z",
     "iopub.status.idle": "2023-05-12T13:57:10.383201Z",
     "shell.execute_reply": "2023-05-12T13:57:10.382531Z",
     "shell.execute_reply.started": "2023-05-12T13:57:08.634699Z"
    }
   },
   "outputs": [],
   "source": [
    "author = \"Caesar\"\n",
    "CI = dataExp.CorpusInterface(corpus_name=\"text_corpus.pickle\", shouldTokenize = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d18596dd",
   "metadata": {},
   "source": [
    "The Corpus Interface is needed so that the unique character mapping for the particular author can be created again. Once it is created, we redefine the Transformer architecture with the embedding look up table being of the size of the vocabulary. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df066576-fb78-4127-a249-62e75fc763e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:57:15.498416Z",
     "iopub.status.busy": "2023-05-12T13:57:15.497646Z",
     "iopub.status.idle": "2023-05-12T13:57:15.517727Z",
     "shell.execute_reply": "2023-05-12T13:57:15.517100Z",
     "shell.execute_reply.started": "2023-05-12T13:57:15.498395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The character vocabulary is : [' ', '!', '\"', \"'\", '(', ')', '+', ',', '-', '.', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "text = CI.get_text_for_author(author=author,shouldShuffle=True).replace(\"\\t\",\"\")\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"The character vocabulary is : {chars}\")\n",
    "\n",
    "# We just create a mapping between our character vocabulary\n",
    "# and their corresponding integer value, and define lambda funcs to do this mapping for us\n",
    "\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "context_size = 256\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = .2\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            This performs the self-attention that we worked through did in the notebook\n",
    "        \"\"\"\n",
    "        batch, time, channel = x.shape\n",
    "\n",
    "        k = self.key(x) # (batch, time, head_size)\n",
    "        q = self.query(x) # (batch, time, head_size)\n",
    "\n",
    "        # compute the affinities/scaled attention scores\n",
    "        weights = q @ k.transpose(-2, -1) * channel **-.5\n",
    "        # don't want to interact with subsequent time step tokens\n",
    "        # i.e. makes it a decoder block\n",
    "        weights = weights.masked_fill(self.tril[:time, :time] == 0, float('-inf'))\n",
    "        # make the probability nicely distributed\n",
    "        weights = F.softmax(weights, dim=-1) # (batch, time, time)\n",
    "        weights = self.dropout(weights)\n",
    "        # weighted aggreagation of values\n",
    "        v = self.value(x)\n",
    "        out = weights @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" just a simple linear layer followed by non-linearity, as there \n",
    "        was a FF part in the paper too\n",
    "\n",
    "        on a per-token layer\n",
    "    \"\"\" \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        # last nn.Linear is the projectin layer back into residual pathway\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication intersperesed with calculation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        \"\"\" n_embd: embedding dimension, n_head: the number of heads we want \"\"\"\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd= FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # make them residual connnections by doing x + \n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_final = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        batch, time = idx.shape\n",
    "\n",
    "        # idx and targets are both (batch, time) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (batch, time, channel)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(time, device=device)) # (time, channel)\n",
    "        x = tok_emb + pos_emb # (batch,time, channel)\n",
    "        x = self.blocks(x) # (batch,time, channel)\n",
    "        x = self.ln_final(x) # (batch,time, channel)\n",
    "        logits = self.lm_head(x) # (batch,time,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch, time, channel = logits.shape\n",
    "            logits = logits.view(batch*time, channel)\n",
    "            targets = targets.view(batch*time)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (batch,time) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last context_size tokens\n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (batch, channel)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (batch, channel)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (batch, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (batch, time step+1)\n",
    "        return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24ad3b88",
   "metadata": {},
   "source": [
    "Once the architecture and the encoder/decoder are defined correctly, we can load in the model's weights, as the keys will now match correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43276384-e1a2-448a-8f63-595ff2696ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:05:25.363572Z",
     "iopub.status.busy": "2023-05-12T13:05:25.362738Z",
     "iopub.status.idle": "2023-05-12T13:05:27.749024Z",
     "shell.execute_reply": "2023-05-12T13:05:27.748189Z",
     "shell.execute_reply.started": "2023-05-12T13:05:25.363549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caesar 616449\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(author, len(text))\n",
    "model = LanguageModel()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(os.getcwd()+f\"/LatinTransformer/{author}_model.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21aa016c",
   "metadata": {},
   "source": [
    "Once the model has been loaded in, new text can be generated in imitation of the particular author selected. We encode the prompt and reshape it for the model, it then generates the continuation with the maximum token size, and then we print the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae33313-d69e-42f5-bdbb-d0ca7a3adf66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T13:57:20.180669Z",
     "iopub.status.busy": "2023-05-12T13:57:20.180022Z",
     "iopub.status.idle": "2023-05-12T13:57:25.179924Z",
     "shell.execute_reply": "2023-05-12T13:57:25.179419Z",
     "shell.execute_reply.started": "2023-05-12T13:57:20.180647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olim rufo nomine iuvenis ingeniosus erat,od minores arbitrium esse ipsi; quibus in contuberioribus rei erant, ut eam civitas quae ad sibi bellum agebant, quod pro cum flumine hostibus aliter reverterant, ne non solum consulebant atque ibi praesidia. cum venisset\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Olim Rufo nomine iuvenis ingeniosus erat,\".lower()\n",
    "context_values = encode(prompt)\n",
    "context = torch.tensor(context_values, dtype=torch.long, device=device).reshape((len(context_values),1))\n",
    "\n",
    "gen =model.generate(context, max_new_tokens=220)[0].tolist()\n",
    "print(prompt+ decode(gen))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
