{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271073d4",
   "metadata": {},
   "source": [
    "# LatinBERT Gen\n",
    "This approach will follow similarly to bert-babble as generated in the paper \"Bert has a voice let it speak, BERT as an RMF\" by X and Y at Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9984f67b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:39:03.532409Z",
     "iopub.status.busy": "2023-03-01T21:39:03.532016Z",
     "iopub.status.idle": "2023-03-01T21:39:11.108886Z",
     "shell.execute_reply": "2023-03-01T21:39:11.107178Z",
     "shell.execute_reply.started": "2023-03-01T21:39:03.532384Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, re\n",
    "from Data import dataExp\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import LatinBERT\n",
    "from LatinBERT.gen_berts import LatinBERT\n",
    "from LatinBERT.LatinTok import LatinTokenizer\n",
    "from LatinBERT.predict_words import predict\n",
    "from transformers import BertModel, BertForMaskedLM, BertPreTrainedModel\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "    from cltk.tokenizers.lat.lat import LatinWordTokenizer as WordTokenizer\n",
    "    from cltk.tokenizers.lat.lat import LatinPunktSentenceTokenizer as SentenceTokenizer\n",
    "from cltk.embeddings.embeddings import Word2VecEmbeddings as W2VE\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff58d3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T19:02:53.329856Z",
     "iopub.status.busy": "2023-03-01T19:02:53.328735Z",
     "iopub.status.idle": "2023-03-01T19:02:55.924278Z",
     "shell.execute_reply": "2023-03-01T19:02:55.922957Z",
     "shell.execute_reply.started": "2023-03-01T19:02:53.329816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the existing corpus\n",
      "abbofloracensis had 1 pieces of work with a total of 34398 characters of text\n",
      "abelard had 1 pieces of work with a total of 15483 characters of text\n",
      "acticussincerius had 1 pieces of work with a total of 5947 characters of text\n",
      "addison had 1 pieces of work with a total of 3074 characters of text\n",
      "adso had 1 pieces of work with a total of 13551 characters of text\n",
      "aelredus had 1 pieces of work with a total of 118173 characters of text\n",
      "agnes had 1 pieces of work with a total of 74784 characters of text\n",
      "alanus had 1 pieces of work with a total of 136527 characters of text\n",
      "albericodamarcellise had 1 pieces of work with a total of 172 characters of text\n",
      "albertanus had 1 pieces of work with a total of 108213 characters of text\n",
      "albertofaix had 1 pieces of work with a total of 51703 characters of text\n",
      "alcuin had 1 pieces of work with a total of 1641 characters of text\n",
      "aleandrogerolamo had 1 pieces of work with a total of 10197 characters of text\n",
      "alfonsi had 1 pieces of work with a total of 105630 characters of text\n",
      "ambrose had 1 pieces of work with a total of 15147 characters of text\n",
      "ammianus had 18 pieces of work with a total of 975460 characters of text\n",
      "ampelius had 1 pieces of work with a total of 52872 characters of text\n",
      "andecavis had 1 pieces of work with a total of 920 characters of text\n",
      "andreasbergoma had 1 pieces of work with a total of 28522 characters of text\n",
      "andronicus had 1 pieces of work with a total of 2238 characters of text\n",
      "angeloambrogini had 1 pieces of work with a total of 6096 characters of text\n",
      "angelopoliziano had 1 pieces of work with a total of 2637 characters of text\n",
      "angilbert had 1 pieces of work with a total of 2094 characters of text\n",
      "annalesregnifrancorum had 1 pieces of work with a total of 155834 characters of text\n",
      "annalesvedastini had 1 pieces of work with a total of 54567 characters of text\n",
      "anon had 1 pieces of work with a total of 793 characters of text\n",
      "anonymous had 1 pieces of work with a total of 6463 characters of text\n",
      "anselmepistula had 1 pieces of work with a total of 5009 characters of text\n",
      "anselmproslogion had 1 pieces of work with a total of 39144 characters of text\n",
      "apicius had 1 pieces of work with a total of 8603 characters of text\n",
      "appverg had 1 pieces of work with a total of 8895 characters of text\n",
      "appvergcomp had 1 pieces of work with a total of 110025 characters of text\n",
      "appvergculex had 1 pieces of work with a total of 18887 characters of text\n",
      "apuleius had 13 pieces of work with a total of 604335 characters of text\n",
      "aquinas had 1 pieces of work with a total of 33728 characters of text\n",
      "arbroath had 1 pieces of work with a total of 7497 characters of text\n",
      "archpoet had 1 pieces of work with a total of 29641 characters of text\n",
      "aristotle had 1 pieces of work with a total of 11537 characters of text\n",
      "arnobius had 1 pieces of work with a total of 66612 characters of text\n",
      "arnulf had 1 pieces of work with a total of 1387 characters of text\n",
      "asconius had 1 pieces of work with a total of 112203 characters of text\n",
      "asserius had 1 pieces of work with a total of 89247 characters of text\n",
      "augustine had 7 pieces of work with a total of 402151 characters of text\n",
      "aureliusvictor had 2 pieces of work with a total of 149735 characters of text\n",
      "aus had 1 pieces of work with a total of 2241 characters of text\n",
      "ausonius had 24 pieces of work with a total of 292442 characters of text\n",
      "ave had 1 pieces of work with a total of 6949 characters of text\n",
      "avianus had 1 pieces of work with a total of 30263 characters of text\n",
      "avienus had 1 pieces of work with a total of 27798 characters of text\n",
      "axio had 1 pieces of work with a total of 16543 characters of text\n",
      "bacon had 1 pieces of work with a total of 45816 characters of text\n",
      "balbus had 1 pieces of work with a total of 16073 characters of text\n",
      "balde had 1 pieces of work with a total of 765 characters of text\n",
      "baldo had 1 pieces of work with a total of 59162 characters of text\n",
      "bebel had 1 pieces of work with a total of 131504 characters of text\n",
      "bede had 5 pieces of work with a total of 575775 characters of text\n",
      "benedict had 1 pieces of work with a total of 92323 characters of text\n",
      "berengar had 1 pieces of work with a total of 37827 characters of text\n",
      "bernardcluny had 1 pieces of work with a total of 62164 characters of text\n",
      "bible had 1 pieces of work with a total of 91761 characters of text\n",
      "biggs had 1 pieces of work with a total of 44386 characters of text\n",
      "bill had 1 pieces of work with a total of 3236 characters of text\n",
      "blesensis had 1 pieces of work with a total of 51232 characters of text\n",
      "boethius had 10 pieces of work with a total of 263001 characters of text\n",
      "boethiusdacia had 1 pieces of work with a total of 50991 characters of text\n",
      "bonaventura had 1 pieces of work with a total of 72084 characters of text\n",
      "boskovic had 1 pieces of work with a total of 22752 characters of text\n",
      "brevechronicon had 1 pieces of work with a total of 8191 characters of text\n",
      "buchanan had 1 pieces of work with a total of 36240 characters of text\n",
      "bultelius had 1 pieces of work with a total of 2472 characters of text\n",
      "caeciliusbalbus had 1 pieces of work with a total of 45340 characters of text\n",
      "caesar had 11 pieces of work with a total of 616439 characters of text\n",
      "caesaraugustus had 1 pieces of work with a total of 18129 characters of text\n",
      "calpurniusflaccus had 1 pieces of work with a total of 48342 characters of text\n",
      "calpurniussiculus had 1 pieces of work with a total of 34152 characters of text\n",
      "campion had 1 pieces of work with a total of 13502 characters of text\n",
      "capellanus had 1 pieces of work with a total of 233924 characters of text\n",
      "carm had 1 pieces of work with a total of 1530 characters of text\n",
      "carmenarvale had 1 pieces of work with a total of 560 characters of text\n",
      "carmeninvictoriam had 1 pieces of work with a total of 13588 characters of text\n",
      "carmensaliare had 1 pieces of work with a total of 306 characters of text\n",
      "cassiodorus had 1 pieces of work with a total of 80306 characters of text\n",
      "catalogueliberien had 1 pieces of work with a total of 7213 characters of text\n",
      "cato had 1 pieces of work with a total of 14361 characters of text\n",
      "catullus had 1 pieces of work with a total of 85900 characters of text\n",
      "celsus had 8 pieces of work with a total of 696619 characters of text\n",
      "celtis had 1 pieces of work with a total of 1855 characters of text\n",
      "censorinus had 1 pieces of work with a total of 71644 characters of text\n",
      "cicero had 122 pieces of work with a total of 7438016 characters of text\n",
      "cinna had 1 pieces of work with a total of 861 characters of text\n",
      "claud had 1 pieces of work with a total of 4961 characters of text\n",
      "claudian had 19 pieces of work with a total of 441142 characters of text\n",
      "clitophon had 1 pieces of work with a total of 9925 characters of text\n",
      "colman had 1 pieces of work with a total of 1737 characters of text\n",
      "columba had 1 pieces of work with a total of 69044 characters of text\n",
      "columbus had 1 pieces of work with a total of 15591 characters of text\n",
      "columella had 9 pieces of work with a total of 562550 characters of text\n",
      "comes had 1 pieces of work with a total of 441 characters of text\n",
      "commodianus had 1 pieces of work with a total of 939 characters of text\n",
      "corippus had 8 pieces of work with a total of 217825 characters of text\n",
      "corneliopaoloamalteo had 1 pieces of work with a total of 503 characters of text\n",
      "corvinus had 1 pieces of work with a total of 4815 characters of text\n",
      "cotta had 1 pieces of work with a total of 12574 characters of text\n",
      "creeds had 1 pieces of work with a total of 6010 characters of text\n",
      "curtius had 1 pieces of work with a total of 51880 characters of text\n",
      "curtiusrufus had 8 pieces of work with a total of 536805 characters of text\n",
      "dante had 1 pieces of work with a total of 3188 characters of text\n",
      "dantealighieri had 1 pieces of work with a total of 120 characters of text\n",
      "dares had 1 pieces of work with a total of 56639 characters of text\n",
      "debury had 1 pieces of work with a total of 115993 characters of text\n",
      "declaratio had 1 pieces of work with a total of 10290 characters of text\n",
      "decretum had 1 pieces of work with a total of 15220 characters of text\n",
      "descartes had 1 pieces of work with a total of 5221 characters of text\n",
      "dicquid had 1 pieces of work with a total of 328 characters of text\n",
      "diesirae had 1 pieces of work with a total of 1430 characters of text\n",
      "diravi had 1 pieces of work with a total of 795 characters of text\n",
      "don had 1 pieces of work with a total of 29181 characters of text\n",
      "donation had 1 pieces of work with a total of 20027 characters of text\n",
      "dracontius had 16 pieces of work with a total of 266831 characters of text\n",
      "dumdiane had 1 pieces of work with a total of 1532 characters of text\n",
      "dumdomus had 1 pieces of work with a total of 349 characters of text\n",
      "ebulo had 1 pieces of work with a total of 73683 characters of text\n",
      "egeria had 1 pieces of work with a total of 53062 characters of text\n",
      "ein had 1 pieces of work with a total of 54962 characters of text\n",
      "enn had 1 pieces of work with a total of 8355 characters of text\n",
      "ennius had 1 pieces of work with a total of 25266 characters of text\n",
      "ennodius had 1 pieces of work with a total of 75591 characters of text\n",
      "ep had 1 pieces of work with a total of 18656 characters of text\n",
      "epistaustras had 1 pieces of work with a total of 34585 characters of text\n",
      "epitaphs had 1 pieces of work with a total of 10341 characters of text\n",
      "epitomecononiana had 1 pieces of work with a total of 39045 characters of text\n",
      "epitomefeliciana had 1 pieces of work with a total of 44673 characters of text\n",
      "erasmus had 1 pieces of work with a total of 177122 characters of text\n",
      "erchempert had 1 pieces of work with a total of 89824 characters of text\n",
      "estas had 1 pieces of work with a total of 597 characters of text\n",
      "eucherius had 1 pieces of work with a total of 28159 characters of text\n",
      "eugenius had 1 pieces of work with a total of 584 characters of text\n",
      "eugippius had 1 pieces of work with a total of 87808 characters of text\n",
      "eutropius had 1 pieces of work with a total of 133407 characters of text\n",
      "exivi had 1 pieces of work with a total of 30157 characters of text\n",
      "fabe had 1 pieces of work with a total of 31196 characters of text\n",
      "falcandus had 1 pieces of work with a total of 262882 characters of text\n",
      "falcone had 1 pieces of work with a total of 273400 characters of text\n",
      "faustoandrelino had 1 pieces of work with a total of 4769 characters of text\n",
      "ferraria had 1 pieces of work with a total of 18083 characters of text\n",
      "ficino had 1 pieces of work with a total of 20924 characters of text\n",
      "fletcher had 1 pieces of work with a total of 41616 characters of text\n",
      "florus had 1 pieces of work with a total of 188724 characters of text\n",
      "foedusaeternum had 1 pieces of work with a total of 3611 characters of text\n",
      "forsett had 1 pieces of work with a total of 33474 characters of text\n",
      "fortunat had 1 pieces of work with a total of 703 characters of text\n",
      "fragmentumlaurentianum had 1 pieces of work with a total of 6067 characters of text\n",
      "fredegarius had 1 pieces of work with a total of 168136 characters of text\n",
      "frodebertus had 1 pieces of work with a total of 6475 characters of text\n",
      "frontinus had 1 pieces of work with a total of 6813 characters of text\n",
      "fronto had 1 pieces of work with a total of 339779 characters of text\n",
      "fulbert had 1 pieces of work with a total of 760 characters of text\n",
      "fulgentius had 1 pieces of work with a total of 28596 characters of text\n",
      "gaius had 1 pieces of work with a total of 78184 characters of text\n",
      "galileo had 1 pieces of work with a total of 71985 characters of text\n",
      "garcilaso had 1 pieces of work with a total of 6652 characters of text\n",
      "garland had 1 pieces of work with a total of 21499 characters of text\n",
      "gaud had 1 pieces of work with a total of 811 characters of text\n",
      "gauss had 1 pieces of work with a total of 66500 characters of text\n",
      "gellius had 21 pieces of work with a total of 812717 characters of text\n",
      "germanicus had 1 pieces of work with a total of 33114 characters of text\n",
      "gestafrancorum had 1 pieces of work with a total of 8247 characters of text\n",
      "gestarom had 1 pieces of work with a total of 103434 characters of text\n",
      "gioacchino had 1 pieces of work with a total of 136901 characters of text\n",
      "girolamoaccelini had 1 pieces of work with a total of 3935 characters of text\n",
      "girolamoamaseo had 1 pieces of work with a total of 16479 characters of text\n",
      "glass had 1 pieces of work with a total of 190152 characters of text\n",
      "godfrey had 1 pieces of work with a total of 48392 characters of text\n",
      "grattius had 1 pieces of work with a total of 24829 characters of text\n",
      "gravissimas had 1 pieces of work with a total of 13221 characters of text\n",
      "greg had 1 pieces of work with a total of 5508 characters of text\n",
      "gregdecretals had 1 pieces of work with a total of 663928 characters of text\n",
      "gregory had 1 pieces of work with a total of 119429 characters of text\n",
      "gregorytours had 1 pieces of work with a total of 93341 characters of text\n",
      "gwinne had 1 pieces of work with a total of 23392 characters of text\n",
      "halley had 1 pieces of work with a total of 2243 characters of text\n",
      "hebet had 1 pieces of work with a total of 660 characters of text\n",
      "henry had 1 pieces of work with a total of 6543 characters of text\n",
      "henrysettimello had 1 pieces of work with a total of 42703 characters of text\n",
      "hipp had 1 pieces of work with a total of 13938 characters of text\n",
      "histapoll had 1 pieces of work with a total of 79949 characters of text\n",
      "histbrit had 1 pieces of work with a total of 64287 characters of text\n",
      "holberg had 1 pieces of work with a total of 208989 characters of text\n",
      "horace had 10 pieces of work with a total of 297826 characters of text\n",
      "hrabanus had 1 pieces of work with a total of 1155 characters of text\n",
      "hugo had 1 pieces of work with a total of 53280 characters of text\n",
      "hydatiuschronicon had 1 pieces of work with a total of 40888 characters of text\n",
      "hydatiusfasti had 1 pieces of work with a total of 45910 characters of text\n",
      "hyginus had 1 pieces of work with a total of 75886 characters of text\n",
      "hymni had 1 pieces of work with a total of 10922 characters of text\n",
      "iabervocius had 1 pieces of work with a total of 4204 characters of text\n",
      "iamdulcis had 1 pieces of work with a total of 1239 characters of text\n",
      "ilias had 1 pieces of work with a total of 47469 characters of text\n",
      "index had 1 pieces of work with a total of 1604 characters of text\n",
      "indices had 1 pieces of work with a total of 1860 characters of text\n",
      "innocent had 1 pieces of work with a total of 101995 characters of text\n",
      "inquisitio had 1 pieces of work with a total of 28586 characters of text\n",
      "inscriptions had 1 pieces of work with a total of 5249 characters of text\n",
      "iordanes had 1 pieces of work with a total of 131914 characters of text\n",
      "ipsavivere had 1 pieces of work with a total of 532 characters of text\n",
      "isidore had 1 pieces of work with a total of 43473 characters of text\n",
      "italicus had 1 pieces of work with a total of 51727 characters of text\n",
      "jacopoallegretti had 1 pieces of work with a total of 4595 characters of text\n",
      "janus had 1 pieces of work with a total of 9758 characters of text\n",
      "jerome had 80 pieces of work with a total of 6201228 characters of text\n",
      "jfkhonor had 1 pieces of work with a total of 1925 characters of text\n",
      "johannes had 1 pieces of work with a total of 81855 characters of text\n",
      "junillus had 1 pieces of work with a total of 71795 characters of text\n",
      "justin had 1 pieces of work with a total of 9316 characters of text\n",
      "justinian had 1 pieces of work with a total of 134315 characters of text\n",
      "juvenal had 1 pieces of work with a total of 170859 characters of text\n",
      "juvencus had 4 pieces of work with a total of 144617 characters of text\n",
      "kalila had 1 pieces of work with a total of 197267 characters of text\n",
      "kempis had 1 pieces of work with a total of 57776 characters of text\n",
      "kepler had 1 pieces of work with a total of 44519 characters of text\n",
      "lactantius had 1 pieces of work with a total of 122596 characters of text\n",
      "landor had 1 pieces of work with a total of 21815 characters of text\n",
      "legenda had 1 pieces of work with a total of 25365 characters of text\n",
      "leo had 1 pieces of work with a total of 41074 characters of text\n",
      "leothegreat had 1 pieces of work with a total of 8765 characters of text\n",
      "letabundus had 1 pieces of work with a total of 969 characters of text\n",
      "levis had 1 pieces of work with a total of 614 characters of text\n",
      "lhomond had 1 pieces of work with a total of 35760 characters of text\n",
      "liberpontificalis had 1 pieces of work with a total of 107856 characters of text\n",
      "livy had 4 pieces of work with a total of 3582763 characters of text\n",
      "lotichius had 1 pieces of work with a total of 921 characters of text\n",
      "lucan had 10 pieces of work with a total of 358610 characters of text\n",
      "lucernarium had 1 pieces of work with a total of 533 characters of text\n",
      "lucretius had 6 pieces of work with a total of 329870 characters of text\n",
      "luther had 1 pieces of work with a total of 5046 characters of text\n",
      "macarius had 1 pieces of work with a total of 32831 characters of text\n",
      "macrobius had 8 pieces of work with a total of 602168 characters of text\n",
      "magnacarta had 1 pieces of work with a total of 25334 characters of text\n",
      "maidstone had 1 pieces of work with a total of 12204 characters of text\n",
      "malaterra had 1 pieces of work with a total of 60216 characters of text\n",
      "manilius had 5 pieces of work with a total of 195808 characters of text\n",
      "mapps had 1 pieces of work with a total of 10434 characters of text\n",
      "marbodus had 1 pieces of work with a total of 11471 characters of text\n",
      "marcantonioaldegati had 1 pieces of work with a total of 48445 characters of text\n",
      "marcellinus had 1 pieces of work with a total of 79538 characters of text\n",
      "marcusmincuiusfelix had 1 pieces of work with a total of 82026 characters of text\n",
      "martial had 14 pieces of work with a total of 275032 characters of text\n",
      "martinbraga had 1 pieces of work with a total of 5255 characters of text\n",
      "marullo had 1 pieces of work with a total of 2388 characters of text\n",
      "marx had 1 pieces of work with a total of 6181 characters of text\n",
      "maximianus had 1 pieces of work with a total of 29420 characters of text\n",
      "may had 1 pieces of work with a total of 18038 characters of text\n",
      "melanchthon had 1 pieces of work with a total of 71805 characters of text\n",
      "milton had 1 pieces of work with a total of 10794 characters of text\n",
      "minucius had 1 pieces of work with a total of 82425 characters of text\n",
      "mirabilia had 1 pieces of work with a total of 32012 characters of text\n",
      "mirandola had 1 pieces of work with a total of 54274 characters of text\n",
      "montanus had 1 pieces of work with a total of 5494 characters of text\n",
      "more had 1 pieces of work with a total of 192797 characters of text\n",
      "musavenit had 1 pieces of work with a total of 627 characters of text\n",
      "naevius had 1 pieces of work with a total of 5843 characters of text\n",
      "navagero had 1 pieces of work with a total of 45281 characters of text\n",
      "nemesianus had 1 pieces of work with a total of 4018 characters of text\n",
      "nepos had 1 pieces of work with a total of 202032 characters of text\n",
      "newton had 1 pieces of work with a total of 10281 characters of text\n",
      "nithardus had 1 pieces of work with a total of 27648 characters of text\n",
      "nobilis had 1 pieces of work with a total of 604 characters of text\n",
      "notitia had 1 pieces of work with a total of 37953 characters of text\n",
      "novatian had 1 pieces of work with a total of 135244 characters of text\n",
      "obsequens had 1 pieces of work with a total of 39105 characters of text\n",
      "omnegenus had 1 pieces of work with a total of 1155 characters of text\n",
      "oratio had 1 pieces of work with a total of 6515 characters of text\n",
      "oresmius had 1 pieces of work with a total of 68052 characters of text\n",
      "origo had 1 pieces of work with a total of 6817 characters of text\n",
      "orosius had 1 pieces of work with a total of 66309 characters of text\n",
      "ottofreising had 1 pieces of work with a total of 114755 characters of text\n",
      "ovid had 41 pieces of work with a total of 1473588 characters of text\n",
      "owen had 1 pieces of work with a total of 3059 characters of text\n",
      "paris had 1 pieces of work with a total of 4805 characters of text\n",
      "pascoli had 1 pieces of work with a total of 6222 characters of text\n",
      "passerat had 1 pieces of work with a total of 3454 characters of text\n",
      "patricius had 1 pieces of work with a total of 21936 characters of text\n",
      "pauldeacon had 1 pieces of work with a total of 21421 characters of text\n",
      "paulinus had 1 pieces of work with a total of 173954 characters of text\n",
      "paulusdiaconus had 1 pieces of work with a total of 69912 characters of text\n",
      "perp had 1 pieces of work with a total of 24115 characters of text\n",
      "persius had 1 pieces of work with a total of 29733 characters of text\n",
      "pervig had 1 pieces of work with a total of 4300 characters of text\n",
      "petrarch had 1 pieces of work with a total of 5987 characters of text\n",
      "petrarchmedicus had 1 pieces of work with a total of 151780 characters of text\n",
      "petronius had 1 pieces of work with a total of 214546 characters of text\n",
      "petroniusfrag had 1 pieces of work with a total of 7962 characters of text\n",
      "phaedr had 1 pieces of work with a total of 7033 characters of text\n",
      "phaedrapp had 1 pieces of work with a total of 18127 characters of text\n",
      "piccolomini had 1 pieces of work with a total of 6556 characters of text\n",
      "planctus had 1 pieces of work with a total of 3076 characters of text\n",
      "plautus had 20 pieces of work with a total of 1062674 characters of text\n",
      "pliny had 1 pieces of work with a total of 57967 characters of text\n",
      "plinytheelder had 7 pieces of work with a total of 2810822 characters of text\n",
      "plinytheyounger had 10 pieces of work with a total of 467614 characters of text\n",
      "poggio had 1 pieces of work with a total of 100501 characters of text\n",
      "polignac had 3 pieces of work with a total of 898418 characters of text\n",
      "pomponius had 1 pieces of work with a total of 41047 characters of text\n",
      "pontano had 1 pieces of work with a total of 5989 characters of text\n",
      "poree had 1 pieces of work with a total of 57100 characters of text\n",
      "porphyrius had 1 pieces of work with a total of 34744 characters of text\n",
      "potatores had 1 pieces of work with a total of 1022 characters of text\n",
      "prataiam had 1 pieces of work with a total of 738 characters of text\n",
      "prec had 1 pieces of work with a total of 1374 characters of text\n",
      "precatio had 1 pieces of work with a total of 899 characters of text\n",
      "priapea had 1 pieces of work with a total of 23249 characters of text\n",
      "priscian had 4 pieces of work with a total of 125385 characters of text\n",
      "professio had 1 pieces of work with a total of 8725 characters of text\n",
      "prop had 1 pieces of work with a total of 39553 characters of text\n",
      "propertius had 1 pieces of work with a total of 164271 characters of text\n",
      "prosperus had 1 pieces of work with a total of 99991 characters of text\n",
      "protospatarius had 1 pieces of work with a total of 30329 characters of text\n",
      "prudentius had 7 pieces of work with a total of 238798 characters of text\n",
      "pseudocicero had 1 pieces of work with a total of 12256 characters of text\n",
      "pseudoquintilian had 1 pieces of work with a total of 475491 characters of text\n",
      "psplato had 1 pieces of work with a total of 8802 characters of text\n",
      "pulchracomis had 1 pieces of work with a total of 217 characters of text\n",
      "qcicero had 1 pieces of work with a total of 28874 characters of text\n",
      "quintilian had 12 pieces of work with a total of 1183641 characters of text\n",
      "quum had 1 pieces of work with a total of 2281 characters of text\n",
      "raoul had 1 pieces of work with a total of 268357 characters of text\n",
      "regula had 1 pieces of work with a total of 40185 characters of text\n",
      "reposianus had 1 pieces of work with a total of 8568 characters of text\n",
      "resgestae had 1 pieces of work with a total of 19561 characters of text\n",
      "rhetores had 1 pieces of work with a total of 678 characters of text\n",
      "richerus had 1 pieces of work with a total of 101221 characters of text\n",
      "rimbaud had 1 pieces of work with a total of 2988 characters of text\n",
      "ruaeus had 1 pieces of work with a total of 17523 characters of text\n",
      "rumor had 1 pieces of work with a total of 943 characters of text\n",
      "rutilius had 1 pieces of work with a total of 29644 characters of text\n",
      "rutiliuslupus had 1 pieces of work with a total of 31477 characters of text\n",
      "sabinus had 1 pieces of work with a total of 4623 characters of text\n",
      "sall had 1 pieces of work with a total of 150818 characters of text\n",
      "sallust had 3 pieces of work with a total of 254194 characters of text\n",
      "sannazaro had 1 pieces of work with a total of 65699 characters of text\n",
      "scaliger had 1 pieces of work with a total of 294 characters of text\n",
      "scbaccanalibus had 1 pieces of work with a total of 4922 characters of text\n",
      "scottus had 1 pieces of work with a total of 1589 characters of text\n",
      "scriptoreshistoriaeaugustae had 4 pieces of work with a total of 527723 characters of text\n",
      "sedulius had 1 pieces of work with a total of 14042 characters of text\n",
      "sen had 1 pieces of work with a total of 53425 characters of text\n",
      "seneca had 42 pieces of work with a total of 2155204 characters of text\n",
      "senecatheelder had 8 pieces of work with a total of 666264 characters of text\n",
      "septsap had 1 pieces of work with a total of 49331 characters of text\n",
      "serviushonoratus had 4 pieces of work with a total of 355299 characters of text\n",
      "sha had 1 pieces of work with a total of 19068 characters of text\n",
      "sicmeafata had 1 pieces of work with a total of 798 characters of text\n",
      "sidonius had 1 pieces of work with a total of 42650 characters of text\n",
      "sigebert had 1 pieces of work with a total of 10679 characters of text\n",
      "silius had 1 pieces of work with a total of 30850 characters of text\n",
      "siliusitalicus had 17 pieces of work with a total of 538363 characters of text\n",
      "simedignetur had 1 pieces of work with a total of 308 characters of text\n",
      "smarius had 1 pieces of work with a total of 6792 characters of text\n",
      "solet had 1 pieces of work with a total of 11763 characters of text\n",
      "solinus had 1 pieces of work with a total of 241481 characters of text\n",
      "spinoza had 1 pieces of work with a total of 117778 characters of text\n",
      "statius had 18 pieces of work with a total of 656190 characters of text\n",
      "suetonius had 12 pieces of work with a total of 518934 characters of text\n",
      "sulpicia had 1 pieces of work with a total of 1697 characters of text\n",
      "sulpiciusseveruschron had 1 pieces of work with a total of 92885 characters of text\n",
      "sulpiciusseverusmartin had 1 pieces of work with a total of 48368 characters of text\n",
      "suscipeflos had 1 pieces of work with a total of 417 characters of text\n",
      "syrus had 1 pieces of work with a total of 56263 characters of text\n",
      "tacitus had 20 pieces of work with a total of 1204956 characters of text\n",
      "tempusest had 1 pieces of work with a total of 1210 characters of text\n",
      "ter had 1 pieces of work with a total of 61820 characters of text\n",
      "terence had 6 pieces of work with a total of 310741 characters of text\n",
      "terraiam had 1 pieces of work with a total of 1107 characters of text\n",
      "tertullian had 2 pieces of work with a total of 186130 characters of text\n",
      "testamentum had 1 pieces of work with a total of 2192 characters of text\n",
      "tevigilans had 1 pieces of work with a total of 351 characters of text\n",
      "theganus had 1 pieces of work with a total of 45690 characters of text\n",
      "theodolus had 1 pieces of work with a total of 16262 characters of text\n",
      "theodosius had 1 pieces of work with a total of 143782 characters of text\n",
      "theophanes had 1 pieces of work with a total of 1930 characters of text\n",
      "thesauro had 1 pieces of work with a total of 12371 characters of text\n",
      "thomasedessa had 1 pieces of work with a total of 73511 characters of text\n",
      "tibullus had 1 pieces of work with a total of 80686 characters of text\n",
      "tunger had 1 pieces of work with a total of 89239 characters of text\n",
      "valeriusflaccus had 8 pieces of work with a total of 278759 characters of text\n",
      "valeriusmaximus had 9 pieces of work with a total of 586001 characters of text\n",
      "valesianus had 1 pieces of work with a total of 120 characters of text\n",
      "valmax had 1 pieces of work with a total of 68958 characters of text\n",
      "varro had 1 pieces of work with a total of 33190 characters of text\n",
      "vegetius had 1 pieces of work with a total of 72149 characters of text\n",
      "vegius had 1 pieces of work with a total of 29868 characters of text\n",
      "vell had 1 pieces of work with a total of 167143 characters of text\n",
      "venantius had 1 pieces of work with a total of 5002 characters of text\n",
      "vergil had 17 pieces of work with a total of 579350 characters of text\n",
      "vicentius had 1 pieces of work with a total of 121964 characters of text\n",
      "vico had 1 pieces of work with a total of 21838 characters of text\n",
      "victor had 1 pieces of work with a total of 69412 characters of text\n",
      "vida had 1 pieces of work with a total of 30105 characters of text\n",
      "vitacaroli had 1 pieces of work with a total of 104331 characters of text\n",
      "vitruvius had 10 pieces of work with a total of 421054 characters of text\n",
      "volovirum had 1 pieces of work with a total of 1071 characters of text\n",
      "voragine had 1 pieces of work with a total of 3830 characters of text\n",
      "waardenburg had 1 pieces of work with a total of 3553 characters of text\n",
      "waltarius had 1 pieces of work with a total of 18263 characters of text\n",
      "walter had 1 pieces of work with a total of 2305 characters of text\n",
      "walton had 1 pieces of work with a total of 26036 characters of text\n",
      "williamapulia had 1 pieces of work with a total of 123908 characters of text\n",
      "williamtyre had 1 pieces of work with a total of 87915 characters of text\n",
      "withof had 1 pieces of work with a total of 7151 characters of text\n",
      "wmconchesdogma had 1 pieces of work with a total of 91687 characters of text\n",
      "wmconchesphil had 1 pieces of work with a total of 39080 characters of text\n",
      "xanten had 1 pieces of work with a total of 49919 characters of text\n",
      "xylander had 1 pieces of work with a total of 106561 characters of text\n",
      "zonaras had 1 pieces of work with a total of 76548 characters of text\n"
     ]
    }
   ],
   "source": [
    "CI = dataExp.CorpusInterface(corpus_name=\"text_corpus.pickle\", shouldTokenize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5945bd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:39:19.960350Z",
     "iopub.status.busy": "2023-03-01T21:39:19.959388Z",
     "iopub.status.idle": "2023-03-01T21:39:25.958457Z",
     "shell.execute_reply": "2023-03-01T21:39:25.957070Z",
     "shell.execute_reply.started": "2023-03-01T21:39:19.960309Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /notebooks/LatinBERT/latin_bert were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32900, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=32900, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizerPath = os.getcwd()+\"/LatinBERT/latin.subword.encoder\"\n",
    "bertPath = os.getcwd()+\"/LatinBERT/latin_bert\"\n",
    "encoder = text_encoder.SubwordTextEncoder(tokenizerPath)\n",
    "tokenizer = LatinTokenizer(encoder)\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(bertPath)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a578452",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:40:40.619602Z",
     "iopub.status.busy": "2023-03-01T21:40:40.619264Z",
     "iopub.status.idle": "2023-03-01T21:40:40.627459Z",
     "shell.execute_reply": "2023-03-01T21:40:40.626350Z",
     "shell.execute_reply.started": "2023-03-01T21:40:40.619577Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_batch(batch):\n",
    "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
    "\n",
    "def untokenize_batch(batch):\n",
    "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
    "\n",
    "def detokenize(sent):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    for i, tok in enumerate(sent):\n",
    "        if tok.startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
    "        else:\n",
    "            new_sent.append(tok)\n",
    "    return new_sent\n",
    "\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "MASK = '[MASK]'\n",
    "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
    "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
    "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f89075dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:40:42.995095Z",
     "iopub.status.busy": "2023-03-01T21:40:42.993902Z",
     "iopub.status.idle": "2023-03-01T21:40:43.006155Z",
     "shell.execute_reply": "2023-03-01T21:40:43.004635Z",
     "shell.execute_reply.started": "2023-03-01T21:40:42.995044Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
    "    \"\"\" Generate a word from from out[gen_idx]\n",
    "    \n",
    "    args:\n",
    "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
    "        - gen_idx (int): location for which to generate for\n",
    "        - top_k (int): if >0, only sample from the top k most probable words\n",
    "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    logits = out[\"logits\"][:, gen_idx]\n",
    "    if temperature is not None:\n",
    "        logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
    "    elif sample:\n",
    "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        idx = dist.sample().squeeze(-1)\n",
    "    else:\n",
    "        idx = torch.argmax(logits, dim=-1)\n",
    "    return idx.tolist() if return_list else idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9f31bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:40:44.856316Z",
     "iopub.status.busy": "2023-03-01T21:40:44.855807Z",
     "iopub.status.idle": "2023-03-01T21:40:44.886370Z",
     "shell.execute_reply": "2023-03-01T21:40:44.885398Z",
     "shell.execute_reply.started": "2023-03-01T21:40:44.856277Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generation modes as functions\n",
    "import math\n",
    "import time\n",
    "\n",
    "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
    "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
    "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
    "    #if rand_init:\n",
    "    #    for ii in range(max_len):\n",
    "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
    "    \n",
    "    return tokenize_batch(batch)\n",
    "\n",
    "def parallel_sequential_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
    "                                   cuda=False, print_every=10, verbose=True):\n",
    "    \"\"\" Generate for one random position at a timestep\n",
    "    \n",
    "    args:\n",
    "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
    "    \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        kk = np.random.randint(0, max_len)\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+kk] = mask_id\n",
    "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        out = model(inp)\n",
    "        topk = top_k if (ii >= burnin) else 0\n",
    "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+kk] = idxs[jj]\n",
    "            \n",
    "        if verbose and np.mod(ii+1, print_every) == 0:\n",
    "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
    "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "            print(\"iter\", ii+1, \" \".join(for_print))\n",
    "            \n",
    "    return untokenize_batch(batch)\n",
    "\n",
    "def parallel_generation(seed_text, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
    "                        cuda=False, print_every=10, verbose=True):\n",
    "    \"\"\" Generate for all positions at a time step \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        out = model(inp)\n",
    "        for kk in range(max_len):\n",
    "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, temperature=temperature, sample=sample)\n",
    "            for jj in range(batch_size):\n",
    "                batch[jj][seed_len+kk] = idxs[jj]\n",
    "            \n",
    "        if verbose and np.mod(ii, print_every) == 0:\n",
    "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
    "    \n",
    "    return untokenize_batch(batch)\n",
    "            \n",
    "def sequential_generation(seed_text, batch_size=2, max_len=15, leed_out_len=15, \n",
    "                          top_k=0, temperature=None, sample=True, cuda=False):\n",
    "    \"\"\" Generate one word at a time, in L->R order \"\"\"\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text, max_len, batch_size)\n",
    "    batch = batch.cuda() if cuda else batch\n",
    "    \n",
    "    for ii in range(max_len):\n",
    "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
    "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "        out = model(inp)\n",
    "        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, temperature=temperature, sample=sample)\n",
    "        for jj in range(batch_size):\n",
    "            batch[jj][seed_len+ii] = idxs[jj]\n",
    "        \n",
    "        return untokenize_batch(batch)\n",
    "\n",
    "\n",
    "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
    "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
    "             cuda=False, print_every=1):\n",
    "    # main generation function to call\n",
    "    sentences = []\n",
    "    n_batches = math.ceil(n_samples / batch_size)\n",
    "    start_time = time.time()\n",
    "    for batch_n in range(n_batches):\n",
    "        batch = parallel_sequential_generation(seed_text, max_len=max_len, top_k=top_k,\n",
    "                                               temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
    "                                               cuda=cuda, verbose=False)\n",
    "        \n",
    "        #batch = sequential_generation(seed_text, batch_size=20, max_len=max_len, top_k=top_k, temperature=temperature, leed_out_len=leed_out_len, sample=sample)\n",
    "        #batch = parallel_generation(seed_text, max_len=max_len, top_k=top_k, temperature=temperature, sample=sample, max_iter=max_iter)\n",
    "        \n",
    "        if (batch_n + 1) % print_every == 0:\n",
    "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "        \n",
    "        sentences += batch\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65d489cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:40:48.411993Z",
     "iopub.status.busy": "2023-03-01T21:40:48.411466Z",
     "iopub.status.idle": "2023-03-01T21:40:48.423312Z",
     "shell.execute_reply": "2023-03-01T21:40:48.421548Z",
     "shell.execute_reply.started": "2023-03-01T21:40:48.411955Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def printer(sent, should_detokenize=True):\n",
    "    if should_detokenize:\n",
    "        sent = detokenize(sent)[1:-1]\n",
    "    print(\" \".join(sent))\n",
    "    \n",
    "def read_sents(in_file, should_detokenize=False):\n",
    "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
    "    if should_detokenize:\n",
    "        sents = [detokenize(sent) for sent in sents]\n",
    "    return sents\n",
    "\n",
    "def write_sents(out_file, sents, should_detokenize=False):\n",
    "    with open(out_file, \"w\") as out_fh:\n",
    "        for sent in sents:\n",
    "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
    "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a47c52fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T21:41:52.232363Z",
     "iopub.status.busy": "2023-03-01T21:41:52.232020Z",
     "iopub.status.idle": "2023-03-01T21:42:32.841070Z",
     "shell.execute_reply": "2023-03-01T21:42:32.839901Z",
     "shell.execute_reply.started": "2023-03-01T21:41:52.232338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished batch 1 in 40.599s\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1\n",
    "batch_size = 10\n",
    "max_len = 40\n",
    "top_k = 100\n",
    "temperature = 0.7\n",
    "\n",
    "leed_out_len = 5 # max_len\n",
    "burnin = 250\n",
    "sample = True\n",
    "max_iter = 500\n",
    "\n",
    "# Choose the prefix context\n",
    "seed_text = \"[CLS]\".split()\n",
    "\n",
    "for temp in [1.0]:\n",
    "    bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
    "                          sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
    "                          cuda=True)\n",
    "    out_file = \"Results/len%d-burnin%d-topk%d-temp%.3f.txt\" % (max_len, burnin, top_k, temp)\n",
    "    write_sents(out_file, bert_sents, should_detokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0d51bcd-ee20-44b3-9f03-6445d93fba01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-01T20:42:31.156138Z",
     "iopub.status.busy": "2023-03-01T20:42:31.155770Z",
     "iopub.status.idle": "2023-03-01T20:42:31.163019Z",
     "shell.execute_reply": "2023-03-01T20:42:31.161822Z",
     "shell.execute_reply.started": "2023-03-01T20:42:31.156112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SubwordTextEncoder.decode of <tensor2tensor.data_generators.text_encoder.SubwordTextEncoder object at 0x7f87441806d0>>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encoder.decode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
